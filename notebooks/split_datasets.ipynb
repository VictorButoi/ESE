{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_integer(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = pathlib.Path(\"/storage/vbutoi/datasets/WMH\")\n",
    "splits = [\"training\", \"test\", \"additional_annotations\"]\n",
    "\n",
    "all_dirs = []\n",
    "\n",
    "for split in splits:\n",
    "    split_path = root / split\n",
    "    for subdir in split_path.iterdir():\n",
    "        print(subdir)\n",
    "        all_dirs.append(subdir)\n",
    "        for l3_dir in subdir.iterdir():\n",
    "            if not is_integer(str(l3_dir.name)):\n",
    "                print(l3_dir)\n",
    "                all_dirs.append(l3_dir)\n",
    "                for l4_dir in l3_dir.iterdir():\n",
    "                    if not is_integer(str(l4_dir.name)):\n",
    "                        print(l4_dir)\n",
    "                        all_dirs.append(l4_dir)\n",
    "                        for l5_dir in l4_dir.iterdir():\n",
    "                            if not is_integer(str(l5_dir.name)):\n",
    "                                print(l5_dir)\n",
    "                                all_dirs.append(l5_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dirs_with_additional = []\n",
    "for path in all_dirs:\n",
    "    all_other_dirs = [p for p in all_dirs if p != path]\n",
    "    is_subdir = False\n",
    "    for other_path in all_other_dirs:\n",
    "        if path in other_path.parents:\n",
    "            is_subdir = True\n",
    "            break\n",
    "    if not is_subdir:\n",
    "        unique_dirs_with_additional.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dirs_with_additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dirs = [ud for ud in unique_dirs_with_additional if \"additional_annotations\" not in str(ud)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import nibabel.processing as nip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def resample_nib(img, voxel_spacing=(1, 1, 1), order=3):\n",
    "    \"\"\"Resamples the nifti from its original spacing to another specified spacing\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    img: nibabel image\n",
    "    voxel_spacing: a tuple of 3 integers specifying the desired new spacing\n",
    "    order: the order of interpolation\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    new_img: The resampled nibabel image \n",
    "    \n",
    "    \"\"\"\n",
    "    # resample to new voxel spacing based on the current x-y-z-orientation\n",
    "    aff = img.affine\n",
    "    shp = img.shape\n",
    "    zms = img.header.get_zooms()\n",
    "    # Calculate new shape\n",
    "    new_shp = tuple(np.rint([\n",
    "        shp[0] * zms[0] / voxel_spacing[0],\n",
    "        shp[1] * zms[1] / voxel_spacing[1],\n",
    "        shp[2] * zms[2] / voxel_spacing[2]\n",
    "        ]).astype(int))\n",
    "    new_aff = nib.affines.rescale_affine(aff, shp, voxel_spacing, new_shp)\n",
    "    new_img = nip.resample_from_to(img, (new_shp, new_aff), order=order, cval=-1024)\n",
    "    return new_img\n",
    "\n",
    "\n",
    "def resample_mask_to(msk, to_img):\n",
    "    \"\"\"Resamples the nifti mask from its original spacing to a new spacing specified by its corresponding image\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    msk: The nibabel nifti mask to be resampled\n",
    "    to_img: The nibabel image that acts as a template for resampling\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    new_msk: The resampled nibabel mask \n",
    "    \n",
    "    \"\"\"\n",
    "    to_img.header['bitpix'] = 8\n",
    "    to_img.header['datatype'] = 2  # uint8\n",
    "    new_msk = nib.processing.resample_from_to(msk, to_img, order=0)\n",
    "    return new_msk\n",
    "\n",
    "\n",
    "def pad_image_numpy(image_array, target_width, target_height):\n",
    "    # Convert the NumPy array to a Pillow image\n",
    "    original_image = Image.fromarray(image_array)\n",
    "\n",
    "    # Get the original dimensions\n",
    "    original_width, original_height = original_image.size\n",
    "\n",
    "    # Calculate the amount of padding needed\n",
    "    width_padding = target_width - original_width\n",
    "    height_padding = target_height - original_height\n",
    "\n",
    "    # Calculate the padding borders\n",
    "    left_padding = width_padding // 2\n",
    "    right_padding = width_padding - left_padding\n",
    "    top_padding = height_padding // 2\n",
    "    bottom_padding = height_padding - top_padding\n",
    "\n",
    "    # Get the border pixel value\n",
    "    border_pixel = original_image.getpixel((10, 10))\n",
    "\n",
    "    # Create a new image with the desired dimensions and fill with border pixel\n",
    "    padded_image = Image.new(original_image.mode, (target_width, target_height), border_pixel)\n",
    "\n",
    "    # Paste the original image onto the padded image with padding borders\n",
    "    padded_image.paste(original_image, (left_padding, top_padding))\n",
    "\n",
    "    # Convert the padded image back to a NumPy array\n",
    "    padded_image_array = np.array(padded_image)\n",
    "\n",
    "    return padded_image_array\n",
    "\n",
    "def normalize_image(image):\n",
    "    # Convert the image to floating point format\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    # Normalize the image between 0 and 1\n",
    "    normalized_image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "    \n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m/storage/vbutoi/projects\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mese\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperiment\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuild_wmh\u001b[39;00m \u001b[39mimport\u001b[39;00m proc_WMH\n\u001b[0;32m----> 6\u001b[0m proc_WMH(\n\u001b[1;32m      7\u001b[0m     unique_dirs, \n\u001b[1;32m      8\u001b[0m     modalities\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mFLAIR\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      9\u001b[0m     show\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     10\u001b[0m     save\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m     )\n",
      "File \u001b[0;32m/storage/vbutoi/projects/ESE/ese/experiment/datasets/utils/build_wmh.py:122\u001b[0m, in \u001b[0;36mproc_WMH\u001b[0;34m(data_dirs, modalities, show, save)\u001b[0m\n\u001b[1;32m    120\u001b[0m image_dir \u001b[39m=\u001b[39m subj \u001b[39m/\u001b[39m pathlib\u001b[39m.\u001b[39mPath(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpre/\u001b[39m\u001b[39m{\u001b[39;00mmodality\u001b[39m}\u001b[39;00m\u001b[39m.nii.gz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading image: \u001b[39m\u001b[39m\"\u001b[39m, image_dir)\n\u001b[0;32m--> 122\u001b[0m img_volume \u001b[39m=\u001b[39m resample_nib(nib\u001b[39m.\u001b[39;49mload(image_dir))\n\u001b[1;32m    123\u001b[0m mid_axis_slice \u001b[39m=\u001b[39m img_volume\u001b[39m.\u001b[39mget_fdata()\u001b[39m.\u001b[39mshape[axis] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \n\u001b[1;32m    124\u001b[0m img_modality_slice \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtake(img_volume\u001b[39m.\u001b[39mget_fdata(), mid_axis_slice, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[0;32m/storage/vbutoi/projects/ESE/ese/experiment/datasets/utils/build_wmh.py:36\u001b[0m, in \u001b[0;36mresample_nib\u001b[0;34m(img, voxel_spacing, order)\u001b[0m\n\u001b[1;32m     30\u001b[0m new_shp \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(np\u001b[39m.\u001b[39mrint([\n\u001b[1;32m     31\u001b[0m     shp[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m zms[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m voxel_spacing[\u001b[39m0\u001b[39m],\n\u001b[1;32m     32\u001b[0m     shp[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m zms[\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m voxel_spacing[\u001b[39m1\u001b[39m],\n\u001b[1;32m     33\u001b[0m     shp[\u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m zms[\u001b[39m2\u001b[39m] \u001b[39m/\u001b[39m voxel_spacing[\u001b[39m2\u001b[39m]\n\u001b[1;32m     34\u001b[0m     ])\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m))\n\u001b[1;32m     35\u001b[0m new_aff \u001b[39m=\u001b[39m nib\u001b[39m.\u001b[39maffines\u001b[39m.\u001b[39mrescale_affine(aff, shp, voxel_spacing, new_shp)\n\u001b[0;32m---> 36\u001b[0m new_img \u001b[39m=\u001b[39m nip\u001b[39m.\u001b[39;49mresample_from_to(img, (new_shp, new_aff), order\u001b[39m=\u001b[39;49morder, cval\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1024\u001b[39;49m)\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m new_img\n",
      "File \u001b[0;32m~/envs/SAM/lib/python3.9/site-packages/nibabel/processing.py:179\u001b[0m, in \u001b[0;36mresample_from_to\u001b[0;34m(from_img, to_vox_map, order, mode, cval, out_class)\u001b[0m\n\u001b[1;32m    177\u001b[0m to_vox2from_vox \u001b[39m=\u001b[39m npl\u001b[39m.\u001b[39minv(a_from_affine)\u001b[39m.\u001b[39mdot(a_to_affine)\n\u001b[1;32m    178\u001b[0m rzs, trans \u001b[39m=\u001b[39m to_matvec(to_vox2from_vox)\n\u001b[0;32m--> 179\u001b[0m data \u001b[39m=\u001b[39m spnd\u001b[39m.\u001b[39;49maffine_transform(\n\u001b[1;32m    180\u001b[0m     from_img\u001b[39m.\u001b[39;49mdataobj, rzs, trans, to_shape, order\u001b[39m=\u001b[39;49morder, mode\u001b[39m=\u001b[39;49mmode, cval\u001b[39m=\u001b[39;49mcval\n\u001b[1;32m    181\u001b[0m )\n\u001b[1;32m    182\u001b[0m \u001b[39mreturn\u001b[39;00m out_class(data, to_affine, from_img\u001b[39m.\u001b[39mheader)\n",
      "File \u001b[0;32m~/envs/SAM/lib/python3.9/site-packages/scipy/ndimage/_interpolation.py:614\u001b[0m, in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    611\u001b[0m     _nd_image\u001b[39m.\u001b[39mzoom_shift(filtered, matrix, offset\u001b[39m/\u001b[39mmatrix, output, order,\n\u001b[1;32m    612\u001b[0m                          mode, cval, npad, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    613\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 614\u001b[0m     _nd_image\u001b[39m.\u001b[39;49mgeometric_transform(filtered, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, matrix, offset,\n\u001b[1;32m    615\u001b[0m                                   output, order, mode, cval, npad, \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    616\u001b[0m                                   \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    617\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/storage/vbutoi/projects/ESE\")\n",
    "sys.path.append(\"/storage/vbutoi/projects\")\n",
    "from ese.experiment.datasets.utils.build_wmh import proc_WMH\n",
    "\n",
    "proc_WMH(\n",
    "    unique_dirs, \n",
    "    modalities=[\"FLAIR\"],\n",
    "    show=True,\n",
    "    save=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thunderpack import ThunderDB\n",
    "\n",
    "def thunderify_WMH(datacenter, modality, version=0.1):\n",
    "\n",
    "    source_dir = pathlib.Path(\"/storage/vbutoi/datasets/WMH/thunder\")\n",
    "    with ThunderDB.open(source_dir + 'data', 'c') as db:\n",
    "        db['metadata'] = {\n",
    "            'version': version, \n",
    "            'n_samples': None\n",
    "        }\n",
    "        \n",
    "        keys = []\n",
    "        for i in range(100):\n",
    "            key = f'sample{i:02d}'\n",
    "            x = np.random.normal(size=(128,128))\n",
    "            y = np.random.normal(size=(128,128))\n",
    "            # Thunderpack will serialize the tuple and numpy arrays automatically\n",
    "            db[key] = (x, y) \n",
    "            keys.append(key)\n",
    "\n",
    "        db['samples'] = keys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM",
   "language": "python",
   "name": "sam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
