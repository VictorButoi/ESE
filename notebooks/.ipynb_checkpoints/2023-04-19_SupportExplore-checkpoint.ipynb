{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb6ea13",
   "metadata": {},
   "source": [
    "# Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ac3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Setup environment correctly first\n",
    "import os \n",
    "# Go to location where UniverSeg data is\n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage',\n",
    "       '/storage/megamedical'\n",
    "))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# UniverSeg Imports\n",
    "from universeg.experiment.experiment import UniversegExperiment\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# IonPy imports\n",
    "from pylot.analysis import ResultsLoader\n",
    "\n",
    "# Misc imports\n",
    "import numpy as np\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from pydantic import validate_arguments\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e95918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f2aee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11c6dfa3b26470c9a7eb5128f9c19f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from universeg.experiment.results import load_configs\n",
    "\n",
    "# Load configs\n",
    "configs = load_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b6f3af",
   "metadata": {},
   "source": [
    "For now load the model with support size = 8 so we have a good idea of subsampling a larger dataset to get to a smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08131f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_support_model = configs.select(support_size=8)\n",
    "model = small_support_model.path.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98850e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/storage/jjgo/results/omni/2023-03-01_Universeg-SupportSet-Ablation_pt2/20230304_125802-NH10-26180efa296261c9502bf25a551378e5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f2a9c",
   "metadata": {},
   "source": [
    "# Part 2: Base Random Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b056fe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/vbutoi/libraries/pylot/util/libcheck.py:51: UserWarning: Intel MKL extensions not available for SciPy\n",
      "  warn(\"Intel MKL extensions not available for SciPy\")\n",
      "/storage/vbutoi/libraries/pylot/util/libcheck.py:53: UserWarning: libjpeg_turbo not enabled for Pillow\n",
      "  warn(\"libjpeg_turbo not enabled for Pillow\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint with tag:max-val_od-dice_score. Last epoch:4430\n"
     ]
    }
   ],
   "source": [
    "from universeg.experiment.analysis import compute_dice_breakdown\n",
    "\n",
    "df = compute_dice_breakdown(\n",
    "                model,\n",
    "                datasets=[\"WBC\"],\n",
    "                support_size=8,\n",
    "                split=\"val\",\n",
    "                checkpoint=\"max-val_od-dice_score\",\n",
    "                augmentations=None,\n",
    "                slicing=\"midslice\",\n",
    "                n_predictions=1,\n",
    "                preload_cuda=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e5821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85023385"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dice_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65546718",
   "metadata": {},
   "source": [
    "# Part 3: DiSeg Hyper-Net (Sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e2b13",
   "metadata": {},
   "source": [
    "First, let's define a sampler which given a (image, label) pair which try to predict the indices of the best support set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a762874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights, ResNet50_Weights, ResNet101_Weights\n",
    "\n",
    "@validate_arguments\n",
    "@dataclass(eq=False, repr=False)\n",
    "class SupportSampler(nn.Module):\n",
    "\n",
    "    dset_size : int\n",
    "    model_type: str = \"resnet18\"\n",
    "    pretrained: bool = True \n",
    "    freeze_backbone: bool = False\n",
    "    use_label: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        if self.model_type == 'resnet18':\n",
    "            weights = ResNet18_Weights if self.pretrained else None\n",
    "            self.model = models.resnet18(weights=weights)\n",
    "        elif self.model_type == 'resnet50':\n",
    "            weights = ResNet50_Weights if self.pretrained else None\n",
    "            self.model = models.resnet50(weights=weights)\n",
    "        elif self.model_type == 'resnet101':\n",
    "            weights = ResNet101_Weights if self.pretrained else None\n",
    "            self.model = models.resnet101(weights=weights)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ResNet model type: {self.model_type}\")\n",
    "        \n",
    "        if self.freeze_backbone:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Make an adaptation head to accept two channels\n",
    "        self.head_conv = nn.Conv2d(2, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        # Make sure the output predicts over the size of the any dataset\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, self.dset_size)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        assert not(self.use_label and y is None), \"If using label, must provide label\"\n",
    "\n",
    "        if self.use_label:\n",
    "            z = torch.cat([x, y], dim=1)\n",
    "            z_in = self.head_conv(z)\n",
    "            z_in = self.relu(z_in)\n",
    "        else:\n",
    "            z_in = x.repeat(1, 3, 1, 1)\n",
    "\n",
    "        indices_logits = self.model(z_in)\n",
    "\n",
    "        return indices_logits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac2a92",
   "metadata": {},
   "source": [
    "Now we need to be able to get some datasets which we can use to train this sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9430ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from universeg.experiment.datasets import MultiBinarySegment2DIndex, Segment2D\n",
    "\n",
    "def get_datasets(datasets, slicing=\"midslice\", split=\"val\"):\n",
    "\n",
    "    index = MultiBinarySegment2DIndex()\n",
    "    task_df = index.task_df(\n",
    "        slicing=slicing,\n",
    "        datasets=datasets,\n",
    "        resolution=128,\n",
    "        version=\"v4.2\",\n",
    "        expand_labels=True,\n",
    "    )\n",
    "\n",
    "    segment_2d_datasets = []\n",
    "\n",
    "    for _, row in task_df.iterrows():\n",
    "        copy_keys = (\"task\", \"label\", \"resolution\", \"slicing\", \"version\")\n",
    "        segment2d_params = dict(\n",
    "            split=split,\n",
    "            min_label_density=0,\n",
    "            preload=True,\n",
    "            **{k: row[k] for k in copy_keys},\n",
    "        )\n",
    "\n",
    "        target_dataset = Segment2D(**segment2d_params)\n",
    "        segment_2d_datasets.append(target_dataset)\n",
    "    \n",
    "    return segment_2d_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b50eb503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pylot.util import to_device\n",
    "\n",
    "\n",
    "class ModifiedCUDACachedDataset(Dataset):\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        assert torch.cuda.is_available()\n",
    "        self._dataset = dataset\n",
    "        # The difference is that we need to trea this is one object.\n",
    "        self.image_block = []\n",
    "        self.labels_block = [] \n",
    "        for (image, label) in self._dataset:\n",
    "            self.image_block.append(image)\n",
    "            self.labels_block.append(label)\n",
    "        self.image_block = to_device(torch.stack(self.image_block), \"cuda\")\n",
    "        self.labels_block = to_device(torch.stack(self.labels_block), \"cuda\")\n",
    "\n",
    "    # Get a particular index if you want, but not the intended use.\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_block[idx, ...], self.labels_block[idx, ...]\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        # This works because __getattr__ is only called as last resort\n",
    "        # https://stackoverflow.com/questions/2405590/how-do-i-override-getattr-without-breaking-the-default-behavior\n",
    "        return getattr(self._dataset, key)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c45bf",
   "metadata": {},
   "source": [
    "Getting closer, now we need to make a loaded version of the model, and then run the meta training procedure on it. First, initialize the model and sampler, Second, run that shit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d07ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint with tag:max-val_od-dice_score. Last epoch:4430\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/vbutoi/libraries/pylot/util/libcheck.py:51: UserWarning: Intel MKL extensions not available for SciPy\n",
      "  warn(\"Intel MKL extensions not available for SciPy\")\n",
      "/storage/vbutoi/libraries/pylot/util/libcheck.py:53: UserWarning: libjpeg_turbo not enabled for Pillow\n",
      "  warn(\"libjpeg_turbo not enabled for Pillow\")\n"
     ]
    }
   ],
   "source": [
    "# load the in-context learning model and freeze the parameters\n",
    "ic_experiment = UniversegExperiment(model)\n",
    "ic_experiment.load('max-val_od-dice_score')\n",
    "ic_experiment.to_device()\n",
    "ic_model = ic_experiment.model\n",
    "for _, p in ic_model.named_parameters():\n",
    "   p.requires_grad = False\n",
    "\n",
    "# load the dataset\n",
    "datasets = get_datasets([\"WBC\"])\n",
    "dset = ModifiedCUDACachedDataset(datasets[0]) # temporary set\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e737db3c",
   "metadata": {},
   "source": [
    "One major issue with the formulation is how do we actually backprop through our sampling and selection? To this, we can provide two solutions that are very simple to start and provide us some flexiblility going forward. For sampling, we can use gumbel softmax to draw samples from our categorical distribution. For selection it is a bit trickier, but using some tactics with matrix multiplication it isn't terribly difficult either.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4f40ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mneurite\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mne\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m gumbel_softmax\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_support\u001b[39m(indice_probs, dataset, temperature\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, support_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m):\n",
      "File \u001b[0;32m/storage/vbutoi/libraries/neurite/neurite/__init__.py:46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[39m# tensorflow is default backend\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mPlease install tensorflow to use this neurite backend\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/tensorflow/python/__init__.py:36\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py:26\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m self_check\n\u001b[1;32m     23\u001b[0m \u001b[39m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[39m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m self_check\u001b[39m.\u001b[39;49mpreload_check()\n\u001b[1;32m     28\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m   \u001b[39m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[1;32m     32\u001b[0m   \u001b[39m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/tensorflow/python/platform/self_check.py:63\u001b[0m, in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mCould not find the DLL(s) \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m. TensorFlow requires that these DLLs \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mbe installed in a directory that is named in your \u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39mPATH\u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(missing))\n\u001b[1;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m   \u001b[39m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[39m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[39m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[39m# SIGILL).\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[1;32m     64\u001b[0m   _pywrap_cpu_feature_guard\u001b[39m.\u001b[39mInfoAboutUnusedCPUFeatures()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import neurite as ne\n",
    "from torch.nn.functional import gumbel_softmax\n",
    "\n",
    "def select_support(indice_probs, dataset, temperature=0.1, support_size=8):\n",
    "    assert indice_probs.shape[0] == 1, \"Only batch size of 1 is supported.\"\n",
    "    indice_probs = indice_probs.squeeze(0) # Remove the batch dimension\n",
    "\n",
    "    gumbel_sampled_indices = torch.stack([gumbel_softmax(indice_probs, tau=temperature, hard=True) for _ in range(support_size)])\n",
    "\n",
    "    one_hot_transposed = gumbel_sampled_indices.T\n",
    "    selection_indices = one_hot_transposed[..., None, None]\n",
    "        \n",
    "    x_sample = (dataset.image_block * selection_indices).sum(dim=0)[:, None, ...]\n",
    "    y_sample = (dataset.labels_block * selection_indices).sum(dim=0)[:, None, ...]\n",
    "\n",
    "    # Add the batch dimensions and return.\n",
    "    return x_sample[None], y_sample[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9bc7bc",
   "metadata": {},
   "source": [
    "Tada! Now we have a fully differentiable way to both sample and select our desired support set. Now let's describe our meta training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482195e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylot.metrics.segmentation import soft_dice_score \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def meta_train(IC_Model, SupportSampler, dataset, support_size, bsize=1, lr=1e-3, iterations=100, temperature=0.1):\n",
    "\n",
    "    optimizer = torch.optim.Adam(SupportSampler.parameters(), lr=lr)\n",
    "\n",
    "    # Go through multiple iterations.\n",
    "    for i in tqdm(range(iterations), desc=\"Training\", unit=\"epoch\"):\n",
    "\n",
    "        # Sample random datapoint and send datapoints to device\n",
    "        x, y = dataset[np.random.choice(len(dataset))]\n",
    "        # add the batch dimension to query image and label\n",
    "        x, y = x[None], y[None]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Get the indices of the optimal support from the sampler \n",
    "        indice_probs = SupportSampler(x, y)\n",
    "\n",
    "        # Sample from the dataset multiple times using the indices to construct multiple batches\n",
    "        # and concatenate them together.\n",
    "        for j in range(bsize):\n",
    "\n",
    "            # Use gumbel softmax to sample from the probability distribution. To do this, make \n",
    "            # several samples from gumbel and then multiply with the indice mesh to get the\n",
    "            # indices of the support set.    \n",
    "            x_sample, y_sample = select_support(indice_probs, dataset, temperature=temperature, support_size=support_size)\n",
    "\n",
    "            if j == 0:\n",
    "                support_image_sets = x_sample\n",
    "                support_label_sets = y_sample\n",
    "            else:\n",
    "                support_image_sets = torch.cat([support_image_sets, x_sample], dim=0)\n",
    "                support_label_sets = torch.cat([support_label_sets, y_sample], dim=0)\n",
    "\n",
    "        # Copy the query image multiple times in the batch dimension\n",
    "        query_images = x.repeat(bsize, 1, 1, 1)\n",
    "        y = y.repeat(bsize, 1, 1, 1)\n",
    "\n",
    "        # Make predictions for supports sampled with the frozen IC model\n",
    "        y_hat = IC_Model(support_image_sets, support_label_sets, query_images)\n",
    "\n",
    "        # Get the average loss amongst predictions\n",
    "        dice_loss = soft_dice_score(y_hat, y, from_logits=True)\n",
    "\n",
    "        # Backpropagate\n",
    "        dice_loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e487f55",
   "metadata": {},
   "source": [
    "Finally, we run our optimization procedure and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119667a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "support_size = 8\n",
    "bsize = 2\n",
    "lr = 1e-3\n",
    "iterations = 1000\n",
    "temperature = 0.1\n",
    "\n",
    "# Initialize Sampler\n",
    "support_sampler = SupportSampler(dset_size=len(dset), model_type=\"resnet18\") \n",
    "support_sampler.to(device)\n",
    "\n",
    "# meta train the model\n",
    "meta_trained_model = meta_train(ic_model, support_sampler, dset, support_size, bsize, lr, iterations, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0cde3",
   "metadata": {},
   "source": [
    "And just like that, we've trained a sampler that can choose what support examples should be selected for a particular query image and label pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a440d",
   "metadata": {},
   "source": [
    "# Part 4: Preliminary Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc6f8b",
   "metadata": {},
   "source": [
    "We need to build out this repo so that these experiments can scale (and thus out of this notebook), but let's look very quickly at some of the suppoert sets that get sampled. First, choose a random image and label pair from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf7237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random datapoint and send datapoints to device\n",
    "x, y = dset[np.random.choice(len(dset))]\n",
    "# add the batch dimension to query image and label\n",
    "x, y = x[None], y[None]\n",
    "\n",
    "ne.plot.slices(torch.cat([x, y], dim=0).squeeze().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c49ae7",
   "metadata": {},
   "source": [
    "Now lets visually inspect the kinds of examples our network will choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_images, support_labels = select_support(support_sampler(x, y), dset, temperature, support_size)\n",
    "\n",
    "ne.plot.slices(support_images.squeeze().detach().cpu())\n",
    "ne.plot.slices(support_labels.squeeze().detach().cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniverSegTF",
   "language": "python",
   "name": "universegtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
