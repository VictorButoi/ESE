{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "\n",
    "# Results loader object does everything\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from pathlib import Path\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_base = \"01_10_24_wLabAmounts\"\n",
    "# exp_base = \"01_14_24_EnsembleAnalysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_calibrator(image_info_df):\n",
    "    # Make sure that the model_class 'Uncalibrated' is first\n",
    "    image_info_df['calibrator'] = image_info_df['calibrator'].astype('category')\n",
    "    image_info_df['calibrator'].cat.reorder_categories(['Uncalibrated', 'Temperature_Scaling', 'Vector_Scaling', 'Dirichlet_Scaling', 'LTS'])\n",
    "    # Make the category normal again\n",
    "    image_info_df['calibrator'] = image_info_df['calibrator'].astype('object')\n",
    "    return image_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "log:\n",
    "    root: /storage/vbutoi/scratch/ESE/inference\n",
    "    load_pixel_meters: True \n",
    "    remove_shared_columns: True\n",
    "    drop_nan_metric_rows: True\n",
    "    min_fg_pixels: 100\n",
    "    inference_paths:\n",
    "        - \"01_14_24_EnsembleAnalysis/WMH_Individual_Uncalibrated\"\n",
    "        - \"01_14_24_EnsembleAnalysis/WMH_Individual_TempScaling\"\n",
    "        - \"01_14_24_EnsembleAnalysis/WMH_Individual_VectorScaling\"\n",
    "        - \"01_14_24_EnsembleAnalysis/WMH_Individual_DirichletScaling\"\n",
    "        - \"01_14_24_EnsembleAnalysis/WMH_Individual_LTS\"\n",
    "        - \"01_14_24_EnsembleAnalysis/WMH_Ensemble_Uncalibrated\"\n",
    "        - \"01_14_24_EnsembleAnalysis/WMH_Ensemble_TempScaling\"\n",
    "        - \"01_14_24_EnsembleAnalysis/WMH_Ensemble_VectorScaling\"\n",
    "        - \"01_14_24_EnsembleAnalysis/WMH_Ensemble_DirichletScaling\"\n",
    "        - \"01_14_24_EnsembleAnalysis/WMH_Ensemble_LTS\"\n",
    "    \n",
    "calibration:\n",
    "    conf_interval:\n",
    "        - 0.5\n",
    "        - 1.\n",
    "    num_bins: 10\n",
    "    square_diff: False \n",
    "    neighborhood_width: 3\n",
    "\n",
    "cal_metrics:\n",
    "    - ECE:\n",
    "        _fn: ese.experiment.metrics.ece.ece_loss\n",
    "    - CW_ECE:\n",
    "        _fn: ese.experiment.metrics.ece.cw_ece_loss\n",
    "    - Edge_ECE:\n",
    "        _fn: ese.experiment.metrics.ece.edge_ece_loss\n",
    "    - ELM:\n",
    "        _fn: ese.experiment.metrics.elm.elm_loss\n",
    "    - Foreground_ECE:\n",
    "        _fn: ese.experiment.metrics.ece.ece_loss\n",
    "        ignore_index: 0\n",
    "    - Foreground_CW_ECE:\n",
    "        _fn: ese.experiment.metrics.ece.cw_ece_loss\n",
    "        ignore_index: 0\n",
    "    - Foreground_Edge_ECE:\n",
    "        _fn: ese.experiment.metrics.ece.edge_ece_loss\n",
    "        ignore_index: 0       \n",
    "    - Foreground_ELM:\n",
    "        _fn: ese.experiment.metrics.elm.elm_loss\n",
    "        ignore_index: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.analysis.inference import load_cal_inference_stats\n",
    "\n",
    "image_info_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to do the same standardization for our df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra variable names.\n",
    "####################################################################\n",
    "\n",
    "def method_name(model_class, pretrained_model_class, pretrained_seed, ensemble, pre_softmax, combine_fn):\n",
    "    if ensemble:\n",
    "        softmax_modifier = \"pre\" if pre_softmax else \"post\"\n",
    "        method_name_string = f\"Ensemble ({combine_fn}, {softmax_modifier})\" \n",
    "    else:\n",
    "        if pretrained_model_class == \"None\":\n",
    "            method_name_string = f\"{model_class.split('.')[-1]} (seed={pretrained_seed})\"\n",
    "        else:\n",
    "            method_name_string = f\"{pretrained_model_class.split('.')[-1]} (seed={pretrained_seed})\"\n",
    "\n",
    "    return method_name_string\n",
    "\n",
    "def calibrator(model_class):\n",
    "    if \"UNet\" in model_class:\n",
    "        return \"Uncalibrated\"\n",
    "    else:\n",
    "        return model_class.split('.')[-1]\n",
    "\n",
    "def configuration(method_name, calibrator):\n",
    "    return f\"{method_name}_{calibrator}\"\n",
    "\n",
    "def model_type(ensemble):\n",
    "    return 'group' if ensemble else 'individual'\n",
    "\n",
    "def metric_type(image_metric):\n",
    "    if 'ECE' in image_metric or 'ELM' in image_metric:\n",
    "        return 'calibration'\n",
    "    else:\n",
    "        return 'quality'\n",
    "\n",
    "image_info_df.augment(metric_type)\n",
    "image_info_df.augment(method_name)\n",
    "image_info_df.augment(calibrator)\n",
    "image_info_df.augment(configuration)\n",
    "image_info_df.augment(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to see if there is any hope with having better ECE/ELM makes better ensembles. Note that this isn't a conclusive result just because the number of samples per images that are used to calculate ECE/ELM are not sufficient to get actual statistical quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First thing we have to do is calculate per slice per model configuration, the delta in performance that each configuration has between that configuration's slice performance and the average un-calibrated UNet performance on that slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_info_df = image_info_df[image_info_df['ensemble'] == False].reset_index(drop=True)\n",
    "# Group everything we need. \n",
    "group_keys = ['data_id', 'slice_idx', 'image_metric', 'calibrator', 'model_class', 'metric_type', 'pretrained_model_class'] \n",
    "average_unet_group = unet_info_df.groupby(group_keys).agg({'metric_score': 'mean', 'num_lab_0_pixels': 'mean', 'num_lab_1_pixels': 'mean'}).reset_index()\n",
    "average_unet_group['pretrained_seed'] = 'Average'\n",
    "average_unet_group['model_type'] = 'group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_name(pretrained_model_class, model_class):\n",
    "    if pretrained_model_class == \"None\":\n",
    "        return f\"{model_class.split('.')[-1]} (seed=Average)\"\n",
    "    else:\n",
    "        return f\"{pretrained_model_class.split('.')[-1]} (seed=Average)\"\n",
    "\n",
    "def configuration(method_name, calibrator):\n",
    "    return f\"{method_name}_{calibrator}\"\n",
    "\n",
    "average_unet_group.augment(method_name)\n",
    "average_unet_group.augment(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this unet group back to image info df\n",
    "image_info_df = pd.concat([image_info_df, average_unet_group], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have to add to each row a column that is the difference betweeen the row's metric_score and the metric_score corresponding to the same image metric as mean uncalibrated UNet performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter the dataframe\n",
    "average_unet_row = image_info_df[(image_info_df['pretrained_seed'] == 'Average') & (image_info_df['calibrator'] == 'Uncalibrated')]\n",
    "# assert that for the same data_id, slice_idx, and image_metric, there is only one row\n",
    "merge_unet_cols = average_unet_row.groupby(['data_id', 'slice_idx', 'image_metric']).size()\n",
    "assert merge_unet_cols.max() == 1,\\\n",
    "    f\"There should be only one row for each data_id, slice_idx, and image_metric combination, got {merge_unet_cols}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Merge based on 'image_metric', 'subject_id', and 'slice_idx'\n",
    "merge_columns = ['image_metric', 'data_id', 'slice_idx']\n",
    "merged_df = pd.merge(image_info_df, average_unet_row[merge_columns + ['metric_score']], on=merge_columns, how='left', suffixes=('', '_average_unet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate the difference\n",
    "merged_df['delta'] = merged_df['metric_score'] - merged_df['metric_score_average_unet'] # Current - Baseline\n",
    "# Drop unnecessary columns from the merged dataframe\n",
    "image_info_df = merged_df.drop(['metric_score_average_unet'], axis=1)\n",
    "# Fill the NaNs with None\n",
    "image_info_df = image_info_df.fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert here that the delta from base from the unet group is 0\n",
    "base_rows = image_info_df[(image_info_df['pretrained_seed'] == 'Average') & (image_info_df['calibrator'] == 'Uncalibrated')]\n",
    "assert (base_rows['delta'] == 0).all(),\\\n",
    "    f\"Delta from base should be 0 for the unet group, got {base_rows['delta']}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can look at trends! We want to make some scatterplots to look at relationships between calibration scores and their relative improvement over the baseline.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we want only the rows corresponding to group metrics, no longer looking at seeds.\n",
    "image_info_df = image_info_df[image_info_df['model_type'] == 'group'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table with 'metric_type' as columns\n",
    "pivot_df = image_info_df.pivot_table(\n",
    "    index=['configuration', 'method_name', 'calibrator', 'data_id', 'slice_idx'],\n",
    "    columns=['metric_type', 'image_metric'], \n",
    "    values=['metric_score', 'delta'], \n",
    "    aggfunc='mean'\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = []\n",
    "for col in pivot_df.columns.values:\n",
    "    if col[0] == 'delta':\n",
    "        new_cols.append(f'delta_{col[-1]}')\n",
    "    elif col[-1] == '':\n",
    "        new_cols.append(col[0])\n",
    "    else:\n",
    "        new_cols.append(col[-1])\n",
    "# Set the column names to be the lowest non empty level per column in the multi-index\n",
    "pivot_df.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df_sorted = sort_by_calibrator(pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at change in calibration vs change in Dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_ECE', \n",
    "    y='delta_Dice',\n",
    "    col='method_name',\n",
    "    row='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=2.5\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_Foreground-ECE', \n",
    "    y='delta_Dice',\n",
    "    col='method_name',\n",
    "    row='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=2.5\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_ELM', \n",
    "    y='delta_Dice',\n",
    "    col='method_name',\n",
    "    row='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=2.5\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_Foreground-ELM', \n",
    "    y='delta_Dice',\n",
    "    col='method_name',\n",
    "    row='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=2.5\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at change in calibration vs change in HD95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_ECE', \n",
    "    y='delta_HD95',\n",
    "    col='method_name',\n",
    "    row='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=2.5\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_Foreground-ECE', \n",
    "    y='delta_HD95',\n",
    "    col='method_name',\n",
    "    row='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=2.5\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_ELM', \n",
    "    y='delta_HD95',\n",
    "    col='method_name',\n",
    "    row='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=2.5\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_Foreground-ELM', \n",
    "    y='delta_HD95',\n",
    "    col='method_name',\n",
    "    row='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=2.5\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniverSegTF",
   "language": "python",
   "name": "universegtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
