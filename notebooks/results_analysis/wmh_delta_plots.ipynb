{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "\n",
    "# Results loader object does everything\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from pathlib import Path\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "# EXPERIMENT SETS:\n",
    "# - Calibration (no weighting): 01_18_24_All_Metrics\n",
    "# - Only Foreground Loss: 01_21_24_Foreground_Calibrators\n",
    "# - Balanced Loss: 01_21_24_Balanced_CE_Calibrators\n",
    "\n",
    "log:\n",
    "    root: /storage/vbutoi/scratch/ESE/inference\n",
    "    load_pixel_meters: True \n",
    "    remove_shared_columns: True\n",
    "    drop_nan_metric_rows: False\n",
    "    min_fg_pixels: 100\n",
    "    inference_paths:\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Individual_Uncalibrated\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Individual_TempScaling\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Individual_VectorScaling\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Individual_DirichletScaling\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Individual_LTS\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Individual_NectarScaling\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Ensemble_Uncalibrated\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Ensemble_TempScaling\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Ensemble_VectorScaling\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Ensemble_DirichletScaling\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Ensemble_LTS\"\n",
    "        - \"01_21_24_Foreground_Calibrators/WMH_Ensemble_NectarScaling\"\n",
    "    \n",
    "calibration:\n",
    "    conf_interval:\n",
    "        - 0.5\n",
    "        - 1.\n",
    "    num_bins: 10\n",
    "    square_diff: False \n",
    "    neighborhood_width: 3\n",
    "\n",
    "# cal_metrics:\n",
    "#     - ECE:\n",
    "#         _fn: ese.experiment.metrics.ece.ece_loss\n",
    "#     - CW_ECE:\n",
    "#         _fn: ese.experiment.metrics.ece.cw_ece_loss\n",
    "#     - Edge_ECE:\n",
    "#         _fn: ese.experiment.metrics.ece.edge_ece_loss\n",
    "#     - ELM:\n",
    "#         _fn: ese.experiment.metrics.elm.elm_loss\n",
    "#     - Foreground_ECE:\n",
    "#         _fn: ese.experiment.metrics.ece.ece_loss\n",
    "#         ignore_index: 0\n",
    "#     - Foreground_CW_ECE:\n",
    "#         _fn: ese.experiment.metrics.ece.cw_ece_loss\n",
    "#         ignore_index: 0\n",
    "#     - Foreground_Edge_ECE:\n",
    "#         _fn: ese.experiment.metrics.ece.edge_ece_loss\n",
    "#         ignore_index: 0       \n",
    "#     - Foreground_ELM:\n",
    "#         _fn: ese.experiment.metrics.elm.elm_loss\n",
    "#         ignore_index: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.analysis.inference import load_cal_inference_stats\n",
    "\n",
    "image_info_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to see if there is any hope with having better ECE/ELM makes better ensembles. Note that this isn't a conclusive result just because the number of samples per images that are used to calculate ECE/ELM are not sufficient to get actual statistical quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to add to each row a column that is the difference betweeen the row's metric_score and the metric_score corresponding to the same image metric as mean uncalibrated UNet performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the rows corresponding to a unet aveages across multiple seeds with no calibration.\n",
    "average_unet_row = image_info_df[(image_info_df['pretrained_seed'] == 'Average') & (image_info_df['calibrator'] == 'Uncalibrated')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_datapoint_cols = ['data_id', 'slice_idx', 'image_metric', 'groupavg_image_metric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that for each datapoint we only have one average unet row.\n",
    "num_avg_unets_per_datapoint = average_unet_row.groupby(unique_datapoint_cols).size()\n",
    "assert num_avg_unets_per_datapoint.max() == 1,\\\n",
    "    f\"There should be only one row for each data_id, slice_idx, image_metric, and groupavg image metric combination, got {num_avg_unets_per_datapoint}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are no NaNs in the average UNet rows.\n",
    "assert average_unet_row['metric_score'].isna().sum() == 0, \"There should be no NaNs in metric_score of UNet rows.\"\n",
    "assert average_unet_row['groupavg_metric_score'].isna().sum() == 0, \"There should be no NaNs in groupavg_metric_score of UNet rows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Merge based on 'image_metric', 'subject_id', and 'slice_idx'\n",
    "info_df_w_avg_unet_cols = pd.merge(\n",
    "    image_info_df, \n",
    "    average_unet_row[unique_datapoint_cols + ['metric_score', 'groupavg_metric_score']], \n",
    "    on=unique_datapoint_cols, \n",
    "    how='left', \n",
    "    suffixes=('', '_average_unet')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate the difference\n",
    "info_df_w_avg_unet_cols['metric_delta'] = info_df_w_avg_unet_cols['metric_score'] - info_df_w_avg_unet_cols['metric_score_average_unet'] # Current - Baseline\n",
    "info_df_w_avg_unet_cols['groupavg_metric_delta'] = info_df_w_avg_unet_cols['groupavg_metric_score'] - info_df_w_avg_unet_cols['groupavg_metric_score_average_unet'] # Current - Baseline\n",
    "# Drop those columns\n",
    "info_df_w_delta = info_df_w_avg_unet_cols.drop(columns=['metric_score_average_unet', 'groupavg_metric_score_average_unet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_df_w_delta = info_df_w_delta.dropna(subset=['metric_delta', 'groupavg_metric_delta'])\n",
    "\n",
    "# We want to drop all of the rows of (data_id, slice_idx) that have NaNs in the delta columns.\n",
    "# This means that the average unet row was not present for that (data_id, slice_idx) combination.\n",
    "\n",
    "# Get the rows that have NaNs in the delta columns\n",
    "# nan_delta_rows = info_df_w_delta[info_df_w_delta['metric_delta'].isna() | info_df_w_delta['groupavg_metric_delta'].isna()]\n",
    "# # Drop the nan delta rows\n",
    "# info_df_w_delta = info_df_w_delta.drop(nan_delta_rows.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can look at trends! We want to make some scatterplots to look at relationships between calibration scores and their relative improvement over the baseline.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we want only the rows corresponding to group metrics, no longer looking at seeds.\n",
    "grouped_models_df = info_df_w_delta[info_df_w_delta['model_type'] == 'group'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECKS, MAKE SURE THAT FOR CALIBRATORS UNCALIBRATED, TEMPERATURE_SCALING, LTS\n",
    "for calibrator in [\"Uncalibrated\", \"Temperature_Scaling\", \"LTS\"]:\n",
    "    unique_qual_metrics = grouped_models_df[grouped_models_df['metric_type'] == 'quality']['image_metric'].unique()\n",
    "    for quality_metric in unique_qual_metrics:\n",
    "        # Checkign that the delta is 0 for the calibrator and the quality_metric\n",
    "        rows = grouped_models_df[\n",
    "            (grouped_models_df['calibrator'] == calibrator) & \n",
    "            (grouped_models_df['image_metric'] == quality_metric) &\n",
    "            (grouped_models_df['method_name'] == 'UNet (seed=Average)')\n",
    "        ]\n",
    "        assert (rows['metric_delta'] == 0).all(),\\\n",
    "            f\"Delta from base should be 0 for the calibrator {calibrator} and the quality metric {quality_metric}, got {rows['metric_delta']}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a bunch of new rows where the image_metric is the groupavg_image_metric and the metric_score is the groupavg_metric_score and the metric_delta is the groupavg_metric_delta\n",
    "groupavg_rows = grouped_models_df.copy()\n",
    "groupavg_rows['image_metric'] = groupavg_rows['groupavg_image_metric']\n",
    "groupavg_rows['metric_score'] = groupavg_rows['groupavg_metric_score']\n",
    "groupavg_rows['metric_delta'] = groupavg_rows['groupavg_metric_delta']\n",
    "# Drop the groupavg columns\n",
    "standard_image_rows = grouped_models_df.drop(columns=['groupavg_image_metric', 'groupavg_metric_score', 'groupavg_metric_delta']) \n",
    "groupavg_rows = groupavg_rows.drop(columns=['groupavg_image_metric', 'groupavg_metric_score', 'groupavg_metric_delta'])\n",
    "# Concatenate the two\n",
    "grouped_models_df = pd.concat([standard_image_rows, groupavg_rows], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table with 'metric_type' as columns\n",
    "pivot_grouped_models_df = grouped_models_df.pivot_table(\n",
    "    index=['configuration', 'method_name', 'calibrator', 'data_id', 'slice_idx'],\n",
    "    values=['metric_score', 'metric_delta'], \n",
    "    columns=['metric_type', 'image_metric'], \n",
    "    aggfunc='mean'\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy so that we can modify the column names\n",
    "pivot_perf_per_datpoint = pivot_grouped_models_df.copy()\n",
    "# Make new column names.\n",
    "new_cols = []\n",
    "for col in pivot_grouped_models_df.columns.values:\n",
    "    if col[0] == 'metric_delta':\n",
    "        new_cols.append(f'delta_{col[-1]}')\n",
    "    elif col[-1] == '':\n",
    "        new_cols.append(col[0])\n",
    "    else:\n",
    "        new_cols.append(col[-1])\n",
    "# Set the column names to be the lowest non empty level per column in the multi-index\n",
    "pivot_perf_per_datpoint.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_perf_per_datpoint.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to sort the pivot_df so that the order of the method names is\n",
    "# UNet (seed=Average), Ensemble (mean, logits), Ensemble (mean, probs),\n",
    "# and the order of the calibrators is Uncalibrated, Temperature Scaling, LTS, Vector Scaling, Dirichlet Scaling\n",
    "method_order_name = ['UNet (seed=Average)', 'Ensemble (mean, logits)', 'Ensemble (mean, probs)']\n",
    "calibrator_order_name = ['Uncalibrated', 'Temperature_Scaling', 'LTS', 'NECTAR_Scaling', 'Vector_Scaling', 'Dirichlet_Scaling']\n",
    "# Sort the methods\n",
    "pivot_perf_per_datpoint['method_name'] = pivot_perf_per_datpoint['method_name'].astype('category')\n",
    "pivot_perf_per_datpoint['method_name'] = pivot_perf_per_datpoint['method_name'].cat.reorder_categories(method_order_name)\n",
    "# Sort the calibrators\n",
    "pivot_perf_per_datpoint['calibrator'] = pivot_perf_per_datpoint['calibrator'].astype('category')\n",
    "pivot_perf_per_datpoint['calibrator'] = pivot_perf_per_datpoint['calibrator'].cat.reorder_categories(calibrator_order_name)\n",
    "# Sort the dataframe\n",
    "sorted_pivot_perf_per_datpoint = pivot_perf_per_datpoint.sort_values(by=['method_name', 'calibrator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at change in *predicted ensemble* calibration vs change in Dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that each subplot will have the same number of datapoints on it.\n",
    "num_per_config = sorted_pivot_perf_per_datpoint.groupby(['method_name', 'calibrator']).size()\n",
    "assert len(num_per_config.unique()) == 1,\\\n",
    "    f\"Each subplot should have the same number of datapoints on it, got {num_per_config}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_per_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.analysis.plot_utils import plot_method_vs_calibrator_scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for foreground_option in ['', 'Foreground-']:\n",
    "    for group_option in ['', 'GroupAvg_']:\n",
    "        x_metric_name = f'delta_{group_option}Image_{foreground_option}Edge-ECE'\n",
    "        y_metric_name = f'delta_Dice'\n",
    "        # Plot the relationship between the two metrics\n",
    "        plot_method_vs_calibrator_scatterplots(\n",
    "            df=sorted_pivot_perf_per_datpoint, \n",
    "            x=x_metric_name, \n",
    "            y=y_metric_name,\n",
    "            sharex=False,\n",
    "            sharey=False,\n",
    "            height=5\n",
    "        )\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for foreground_option in ['', 'Foreground-']:\n",
    "    for group_option in ['', 'GroupAvg_']:\n",
    "        x_metric_name = f'delta_{group_option}Image_{foreground_option}Edge-ECE'\n",
    "        y_metric_name = f'delta_Dice'\n",
    "        # Bin the predictions\n",
    "        num_bins = 100\n",
    "        # Make a copy of the dataframe\n",
    "        pivot_df_copy = sorted_pivot_perf_per_datpoint.copy()\n",
    "        # Use pandas qcut to create quantile-based bins and calculate average x and y values within each bin\n",
    "        pivot_df_copy['bin'] = pd.qcut(pivot_df_copy[x_metric_name].rank(method='first'), q=num_bins, labels=False)\n",
    "        binned_pivot_per_datapoint = pivot_df_copy.groupby(['bin', 'method_name', 'calibrator']).agg({\n",
    "            x_metric_name: 'mean', \n",
    "            y_metric_name: 'mean'\n",
    "            }).reset_index()\n",
    "        # Assert that the number of points in the plot is the same\n",
    "        num_per_config = binned_pivot_per_datapoint.groupby(['method_name', 'calibrator']).size()\n",
    "        assert num_per_config.max() == num_per_config.min(),\\\n",
    "            f\"Each subplot should have the same number of datapoints on it, got {num_per_config}.\"\n",
    "        # Plot the relationship between the two metrics\n",
    "        plot_method_vs_calibrator_scatterplots(\n",
    "            df=binned_pivot_per_datapoint, \n",
    "            x=x_metric_name, \n",
    "            y=y_metric_name,\n",
    "            sharex=False,\n",
    "            sharey=False,\n",
    "            height=5\n",
    "        )\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniverSegTF",
   "language": "python",
   "name": "universegtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
