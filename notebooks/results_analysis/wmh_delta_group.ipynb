{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "\n",
    "# Results loader object does everything\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from pathlib import Path\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "log:\n",
    "    root: /storage/vbutoi/scratch/ESE/inference\n",
    "    load_pixel_meters: True \n",
    "    remove_shared_columns: True\n",
    "    drop_nan_metric_rows: True\n",
    "    min_fg_pixels: 100\n",
    "    inference_paths:\n",
    "        - \"01_17_24_Ignore_None/WMH_Individual_Uncalibrated\"\n",
    "        - \"01_17_24_Ignore_None/WMH_Individual_TempScaling\"\n",
    "        - \"01_17_24_Ignore_None/WMH_Individual_VectorScaling\"\n",
    "        - \"01_17_24_Ignore_None/WMH_Individual_DirichletScaling\"\n",
    "        - \"01_17_24_Ignore_None/WMH_Individual_LTS\"\n",
    "        - \"01_17_24_Ignore_None/WMH_Ensemble_Uncalibrated\"\n",
    "        - \"01_17_24_Ignore_None/WMH_Ensemble_TempScaling\"\n",
    "        - \"01_17_24_Ignore_None/WMH_Ensemble_VectorScaling\"\n",
    "        - \"01_17_24_Ignore_None/WMH_Ensemble_DirichletScaling\"\n",
    "        - \"01_17_24_Ignore_None/WMH_Ensemble_LTS\"\n",
    "    \n",
    "calibration:\n",
    "    conf_interval:\n",
    "        - 0.5\n",
    "        - 1.\n",
    "    num_bins: 10\n",
    "    square_diff: False \n",
    "    neighborhood_width: 3\n",
    "\n",
    "cal_metrics:\n",
    "    - ECE:\n",
    "        _fn: ese.experiment.metrics.ece.ece_loss\n",
    "    - CW_ECE:\n",
    "        _fn: ese.experiment.metrics.ece.cw_ece_loss\n",
    "    - Edge_ECE:\n",
    "        _fn: ese.experiment.metrics.ece.edge_ece_loss\n",
    "    - ELM:\n",
    "        _fn: ese.experiment.metrics.elm.elm_loss\n",
    "    - Foreground_ECE:\n",
    "        _fn: ese.experiment.metrics.ece.ece_loss\n",
    "        ignore_index: 0\n",
    "    - Foreground_CW_ECE:\n",
    "        _fn: ese.experiment.metrics.ece.cw_ece_loss\n",
    "        ignore_index: 0\n",
    "    - Foreground_Edge_ECE:\n",
    "        _fn: ese.experiment.metrics.ece.edge_ece_loss\n",
    "        ignore_index: 0       \n",
    "    - Foreground_ELM:\n",
    "        _fn: ese.experiment.metrics.elm.elm_loss\n",
    "        ignore_index: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.analysis.inference import load_cal_inference_stats\n",
    "\n",
    "image_info_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to do the same standardization for our df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra variable names.\n",
    "####################################################################\n",
    "\n",
    "def method_name(model_class, pretrained_model_class, pretrained_seed, ensemble, pre_softmax, combine_fn):\n",
    "    if ensemble:\n",
    "        softmax_modifier = \"logits\" if pre_softmax else \"probs\"\n",
    "        method_name_string = f\"Ensemble ({combine_fn}, {softmax_modifier})\" \n",
    "    else:\n",
    "        if pretrained_model_class == \"None\":\n",
    "            method_name_string = f\"{model_class.split('.')[-1]} (seed={pretrained_seed})\"\n",
    "        else:\n",
    "            method_name_string = f\"{pretrained_model_class.split('.')[-1]} (seed={pretrained_seed})\"\n",
    "\n",
    "    return method_name_string\n",
    "\n",
    "def calibrator(model_class):\n",
    "    if \"UNet\" in model_class:\n",
    "        return \"Uncalibrated\"\n",
    "    else:\n",
    "        return model_class.split('.')[-1]\n",
    "\n",
    "def configuration(method_name, calibrator):\n",
    "    return f\"{method_name}_{calibrator}\"\n",
    "\n",
    "def model_type(ensemble):\n",
    "    return 'group' if ensemble else 'individual'\n",
    "\n",
    "def metric_type(image_metric):\n",
    "    if 'ECE' in image_metric or 'ELM' in image_metric:\n",
    "        return 'calibration'\n",
    "    else:\n",
    "        return 'quality'\n",
    "\n",
    "image_info_df.augment(metric_type)\n",
    "image_info_df.augment(method_name)\n",
    "image_info_df.augment(calibrator)\n",
    "image_info_df.augment(configuration)\n",
    "image_info_df.augment(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to see if there is any hope with having better ECE/ELM makes better ensembles. Note that this isn't a conclusive result just because the number of samples per images that are used to calculate ECE/ELM are not sufficient to get actual statistical quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First thing we have to do is calculate per slice per model configuration, the delta in performance that each configuration has between that configuration's slice performance and the average un-calibrated UNet performance on that slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_info_df = image_info_df[image_info_df['ensemble'] == False].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group everything we need. \n",
    "group_keys = ['data_id', 'slice_idx', 'image_metric', 'calibrator', 'model_class', 'metric_type', 'pretrained_model_class', 'ensemble'] \n",
    "average_unet_group = unet_info_df.groupby(group_keys).agg({\n",
    "    'metric_score': 'mean', \n",
    "    'num_lab_0_pixels': 'mean', # Weird to do this tbh.\n",
    "    'num_lab_1_pixels': 'mean'\n",
    "    }).reset_index()\n",
    "# Set some useful variables.\n",
    "average_unet_group['pretrained_seed'] = 'Average'\n",
    "average_unet_group['model_type'] = 'group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_name(pretrained_model_class, model_class):\n",
    "    if pretrained_model_class == \"None\":\n",
    "        return f\"{model_class.split('.')[-1]} (seed=Average)\"\n",
    "    else:\n",
    "        return f\"{pretrained_model_class.split('.')[-1]} (seed=Average)\"\n",
    "\n",
    "def configuration(method_name, calibrator):\n",
    "    return f\"{method_name}_{calibrator}\"\n",
    "\n",
    "average_unet_group.augment(method_name)\n",
    "average_unet_group.augment(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this unet group back to image info df\n",
    "image_info_df = pd.concat([image_info_df, average_unet_group], axis=0, ignore_index=True)\n",
    "\n",
    "def groupavg_image_metric(ensemble, groupavg_image_metric, image_metric):\n",
    "    if ensemble:\n",
    "        return groupavg_image_metric\n",
    "    else:\n",
    "        return f\"GroupAvg_{image_metric}\"\n",
    "\n",
    "def groupavg_metric_score(ensemble, groupavg_metric_score, metric_score):\n",
    "    if ensemble:\n",
    "        return groupavg_metric_score\n",
    "    else:\n",
    "        return metric_score\n",
    "\n",
    "image_info_df.augment(groupavg_image_metric)\n",
    "image_info_df.augment(groupavg_metric_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have to add to each row a column that is the difference betweeen the row's metric_score and the metric_score corresponding to the same image metric as mean uncalibrated UNet performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter the dataframe\n",
    "average_unet_row = image_info_df[(image_info_df['pretrained_seed'] == 'Average') & (image_info_df['calibrator'] == 'Uncalibrated')]\n",
    "# assert that for the same data_id, slice_idx, and image_metric, there is only one row\n",
    "merge_unet_cols = average_unet_row.groupby(['data_id', 'slice_idx', 'image_metric', 'groupavg_image_metric']).size()\n",
    "assert merge_unet_cols.max() == 1,\\\n",
    "    f\"There should be only one row for each data_id, slice_idx, image_metric, and groupavg image metric combination, got {merge_unet_cols}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Merge based on 'image_metric', 'subject_id', and 'slice_idx'\n",
    "merge_columns = ['image_metric', 'groupavg_image_metric', 'data_id', 'slice_idx']\n",
    "merged_df = pd.merge(\n",
    "    image_info_df, \n",
    "    average_unet_row[merge_columns + ['metric_score', 'groupavg_metric_score']], \n",
    "    on=merge_columns, \n",
    "    how='left', \n",
    "    suffixes=('', '_average_unet')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate the difference\n",
    "merged_df['metric_delta'] = merged_df['metric_score'] - merged_df['metric_score_average_unet'] # Current - Baseline\n",
    "merged_df['groupavg_metric_delta'] = merged_df['groupavg_metric_score'] - merged_df['groupavg_metric_score_average_unet'] # Current - Baseline\n",
    "# Fill the NaNs with None\n",
    "image_info_df = merged_df.fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert here that the delta from base from the unet group is 0\n",
    "base_rows = image_info_df[(image_info_df['pretrained_seed'] == 'Average') & (image_info_df['calibrator'] == 'Uncalibrated')]\n",
    "assert (base_rows['metric_delta'] == 0).all(),\\\n",
    "    f\"Delta from base should be 0 for the unet group, got {base_rows['metric_delta']}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can look at trends! We want to make some scatterplots to look at relationships between calibration scores and their relative improvement over the baseline.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we want only the rows corresponding to group metrics, no longer looking at seeds.\n",
    "image_info_df = image_info_df[image_info_df['model_type'] == 'group'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop through all unique quality metrics\n",
    "# for quality_metric in image_info_df[image_info_df['metric_type'] == 'quality']['image_metric'].unique():\n",
    "#     for calibrator in ['Uncalibrated', 'Temperature_Scaling', 'LTS']:\n",
    "#         # Ensure that the delta for the rows with the same configuration is 0\n",
    "#         qual_cal_rows = image_info_df[(image_info_df['image_metric'] == quality_metric) & (image_info_df['calibrator'] == calibrator)]\n",
    "#         print(qual_cal_rows['metric_delta'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table with 'metric_type' as columns\n",
    "pivot_df_raw = image_info_df.pivot_table(\n",
    "    index=['configuration', 'method_name', 'calibrator', 'data_id', 'slice_idx'],\n",
    "    values=['metric_score', 'metric_delta'], \n",
    "    columns=['metric_type', 'image_metric'], \n",
    "    aggfunc='mean'\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy so that we can modify the column names\n",
    "pivot_df = pivot_df_raw.copy()\n",
    "# Make new column names.\n",
    "new_cols = []\n",
    "for col in pivot_df.columns.values:\n",
    "    if col[0] == 'metric_delta':\n",
    "        new_cols.append(f'delta_{col[-1]}')\n",
    "    elif col[-1] == '':\n",
    "        new_cols.append(col[0])\n",
    "    else:\n",
    "        new_cols.append(col[-1])\n",
    "# Set the column names to be the lowest non empty level per column in the multi-index\n",
    "pivot_df.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to sort the pivot_df so that the order of the method names is\n",
    "# UNet (seed=Average), Ensemble (mean, logits), Ensemble (mean, probs),\n",
    "# and the order of the calibrators is Uncalibrated, Temperature Scaling, LTS, Vector Scaling, Dirichlet Scaling\n",
    "method_order_name = ['UNet (seed=Average)', 'Ensemble (mean, logits)', 'Ensemble (mean, probs)']\n",
    "calibrator_order_name = ['Uncalibrated', 'Temperature_Scaling', 'LTS', 'Vector_Scaling', 'Dirichlet_Scaling']\n",
    "# Sort the methods\n",
    "pivot_df['method_name'] = pivot_df['method_name'].astype('category')\n",
    "pivot_df['method_name'] = pivot_df['method_name'].cat.reorder_categories(method_order_name)\n",
    "# Sort the calibrators\n",
    "pivot_df['calibrator'] = pivot_df['calibrator'].astype('category')\n",
    "pivot_df['calibrator'] = pivot_df['calibrator'].cat.reorder_categories(calibrator_order_name)\n",
    "# Sort the dataframe\n",
    "pivot_df_sorted = pivot_df.sort_values(by=['method_name', 'calibrator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at change in calibration vs change in Dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_ECE', \n",
    "    y='delta_Dice',\n",
    "    row='method_name',\n",
    "    col='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    kind='scatter',\n",
    "    height=4,\n",
    "    facet_kws=dict(margin_titles=True)\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string\n",
    "g.set(xlim=(-0.006, 0.006), ylim=(-0.12, 0.12))\n",
    "# Prevent overlap with rotation.\n",
    "loc, labels = plt.xticks()\n",
    "g.set_xticklabels(labels, rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_Foreground-ECE', \n",
    "    y='delta_Dice',\n",
    "    row='method_name',\n",
    "    col='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=4,\n",
    "    facet_kws=dict(margin_titles=True)\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string\n",
    "# Set the x axis to be between -0.005 and 0.005\n",
    "g.set(xlim=(-0.15, 0.15), ylim=(-0.12, 0.12))\n",
    "# Prevent overlap with rotation.\n",
    "loc, labels = plt.xticks()\n",
    "g.set_xticklabels(labels, rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_ELM', \n",
    "    y='delta_Dice',\n",
    "    row='method_name',\n",
    "    col='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=4,\n",
    "    facet_kws=dict(margin_titles=True)\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string\n",
    "# Set the x axis to be between -0.005 and 0.005\n",
    "g.set(xlim=(-0.006, 0.006), ylim=(-0.12, 0.12))\n",
    "# Prevent overlap with rotation.\n",
    "loc, labels = plt.xticks()\n",
    "g.set_xticklabels(labels, rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_Foreground-ELM', \n",
    "    y='delta_Dice',\n",
    "    row='method_name',\n",
    "    col='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=4,\n",
    "    facet_kws=dict(margin_titles=True)\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string\n",
    "# Set the x axis to be between -0.005 and 0.005\n",
    "g.set(xlim=(-0.15, 0.15), ylim=(-0.12, 0.12))\n",
    "# Prevent overlap with rotation.\n",
    "loc, labels = plt.xticks()\n",
    "g.set_xticklabels(labels, rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at change in calibration vs change in HD95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_ECE', \n",
    "    y='delta_HD95',\n",
    "    row='method_name',\n",
    "    col='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    kind='scatter',\n",
    "    height=4,\n",
    "    facet_kws=dict(margin_titles=True)\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string\n",
    "g.set(xlim=(-0.004, 0.004))\n",
    "# Prevent overlap with rotation.\n",
    "loc, labels = plt.xticks()\n",
    "g.set_xticklabels(labels, rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_Foreground-ECE', \n",
    "    y='delta_HD95',\n",
    "    row='method_name',\n",
    "    col='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=4,\n",
    "    facet_kws=dict(margin_titles=True)\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string\n",
    "# Set the x axis to be between -0.005 and 0.005\n",
    "g.set(xlim=(-0.18, 0.18))\n",
    "# Prevent overlap with rotation.\n",
    "loc, labels = plt.xticks()\n",
    "g.set_xticklabels(labels, rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_ELM', \n",
    "    y='delta_HD95',\n",
    "    row='method_name',\n",
    "    col='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=4,\n",
    "    facet_kws=dict(margin_titles=True)\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string\n",
    "# Set the x axis to be between -0.005 and 0.005\n",
    "g.set(xlim=(-0.004, 0.004))\n",
    "# Prevent overlap with rotation.\n",
    "loc, labels = plt.xticks()\n",
    "g.set_xticklabels(labels, rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=pivot_df_sorted, \n",
    "    x='delta_Image_Foreground-ELM', \n",
    "    y='delta_HD95',\n",
    "    row='method_name',\n",
    "    col='calibrator',\n",
    "    hue='method_name',\n",
    "    style='calibrator',\n",
    "    height=4,\n",
    "    facet_kws=dict(margin_titles=True)\n",
    "    )\n",
    "g.set_titles(\"\")  # Set titles to empty string\n",
    "# Set the x axis to be between -0.005 and 0.005\n",
    "g.set(xlim=(-0.15, 0.15))\n",
    "# Prevent overlap with rotation.\n",
    "loc, labels = plt.xticks()\n",
    "g.set_xticklabels(labels, rotation=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniverSegTF",
   "language": "python",
   "name": "universegtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
