{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from ese.experiment.experiment import CalibrationExperiment\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3' \n",
    "\n",
    "%load_ext yamlmagic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results loader object does everything\n",
    "rs = ResultsLoader()\n",
    "root = \"/storage/vbutoi/scratch/ESE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{root}/WMH_aug_runs\"\n",
    "\n",
    "dfc = rs.load_configs(\n",
    "    path,\n",
    "    properties=False,\n",
    ")\n",
    "\n",
    "df = rs.load_metrics(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_exp = rs.get_experiment(\n",
    "    df=df,\n",
    "    exp_class=CalibrationExperiment,\n",
    "    metric=\"val-dice_score\",\n",
    "    checkpoint=\"max-val-dice_score\",\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_exp.vis_loss_curves(height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml dataset_cfg \n",
    "\n",
    "_class: ese.experiment.datasets.WMH\n",
    "annotator: observer_o12\n",
    "axis: 0\n",
    "split: val \n",
    "num_slices: 1\n",
    "slicing: midslice \n",
    "task: Amsterdam \n",
    "version: 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ionpy.experiment.util import absolute_import\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_cls = absolute_import(dataset_cfg.pop(\"_class\"))\n",
    "WMH_Dataset = dataset_cls(**dataset_cfg)\n",
    "wmh_dataloader = DataLoader(WMH_Dataset, batch_size=1, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ionpy.util.torchutils import to_device\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from ionpy.metrics import dice_score\n",
    "import einops\n",
    "\n",
    "def get_dataset_perf(\n",
    "        exp, \n",
    "        dataloader, \n",
    "        background_threshold=0.0001\n",
    "        ):\n",
    "\n",
    "    items = []\n",
    "    with torch.no_grad():\n",
    "        for _, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            \n",
    "            # Get your image label pair and define some regions.\n",
    "            x, y = to_device(batch, exp.device)\n",
    "            \n",
    "            # Reshape to a good size\n",
    "            x = einops.rearrange(x, \"b c h w -> (b c) 1 h w\")\n",
    "            y = einops.rearrange(y, \"b c h w -> (b c) 1 h w\")\n",
    "                             \n",
    "            yhat = exp.model(x)  \n",
    "            fore_regions = (y==1)\n",
    "            back_regions = (y==0)\n",
    "            \n",
    "            # Extract predictions\n",
    "            soft_foreground_pred = torch.sigmoid(yhat)\n",
    "            # If you want to eliminate background pixels, do so\n",
    "            if background_threshold > 0:\n",
    "                soft_foreground_pred[soft_foreground_pred < background_threshold] = 0\n",
    "\n",
    "            soft_background_pred = 1 - soft_foreground_pred \n",
    "\n",
    "            hard_foreground_pred = (soft_foreground_pred > 0.5).float()\n",
    "            hard_background_pred = 1 - hard_foreground_pred\n",
    "\n",
    "            # Compute metrics\n",
    "            dice_met = np.round(dice_score(yhat, y).cpu().numpy(), 3)\n",
    "            loss_image = (y - soft_foreground_pred) # Not really all that meaningful, just looking at per-pixel differences (can be thought of as soft accuracy).\n",
    "\n",
    "            ## Calculate calibration image.\n",
    "\n",
    "            # The calibration image is define as PER-PIXEL the difference between (accu(P) - conf(P)).\n",
    "            calibration_image = torch.zeros_like(loss_image)\n",
    "            foreground_accuracy = (y == hard_foreground_pred).float()\n",
    "            background_accuracy = ((1 - y) == hard_background_pred).float()\n",
    "\n",
    "            # Set the regions of the image corresponding to groundtruth label.\n",
    "            calibration_image[fore_regions] = (foreground_accuracy - soft_foreground_pred).abs()[fore_regions]\n",
    "            calibration_image[back_regions] = (background_accuracy - soft_background_pred).abs()[back_regions]\n",
    "            \n",
    "            ## Finally, we want to calculate two tensors (one for each class) that consists of two channels:\n",
    "            # 1. The match of the prediction\n",
    "            # 2. The confidence of the prediction\n",
    "            pix_info = {\n",
    "                \"foreground\": torch.stack([foreground_accuracy, soft_foreground_pred], dim=0).view(2, -1).cpu().numpy(),\n",
    "                \"background\": torch.stack([background_accuracy, soft_background_pred], dim=0).view(2, -1).cpu().numpy()\n",
    "            }\n",
    "\n",
    "            # Wrap it in an item\n",
    "            items.append({\n",
    "                \"image\": x.cpu().squeeze(),\n",
    "                \"label\": y.cpu().squeeze(),\n",
    "                \"soft_pred\": soft_foreground_pred.cpu().squeeze(),\n",
    "                \"hard_pred\": hard_foreground_pred.cpu().squeeze(),\n",
    "                \"dice_score\": dice_met,\n",
    "                \"loss\": loss_image.cpu().squeeze(),\n",
    "                \"calibration err\": calibration_image.cpu().squeeze(),\n",
    "                \"pix_info\": pix_info\n",
    "            })\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_perf is a dict where each item is the subj id\n",
    "# with the y, ypred, yloss, ydice\n",
    "predictions_dict = get_dataset_perf(\n",
    "    exp=best_exp, \n",
    "    dataloader=wmh_dataloader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(perf_dict):\n",
    "    # We are going to visualize 5 things here\n",
    "    # - The image\n",
    "    # - The ground truth\n",
    "    # - The hard prediction (with loss above it)\n",
    "    # - The delta between the ground truth and the soft prediction\n",
    "    # - The Calibration error (Acc - Soft Pred)\n",
    "\n",
    "    color_dict = {\n",
    "        \"loss\": \"twilight\",\n",
    "        \"calibration err\": \"plasma\",\n",
    "    }\n",
    "\n",
    "    skip_keys = [\"pix_info\"]\n",
    "\n",
    "    for subj in perf_dict:\n",
    "\n",
    "        # Create the subplots for this subject\n",
    "        num_keys = len(subj.keys()) - len(skip_keys)\n",
    "        f, axarr = plt.subplots(1, num_keys, figsize=(5 * num_keys, 5))\n",
    "\n",
    "        for k_idx, key in enumerate(subj.keys()):\n",
    "            if key not in skip_keys:\n",
    "                if key in color_dict:\n",
    "                    im = axarr[k_idx].imshow(subj[key], cmap=color_dict[key])\n",
    "                else:\n",
    "                    im = axarr[k_idx].imshow(subj[key], cmap=\"gray\")\n",
    "                axarr[k_idx].axis(\"off\")\n",
    "                axarr[k_idx].set_title(key)\n",
    "                f.colorbar(im, ax=axarr[k_idx])\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Let's look at how the ECE looks like if we evaluate over subjects, and what the distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.metrics import ece_score \n",
    "\n",
    "ece_foreground_scores = [ece_score(bins=torch.from_numpy(np.linspace(0.5, 1, 10)[:-1]),\n",
    "                             confidences=torch.from_numpy(subj[\"pix_info\"][\"foreground\"][1]), \n",
    "                             accuracies=torch.from_numpy(subj[\"pix_info\"][\"foreground\"][0])) for subj in predictions_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ece_foreground_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(ece_foreground_scores, bins=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Expected Semantic Error (ESE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.analysis.diagrams import subject_plot\n",
    "\n",
    "subject_plot(\n",
    "    subject_dict=predictions_dict,\n",
    "    num_bins=10,\n",
    "    show_bin_amounts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.analysis.diagrams import aggregate_plot\n",
    "\n",
    "aggregate_plot(\n",
    "    subject_dict=predictions_dict,\n",
    "    num_bins=10,\n",
    "    metric=\"ESE\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniverSegTF",
   "language": "python",
   "name": "universegtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
