{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from ese.experiment.experiment import CalibrationExperiment\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3' \n",
    "\n",
    "%load_ext yamlmagic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results loader object does everything\n",
    "rs = ResultsLoader()\n",
    "root = \"/storage/vbutoi/scratch/ESE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{root}/WMH_aug_runs\"\n",
    "\n",
    "dfc = rs.load_configs(\n",
    "    path,\n",
    "    properties=False,\n",
    ")\n",
    "\n",
    "df = rs.load_metrics(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_exp = rs.get_experiment(\n",
    "    df=df,\n",
    "    exp_class=CalibrationExperiment,\n",
    "    metric=\"val-dice_score\",\n",
    "    checkpoint=\"max-val-dice_score\",\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_exp.vis_loss_curves(height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml dataset_cfg \n",
    "\n",
    "_class: ese.experiment.datasets.WMH\n",
    "annotator: observer_o12\n",
    "axis: 0\n",
    "split: train \n",
    "num_slices: 1\n",
    "slicing: midslice \n",
    "task: Amsterdam \n",
    "version: 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ionpy.experiment.util import absolute_import\n",
    "from torch.utils.data import DataLoader\n",
    "from ionpy.util.torchutils import to_device\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from ionpy.metrics import dice_score\n",
    "\n",
    "def get_dataset_perf(exp, dataset_config):\n",
    "    data_cfg = dataset_config.to_dict()\n",
    "    dataset_cls = absolute_import(data_cfg.pop(\"_class\"))\n",
    "    Dataset = dataset_cls(**data_cfg)\n",
    "    dataloader = DataLoader(Dataset, batch_size=1, shuffle=False, drop_last=False)\n",
    "\n",
    "    items = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in tqdm(enumerate(dataloader)):\n",
    "            x, y = to_device(batch, exp.device)\n",
    "            yhat = exp.model(x)\n",
    "            \n",
    "            # Extract predictions\n",
    "            soft_pred = torch.sigmoid(yhat)\n",
    "            hard_pred = (soft_pred > 0.5).float()\n",
    "            background_pred = 1 - soft_pred\n",
    "\n",
    "            # Compute metrics\n",
    "            dice_met = np.round(dice_score(yhat, y).cpu().numpy(), 3)\n",
    "            loss_image = (y - soft_pred)\n",
    "\n",
    "            # Calculate calibration image\n",
    "            calibration_image = torch.zeros_like(soft_pred)\n",
    "            pred_match = (y==hard_pred).float()\n",
    "            calibration_image[hard_pred == 1] = (pred_match - soft_pred).abs()[hard_pred == 1]\n",
    "            calibration_image[hard_pred == 0] = (pred_match - background_pred).abs()[hard_pred == 0] \n",
    "\n",
    "            # We want the confidence per pixel and whether or not it was correct we will treat this \n",
    "            # as a 3D tensor which is (2 x H x W) where the first channel is the confidence and the second\n",
    "            # channel is whether or not it was correct\n",
    "            confidence_channel = torch.zeros_like(soft_pred)\n",
    "            confidence_channel[hard_pred == 1] = soft_pred[hard_pred == 1]\n",
    "            confidence_channel[hard_pred == 0] = background_pred[hard_pred == 0]\n",
    "            pixel_wise_pred = torch.stack((confidence_channel, pred_match), dim=0).view(2, -1)\n",
    "\n",
    "            # Maybe we also only want to look at the cases where pixels should be 1\n",
    "            foreground_pixels = (y==1)\n",
    "            foreground_conf = confidence_channel[foreground_pixels]\n",
    "            foreground_match = pred_match[foreground_pixels]\n",
    "            foreground_pixel_wise_pred = torch.stack((foreground_conf, foreground_match), dim=0).view(2, -1)\n",
    "\n",
    "            # Wrap it in an item\n",
    "            items.append({\n",
    "                \"image\": x.cpu().numpy().squeeze(),\n",
    "                \"label\": y.cpu().numpy().squeeze(),\n",
    "                \"pred | Dice: \" + str(dice_met): hard_pred.cpu().numpy().squeeze(),\n",
    "                \"loss\": loss_image.cpu().numpy().squeeze(),\n",
    "                \"calibration error\": calibration_image.cpu().numpy().squeeze(),\n",
    "                \"pixel_wise_pred\": pixel_wise_pred.cpu().numpy().squeeze(),\n",
    "                \"foreground_pixel_wise_pred\": foreground_pixel_wise_pred.cpu().numpy(),\n",
    "            })\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ionpy.util import Config\n",
    "\n",
    "# val_perf is a dict where each item is the subj id\n",
    "# with the y, ypred, yloss, ydice\n",
    "val_perf = get_dataset_perf(\n",
    "    exp=best_exp, \n",
    "    dataset_config=Config(dataset_cfg)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_subjs = len(val_perf)\n",
    "                       \n",
    "# We are going to visualize 5 things here\n",
    "# - The image\n",
    "# - The ground truth\n",
    "# - The hard prediction (with loss above it)\n",
    "# - The delta between the ground truth and the soft prediction\n",
    "# - The Calibration error (Acc - Soft Pred)\n",
    "\n",
    "f, axarr = plt.subplots(num_subjs, 5, figsize=(25, 5*num_subjs))\n",
    "\n",
    "color_dict = {\n",
    "    \"loss\": \"twilight\",\n",
    "    \"calibration error\": \"plasma\" \n",
    "}\n",
    "skip_keys = [\"pixel_wise_pred\", \"foreground_pixel_wise_pred\"]\n",
    "\n",
    "for i, subj in enumerate(val_perf):\n",
    "    for k_idx, key in enumerate(subj.keys()):\n",
    "        if key not in skip_keys:\n",
    "            if key in color_dict:\n",
    "                im = axarr[i, k_idx].imshow(subj[key], cmap=color_dict[key])\n",
    "            else:\n",
    "                im = axarr[i, k_idx].imshow(subj[key], cmap=\"gray\")\n",
    "            axarr[i, k_idx].axis(\"off\")\n",
    "            axarr[i, k_idx].set_title(key)\n",
    "            f.colorbar(im, ax=axarr[i, k_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECE(accuracies, confidences, num_bins=10, round_to=5):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Calibration Error (ECE) for a model.\n",
    "    Args:\n",
    "        accuracies: numpy array of calibration accuracies for each bin\n",
    "        confidences: numpy array of confidences outputted by the model\n",
    "        num_bins (int): number of confidence interval bins\n",
    "    Returns:\n",
    "        float: Expected Calibration Error\n",
    "    \"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, num_bins+1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = 0.0\n",
    "\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # Calculated |confidence - accuracy| in each bin\n",
    "        in_bin = np.logical_and(confidences >= bin_lower, confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "    return np.round(ece, round_to)\n",
    "\n",
    "\n",
    "def plot_calibration_plot(\n",
    "    accuracies,\n",
    "    confidences,\n",
    "    num_plots=6,\n",
    "    bin_multiplier=20,\n",
    "    style='bar'\n",
    "    ):\n",
    "\n",
    "    # One issue is that we want to look maybe with different bin sizes, so \n",
    "    # lets look at a range.\n",
    "    f, ax = plt.subplots(1, num_plots, figsize=(num_plots*6, 6))\n",
    "    for g_idx in range(1, num_plots+1):\n",
    "        \n",
    "        # Calulate the bins and spacing\n",
    "        interval_size = 1 / (bin_multiplier*g_idx)\n",
    "        bins = np.linspace(0, 1, bin_multiplier*g_idx + 1) # Off by one error\n",
    "\n",
    "        # For each bin, calculate the mean accuracy within the bin  \n",
    "        bar_heights = [np.mean(accuracies[(confidences >= b) & (confidences < (b + interval_size))]) for b in bins]\n",
    "        ideal_heights = bins\n",
    "\n",
    "        if style == 'bar':\n",
    "            ax[g_idx-1].bar(bins, ideal_heights, width=interval_size, color='red', alpha=0.2)\n",
    "            ax[g_idx-1].bar(bins, bar_heights, width=interval_size, color='blue')\n",
    "        elif style == 'line':\n",
    "            ax[g_idx-1].plot(bins, bar_heights)\n",
    "        else:\n",
    "            raise ValueError(\"Style must be bar or line.\")\n",
    "\n",
    "        ax[g_idx-1].plot([0, 1], [0, 1], linestyle='dotted', linewidth=2, color='gray')\n",
    "        ax[g_idx-1].set_title(f\"{20*g_idx} Bins, ECE: {ECE(accuracies, confidences, num_bins=20*g_idx)}\")\n",
    "        ax[g_idx-1].set_xlabel(\"Confidence\")\n",
    "        ax[g_idx-1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What if we mix all calibration errors from all of the pixels of all of the images? Here is that histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pixel_preds = np.concatenate([subj[\"pixel_wise_pred\"] for subj in val_perf], axis=1)\n",
    "plot_calibration_plot(accuracies=all_pixel_preds[1], confidences=all_pixel_preds[0], num_plots=5, style='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pixel_preds = np.concatenate([subj[\"pixel_wise_pred\"] for subj in val_perf], axis=1)\n",
    "plot_calibration_plot(accuracies=all_pixel_preds[1], confidences=all_pixel_preds[0], num_plots=5, style='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Let's do the same experiment EXCEPT only look at positive pixel regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pixel_preds = np.concatenate([subj[\"foreground_pixel_wise_pred\"] for subj in val_perf], axis=1)\n",
    "plot_calibration_plot(accuracies=all_pixel_preds[1], confidences=all_pixel_preds[0], num_plots=5, style='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pixel_preds = np.concatenate([subj[\"foreground_pixel_wise_pred\"] for subj in val_perf], axis=1)\n",
    "plot_calibration_plot(accuracies=all_pixel_preds[1], confidences=all_pixel_preds[0], num_plots=5, style='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Far worse!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniverSegTF",
   "language": "python",
   "name": "universegtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
