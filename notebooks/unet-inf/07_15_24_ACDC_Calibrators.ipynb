{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/vbutoi/envs/UniverSegTF/lib/python3.9/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_outputs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "\n",
    "from ese.experiment.analysis.analyze_inf import load_cal_inference_stats\n",
    "# Results loader object does everything\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from pathlib import Path\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            require(\n                [\n                    \"notebook/js/codecell\",\n                    \"codemirror/mode/yaml/yaml\"\n                ],\n                function(cc){\n                    cc.CodeCell.options_default.highlight_modes.magic_yaml = {\n                        reg: [\"^%%yaml\"]\n                    }\n                }\n            );\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "log:\n",
    "    root: /storage/vbutoi/scratch/ESE/inference\n",
    "    inference_groups: \n",
    "        - '07_15_24_ACDC_Experiment_1'\n",
    "\n",
    "options:\n",
    "    add_dice_loss_rows: True\n",
    "    drop_nan_metric_rows: True \n",
    "    remove_shared_columns: False\n",
    "    equal_rows_per_cfg_assert: False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping (datapoint, metric) pairs with NaN metric score. Dropped from 1080 -> 1080 rows.\n",
      "Finished loading inference stats.\n",
      "Log amounts: log_root                                                          log_set                                              \n",
      "/storage/vbutoi/scratch/ESE/inference/07_15_24_ACDC_Experiment_1  20240715_105437-EM7O-4255c46db24e28251aadf0c12339840b    70\n",
      "                                                                  20240715_105441-0FKA-29850f5ad58cddd465aa5c4f88b0453c    70\n",
      "                                                                  20240715_105446-MQD1-09c0aefe0a690d24c36b0173524a3a9c    70\n",
      "                                                                  20240715_105450-XSYF-f833cc86594ce90c988453066e771def    70\n",
      "                                                                  20240715_105454-9LCP-64b38586cead5861aca54bc2a2844a2f    70\n",
      "                                                                  20240715_105458-56MZ-5e41e16339ecea0f3cd3aca21d7dd5b8    70\n",
      "                                                                  20240715_105502-QYJ1-b71521611ef2a2b82c745604ec570a8e    70\n",
      "                                                                  20240715_105506-03U9-2fb1c163b6d929beca12017a65b6b332    70\n",
      "                                                                  20240715_105510-NJPB-e07bc47bfbf5b0c856f2c7b70d7f657a    70\n",
      "                                                                  20240715_105514-8RZ4-125a30f6e8f858c0104d1c2ad98214dc    70\n",
      "                                                                  20240715_105518-Z871-97108b8b3441d0d8fd19db3e42a37bc7    70\n",
      "                                                                  20240715_105525-I6FZ-df9b5b359bdc8f8bf522ce17373a9473    70\n",
      "                                                                  20240715_105529-F7IA-3a177824c26d882b433d593c36dca599    70\n",
      "                                                                  20240715_105530-4QSU-9da33ff4e0a26cabfd3b4718037f6a7b    70\n",
      "                                                                  20240715_105537-1IGU-b01aeaae4f2212a2cf8a6812982ce3e0    70\n",
      "                                                                  20240715_105538-R4N6-526b1cf1df379146f206f79078444774    70\n",
      "                                                                  20240715_105542-V6F0-e881b6e9821cc7dd3ca68f90073c3738    70\n",
      "                                                                  20240715_105546-S21X-b6ce1cea99369f4b3e564df4759c0658    70\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "inference_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg,\n",
    "    load_cached=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_hash\n",
      "image_metric\n",
      "metric_score\n",
      "batch_idx\n",
      "data_cfg_str\n",
      "label_idx\n",
      "gt_proportion\n",
      "data_id\n",
      "gt_volume\n",
      "soft_volume\n",
      "hard_volume\n",
      "new_gt_proportion\n",
      "soft_proportion\n",
      "hard_proportion\n",
      "log_set\n",
      "global_cal_metrics\n",
      "image_cal_metrics\n",
      "qual_metrics\n",
      "slice_idx\n",
      "calibrator_name\n",
      "data_class\n",
      "data_input_type\n",
      "data_label_threshold\n",
      "data_preload\n",
      "data_return_gt_proportion\n",
      "data_split\n",
      "data_version\n",
      "dataloader_batch_size\n",
      "dataloader_num_workers\n",
      "dataloader_pin_memory\n",
      "dataset_class\n",
      "dataset_augmentations\n",
      "dataset_label_threshold\n",
      "dataset_preload\n",
      "dataset_return_data_id\n",
      "dataset_return_gt_proportion\n",
      "dataset_split\n",
      "dataset_version\n",
      "experiment_dataset_name\n",
      "experiment_exp_name\n",
      "experiment_exp_root\n",
      "experiment_hard_pred_threshold\n",
      "experiment_inference_seed\n",
      "experiment_pretrained_seed\n",
      "global_calibration_neighborhood_width\n",
      "global_calibration_num_classes\n",
      "global_calibration_num_prob_bins\n",
      "local_calibration_neighborhood_width\n",
      "local_calibration_num_prob_bins\n",
      "log_gether_inference_stats\n",
      "log_log_image_stats\n",
      "log_log_interval\n",
      "log_log_pixel_stats\n",
      "log_root\n",
      "log_summary_compute_global_metrics\n",
      "log_track_label_amounts\n",
      "loss_func_class\n",
      "loss_func_batch_reduction\n",
      "loss_func_from_logits\n",
      "model_class\n",
      "model__pretrained_class\n",
      "model__type\n",
      "model_checkpoint\n",
      "model_pred_label\n",
      "model_pretrained_exp_root\n",
      "model_image_channels\n",
      "model_num_classes\n",
      "model_convs_per_block\n",
      "model_filters\n",
      "model_in_channels\n",
      "model_out_channels\n",
      "model_img_channels\n"
     ]
    }
   ],
   "source": [
    "for ikey in inference_df.keys():\n",
    "    print(ikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154706-P0G8-866b6e4b383f1d1caf616b985ae6cf63',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154822-Q109-807b8aa8e96ed7556be90774d6166275',\n",
       "       '/storage/vbutoi/scratch/ESE/training/07_09_24_ACDC_SoftDiceLoss/20240709_115941-2WD3-0a311a8965d658cbf9bcf19bfaedda25',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154642-7I27-d695e514dffaeb31523b417765693317',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154838-YCH8-4b9a9cc2b9bc9b0502cac8077a19ea3e',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154829-NGAD-7b5f197513e76c3e17b10c6a3cbbb50a',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154703-FXN2-b102814d234788debf817cb008907c65',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154650-46VJ-8a76a9e96535566b1bc0c6d9b399a4d9',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154846-U0FQ-7bd4a211175e556f5de2bf0d61181c7e',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154818-J4GC-505fd50c96f2500f100260d35801e1af',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154833-CX5U-1a4c51cc90e683f359ed3ab623c16584',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154825-7UVF-110f618b6bdccd38bfa1d78b6c4c0959',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154646-4SXP-3dc40b1f4c5ad32e2b445be8d8e7b080',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154637-7PFE-f7c2697e3b353c746d3b3e9dc60c03c9',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154658-PYUL-753a13f028df8e541e431d0149b141af',\n",
       "       '/storage/vbutoi/scratch/ESE/training/07_09_24_ACDC_PixelCELoss/20240709_120223-2YG5-4666f171cc2ff0127caa727844be2be1',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154842-BF2I-2677ef6361fb1a81e4fe196854f7fb9f',\n",
       "       '/storage/vbutoi/scratch/ESE/calibration/07_12_24_ACDC_CalibratorSet_Experiment1/20240712_154654-QO1K-bac57134bfb85cafe29d71d26724be41'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_df['model_pretrained_exp_root'].unique()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ese.experiment.losses.PixelCELoss',\n",
       "       'ese.experiment.losses.SoftDiceLoss'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_df['loss_func_class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['None', 'ese.experiment.models.calibrators.Temperature_Scaling',\n",
       "       'ese.experiment.models.unet.UNet',\n",
       "       'ese.experiment.models.calibrators.LocalTS',\n",
       "       'ese.experiment.models.calibrators.ImageBasedTS'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_df['model_class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_loss_func(model_class, loss_func_class):\n",
    "    if model_class.split('.')[-1] == \"UNet\":\n",
    "        return loss_func_class\n",
    "    else:\n",
    "        #TODO: FILL ME!\n",
    "        return None\n",
    "\n",
    "def finetune_loss_func(model_class, loss_func_class):\n",
    "    if model_class.split('.')[-1] == \"UNet\":\n",
    "        return \"None\"\n",
    "    else:\n",
    "        return loss_func_class.split('.')[-1]\n",
    "\n",
    "def posthoc_method(model_class):\n",
    "    model_class_name = model_class.split('.')[-1]\n",
    "    if model_class_name == \"None\":\n",
    "        return \"Finetune\"\n",
    "    else:\n",
    "        return model_class_name\n",
    "\n",
    "inference_df.augment(pretrain_loss_func)\n",
    "inference_df.augment(finetune_loss_func)\n",
    "inference_df.augment(posthoc_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['loss_func', 'split'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m exp_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_func\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Take these columns of the inference_df, drop other columns.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m exp_df \u001b[38;5;241m=\u001b[39m \u001b[43minference_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexp_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/pandas/core/indexes/base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['loss_func', 'split'] not in index\""
     ]
    }
   ],
   "source": [
    "# For the purpose of this experiment, we only care about a few columns in particular:\n",
    "exp_columns = [\n",
    "    \"data_id\",\n",
    "    \"loss_func\",\n",
    "    \"new_gt_proportion\", # This is after our resizing and blurring\n",
    "    \"gt_proportion\",\n",
    "    \"soft_proportion\",\n",
    "    \"hard_proportion\",\n",
    "    \"experiment_pretrained_seed\", \n",
    "    \"model_pretrained_exp_root\",\n",
    "    \"split\",\n",
    "]\n",
    "# Take these columns of the inference_df, drop other columns.\n",
    "exp_df = inference_df[exp_columns].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of examples we are evaluating on.\n",
    "len(exp_df['data_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to rename 'proportion' to 'proportion' for the sake of the experiment.\n",
    "proportion_df = exp_df.rename(columns={\n",
    "    \"gt_proportion\": \"gt proportion\",\n",
    "    \"new_gt_proportion\": \"new gt proportion\",\n",
    "    \"hard_proportion\": \"hard proportion\",\n",
    "    \"soft_proportion\": \"soft proportion\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two new columns, one for the soft proportion error and one for the hard proportion error.\n",
    "proportion_df['new gt error'] = (proportion_df['new gt proportion'] - proportion_df['gt proportion'])\n",
    "proportion_df['soft error'] = (proportion_df['soft proportion'] - proportion_df['gt proportion'])\n",
    "proportion_df['hard error'] = (proportion_df['hard proportion'] - proportion_df['gt proportion'])\n",
    "# Make the normalized metric that divides the error by the ground truth proportion.\n",
    "proportion_df['new gt relative error'] = proportion_df['new gt error'] / proportion_df['gt proportion']\n",
    "proportion_df['soft relative error'] = proportion_df['soft error'] / proportion_df['gt proportion']\n",
    "proportion_df['hard relative error'] = proportion_df['hard error'] / proportion_df['gt proportion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important for this experiment that we only consider the 'val' and 'cal' splits because we trained on the 'train' split.\n",
    "proportion_df['split'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_error_df(raw_df, groupby_keys, value_vars, var_name, value_name):\n",
    "    # Make a clone of the proportion df.\n",
    "    input_df = raw_df.copy()\n",
    "    # Melt the dataframe to have a single column for the error.\n",
    "    error_df = pd.melt(\n",
    "        input_df,\n",
    "        id_vars=groupby_keys,\n",
    "        value_vars=value_vars,\n",
    "        var_name=var_name,\n",
    "        value_name=value_name,\n",
    "    )\n",
    "    # Make some columns that are useful for plotting.\n",
    "    error_df[f'absolute {value_name}'] = error_df[value_name].abs()\n",
    "    # Return the melted dataframe.\n",
    "    return error_df\n",
    "\n",
    "\n",
    "def calibrator(model_pretrained_exp_root):\n",
    "    if \"SVLS\" in model_pretrained_exp_root:\n",
    "        return \"SVLS\"\n",
    "    else:\n",
    "        return \"Uncalibrated\"\n",
    "\n",
    "\n",
    "def process_method_names(input_df, value_name):\n",
    "    # Make a clone of the input_df\n",
    "    df = input_df.copy()\n",
    "    # Drop all the rows where calibrator != Uncalibrated AND the proportion_type is hard_proportion_error.\n",
    "    df = df[~((df['calibrator'] != 'Uncalibrated') & (df['proportion_type'] == f'hard {value_name}'))]\n",
    "    # Then we augment the proportion_type with the calibrator name.\n",
    "    def proportion_type(calibrator, loss_func, proportion_type):\n",
    "        if proportion_type in [\"new gt error\", \"new gt relative error\"]:\n",
    "            return \"New GT\"\n",
    "        elif calibrator == \"Uncalibrated\":\n",
    "            return \"Uncalibrated \" + proportion_type.split(\" \")[0] + f\" ({loss_func})\"\n",
    "        else:\n",
    "            return calibrator + \" soft\" + f\" ({loss_func})\"\n",
    "    # Finally, sort by data_id\n",
    "    df['proportion type'] = df.apply(lambda x: proportion_type(x['calibrator'], x['loss_func'], x['proportion_type']), axis=1)\n",
    "    df = df.sort_values(by=\"data_id\")\n",
    "    # Drop the duplicate rows and reset the index.\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    # Return the augmented dataframe.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, let's look at absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_df.augment(calibrator)\n",
    "# Make some columns that are useful for plotting.\n",
    "raw_melted_error_df = prepare_error_df(\n",
    "    proportion_df, \n",
    "    groupby_keys=[\n",
    "        \"calibrator\",\n",
    "        \"loss_func\",\n",
    "        \"data_id\", \n",
    "        \"experiment_pretrained_seed\", \n",
    "        \"gt proportion\", \n",
    "        \"new gt proportion\",\n",
    "        \"soft proportion\", \n",
    "        \"hard proportion\",\n",
    "        \"split\",\n",
    "    ],\n",
    "    value_vars=[\"new gt error\", \"soft error\", \"hard error\"],\n",
    "    var_name=\"proportion_type\",\n",
    "    value_name=\"error\"\n",
    ")\n",
    "# Process the proportion types.\n",
    "melted_error_df = process_method_names(raw_melted_error_df, value_name=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_error_df['proportion type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we want to change the order that the methods appear in.\n",
    "melted_error_df['proportion type'] = melted_error_df['proportion type'].astype('category')\n",
    "melted_error_df['proportion type'] = melted_error_df['proportion type'].cat.reorder_categories([\n",
    "    'New GT',\n",
    "    'Uncalibrated hard (PixelCELoss)',\n",
    "    'Uncalibrated soft (PixelCELoss)',\n",
    "    'Uncalibrated hard (SoftDiceLoss)',\n",
    "    'Uncalibrated soft (SoftDiceLoss)',\n",
    "    'Uncalibrated hard (PixelFocalLoss)',\n",
    "    'Uncalibrated soft (PixelFocalLoss)',\n",
    "    'SVLS soft (PixelCELoss)',\n",
    "    'SVLS soft (SoftDiceLoss)',\n",
    "    'SVLS soft (PixelFocalLoss)',\n",
    "])\n",
    "\n",
    "# Custom palette dictionary\n",
    "custom_palette = {\n",
    "    'New GT': 'red',\n",
    "    'Uncalibrated hard (PixelCELoss)': 'blue',\n",
    "    'Uncalibrated soft (PixelCELoss)': 'royalblue',\n",
    "    'Uncalibrated hard (SoftDiceLoss)': 'darkgreen',\n",
    "    'Uncalibrated soft (SoftDiceLoss)': 'mediumseagreen',\n",
    "    'Uncalibrated hard (PixelFocalLoss)': 'indianred',\n",
    "    'Uncalibrated soft (PixelFocalLoss)': 'lightcoral',\n",
    "    'SVLS soft (PixelCELoss)': 'goldenrod',\n",
    "    'SVLS soft (SoftDiceLoss)': 'gold',\n",
    "    'SVLS soft (PixelFocalLoss)': 'khaki',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=melted_error_df,      # Ensure you use the 'data' parameter correctly.\n",
    "    x=\"proportion type\",\n",
    "    y=\"absolute error\",\n",
    "    col=\"experiment_pretrained_seed\",\n",
    "    hue=\"proportion type\",\n",
    "    col_wrap=4,\n",
    "    sharex=False,\n",
    "    aspect=1.5,\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "# For each subplot, add a line at y = 0 to show the error.\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, ls='--', color='red')\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Soft/Hard proportion Error', fontsize=30)\n",
    "\n",
    "# Add a legend\n",
    "g.add_legend(title='proportion Type')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to calculate now the average (across seeds) of the the absolute error for each method.\n",
    "# That means first averaging within each seed, then averaging across seeds.\n",
    "# We group by the data_id and the proportion_type.\n",
    "error_per_seed = melted_error_df.groupby([\"experiment_pretrained_seed\", \"proportion type\"]).agg(\n",
    "    abs_error=(\"absolute error\", \"mean\"),\n",
    ").reset_index()\n",
    "# Rename abs_error to absolute error.\n",
    "error_per_seed = error_per_seed.rename(columns={\"abs_error\": \"absolute error\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table with each row being the proportion type, and the column being the abs_error mean with standard deviation in another column.\n",
    "method_error_table = error_per_seed.pivot_table(\n",
    "    values=\"absolute error\",\n",
    "    index=\"proportion type\",\n",
    "    columns=\"experiment_pretrained_seed\",\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "# Average across the pretrained seeds to make an average column, and then make a stdv column.\n",
    "method_error_table['mean'] = method_error_table.mean(axis=1)\n",
    "method_error_table['stdv'] = method_error_table.std(axis=1)\n",
    "# We can drop all other columns that aren't mean and stdv\n",
    "method_error_table = method_error_table[['mean', 'stdv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_error_table.sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, let's look at relative error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_df.augment(calibrator)\n",
    "# Make some columns that are useful for plotting.\n",
    "raw_melted_relative_df = prepare_error_df(\n",
    "    proportion_df, \n",
    "    groupby_keys=[\n",
    "        \"calibrator\",\n",
    "        \"loss_func\",\n",
    "        \"data_id\", \n",
    "        \"experiment_pretrained_seed\", \n",
    "        \"gt proportion\", \n",
    "        \"new gt proportion\",\n",
    "        \"soft proportion\", \n",
    "        \"hard proportion\",\n",
    "        \"split\",\n",
    "    ],\n",
    "    value_vars=[\"new gt relative error\", \"soft relative error\", \"hard relative error\"],\n",
    "    var_name=\"proportion_type\",\n",
    "    value_name=\"relative error\"\n",
    ")\n",
    "# Process the proportion types.\n",
    "melted_relative_df = process_method_names(raw_melted_relative_df, value_name=\"relative error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_relative_df['proportion type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we want to change the order that the methods appear in.\n",
    "melted_error_df['proportion type'] = melted_error_df['proportion type'].astype('category')\n",
    "melted_error_df['proportion type'] = melted_error_df['proportion type'].cat.reorder_categories([\n",
    "    'New GT',\n",
    "    'Uncalibrated hard (PixelCELoss)',\n",
    "    'Uncalibrated soft (PixelCELoss)',\n",
    "    'Uncalibrated hard (SoftDiceLoss)',\n",
    "    'Uncalibrated soft (SoftDiceLoss)',\n",
    "    'Uncalibrated hard (PixelFocalLoss)',\n",
    "    'Uncalibrated soft (PixelFocalLoss)',\n",
    "    'SVLS soft (PixelCELoss)',\n",
    "    'SVLS soft (SoftDiceLoss)',\n",
    "    'SVLS soft (PixelFocalLoss)',\n",
    "])\n",
    "\n",
    "# Custom palette dictionary\n",
    "custom_palette = {\n",
    "    'New GT': 'red',\n",
    "    'Uncalibrated hard (PixelCELoss)': 'blue',\n",
    "    'Uncalibrated soft (PixelCELoss)': 'royalblue',\n",
    "    'Uncalibrated hard (SoftDiceLoss)': 'darkgreen',\n",
    "    'Uncalibrated soft (SoftDiceLoss)': 'mediumseagreen',\n",
    "    'Uncalibrated hard (PixelFocalLoss)': 'indianred',\n",
    "    'Uncalibrated soft (PixelFocalLoss)': 'lightcoral',\n",
    "    'SVLS soft (PixelCELoss)': 'goldenrod',\n",
    "    'SVLS soft (SoftDiceLoss)': 'gold',\n",
    "    'SVLS soft (PixelFocalLoss)': 'khaki',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=melted_relative_df,      # Ensure you use the 'data' parameter correctly.\n",
    "    x=\"proportion type\",\n",
    "    y=\"relative error\",\n",
    "    col=\"experiment_pretrained_seed\",\n",
    "    hue=\"proportion type\",\n",
    "    col_wrap=4,\n",
    "    sharex=False,\n",
    "    aspect=1.5,\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "# Calculate the means\n",
    "means = melted_relative_df.groupby(['experiment_pretrained_seed', 'proportion type'])['relative error'].mean().reset_index()\n",
    "\n",
    "# For each subplot make a line at y = 0 to show the error and add the means\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0.0, ls='--', color='red')\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    # Get the corresponding seed for the current subplot\n",
    "    seed = ax.get_title().split(' = ')[-1]\n",
    "    \n",
    "    # Filter means for the current subplot\n",
    "    seed_means = means[means['experiment_pretrained_seed'] == int(seed)]\n",
    "    \n",
    "    # Plot a purple circle at the mean position\n",
    "    for _, row in seed_means.iterrows():\n",
    "        ax.plot(row['proportion type'], row['relative error'], 'o', color='purple', markersize=8, zorder=10)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Relative Proportion Error (Blobs)', fontsize=30)\n",
    "\n",
    "# Add a legend\n",
    "g.add_legend(title='proportion Type')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=melted_relative_df,      # Ensure you use the 'data' parameter correctly.\n",
    "    x=\"proportion type\",\n",
    "    y=\"absolute relative error\",\n",
    "    col=\"experiment_pretrained_seed\",\n",
    "    hue=\"proportion type\",\n",
    "    col_wrap=4,\n",
    "    sharex=False,\n",
    "    aspect=1.5,\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "# Calculate the means\n",
    "means = melted_relative_df.groupby(['experiment_pretrained_seed', 'proportion type'])['absolute relative error'].mean().reset_index()\n",
    "\n",
    "# For each subplot make a line at y = 0 to show the error and add the means\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0.0, ls='--', color='red')\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    # Get the corresponding seed for the current subplot\n",
    "    seed = ax.get_title().split(' = ')[-1]\n",
    "    \n",
    "    # Filter means for the current subplot\n",
    "    seed_means = means[means['experiment_pretrained_seed'] == int(seed)]\n",
    "    \n",
    "    # Plot a purple circle at the mean position\n",
    "    for _, row in seed_means.iterrows():\n",
    "        ax.plot(row['proportion type'], row['absolute relative error'], 'o', color='purple', markersize=8, zorder=10)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Absolute Relative Proportion Error (Heart Wall)', fontsize=30)\n",
    "\n",
    "# Add a legend\n",
    "g.add_legend(title='proportion Type')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at this averaged over seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to calculate now the average (across seeds) of the the absolute error for each method.\n",
    "# That means first averaging within each seed, then averaging across seeds.\n",
    "# We group by the data_id and the proportion_type.\n",
    "relative_per_seed = melted_relative_df.groupby([\"experiment_pretrained_seed\", \"proportion type\"]).agg(\n",
    "    abs_relative_error=(\"absolute relative error\", \"mean\"),\n",
    ").reset_index()\n",
    "# Rename abs_error to absolute error.\n",
    "relative_per_seed = relative_per_seed.rename(columns={\"abs_relative_error\": \"absolute relative error\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table with each row being the proportion type, and the column being the abs_error mean with standard deviation in another column.\n",
    "method_relative_table = relative_per_seed.pivot_table(\n",
    "    values=\"absolute relative error\",\n",
    "    index=\"proportion type\",\n",
    "    columns=\"experiment_pretrained_seed\",\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "# Average across the pretrained seeds to make an average column, and then make a stdv column.\n",
    "method_relative_table['mean'] = method_relative_table.mean(axis=1)\n",
    "method_relative_table['stdv'] = method_relative_table.std(axis=1)\n",
    "# We can drop all other columns that aren't mean and stdv\n",
    "method_relative_table = method_relative_table[['mean', 'stdv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_relative_table.sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And lets look at this over Data_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to calculate now the average (across seeds) of the the absolute error for each method.\n",
    "# That means first averaging within each seed, then averaging across seeds.\n",
    "# We group by the data_id and the proportion_type.\n",
    "relative_per_dataid = melted_relative_df.groupby([\"data_id\", \"proportion type\"]).agg(\n",
    "    abs_relative_error=(\"absolute relative error\", \"mean\"),\n",
    ").reset_index()\n",
    "# Rename abs_error to absolute error.\n",
    "relative_per_dataid = relative_per_dataid.rename(columns={\"abs_relative_error\": \"absolute relative error\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table with each row being the proportion type, and the column being the abs_error mean with standard deviation in another column.\n",
    "method_relative_table_dataid = relative_per_dataid.pivot_table(\n",
    "    values=\"absolute relative error\",\n",
    "    index=\"proportion type\",\n",
    "    columns=\"data_id\",\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "# Average across the pretrained seeds to make an average column, and then make a stdv column.\n",
    "method_relative_table_dataid['mean'] = method_relative_table_dataid.mean(axis=1)\n",
    "method_relative_table_dataid['stdv'] = method_relative_table_dataid.std(axis=1)\n",
    "# We can drop all other columns that aren't mean and stdv\n",
    "method_relative_table_dataid = method_relative_table_dataid[['mean', 'stdv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_relative_table_dataid.sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, we want to look at the Dice score and Image ECE of all three methods as we are arguing that Image ECE is what we should care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relevant columns for looking at the Dice score and Image ECE\n",
    "metric_cols = [\n",
    "    \"data_id\",\n",
    "    \"loss_func\",\n",
    "    \"experiment_pretrained_seed\",\n",
    "    \"model_pretrained_exp_root\",\n",
    "    \"split\",\n",
    "    \"image_metric\",\n",
    "    \"metric_score\"\n",
    "]\n",
    "# Take these columns of the inference_df, drop other columns and delete duplicate rows.\n",
    "metric_df = inference_df[metric_cols].drop_duplicates().reset_index(drop=True)\n",
    "metric_df.augment(calibrator)\n",
    "# Drop the 'pretrained_exp_root column\n",
    "metric_df = metric_df.drop(columns=[\"model_pretrained_exp_root\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_method(calibrator, loss_func):\n",
    "    return calibrator + f\" ({loss_func})\"\n",
    "\n",
    "metric_df.augment(train_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby the pretrained_seed and image_metrics, and calibrator, and take the mean of the metric_score.\n",
    "metric_per_seed = metric_df.groupby([\"experiment_pretrained_seed\", \"loss_func\", \"image_metric\", \"train_method\"]).agg(\n",
    "    metric_score=(\"metric_score\", \"mean\"),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we want to change the order that the methods appear in.\n",
    "metric_per_seed['train_method'] = metric_per_seed['train_method'].astype('category')\n",
    "metric_per_seed['train_method'] = metric_per_seed['train_method'].cat.reorder_categories([\n",
    "    'Uncalibrated (PixelCELoss)',\n",
    "    'Uncalibrated (SoftDiceLoss)',\n",
    "    'Uncalibrated (PixelFocalLoss)',\n",
    "    'SVLS (PixelCELoss)',\n",
    "    'SVLS (SoftDiceLoss)',\n",
    "    'SVLS (PixelFocalLoss)',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = metric_per_seed.groupby(['train_method', 'image_metric']).agg(\n",
    "    mean_metric_score=('metric_score', 'mean'),\n",
    "    std_metric_score=('metric_score', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Pivot the table to have one row for each train_method\n",
    "result = result.pivot(\n",
    "    index='train_method', \n",
    "    columns='image_metric', \n",
    "    values=[\n",
    "        'mean_metric_score', \n",
    "        'std_metric_score'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Flatten the column multi-index\n",
    "result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "result = result.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the of the columns that are 'calibrator' and have 'Dice_Loss' in them\n",
    "dice_cols = [col for col in result.columns if 'Dice Loss' in col or 'train_method' in col]\n",
    "dice_result = result[dice_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_result.sort_values(by='mean_metric_score_Dice Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the of the columns that are 'calibrator' and have 'Image_ECE' in them\n",
    "image_ece_cols = [col for col in result.columns if 'Image_ECE' in col or 'train_method' in col]\n",
    "ece_result = result[image_ece_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_result.sort_values(by='mean_metric_score_Image_ECE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniverSegTF",
   "language": "python",
   "name": "universegtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
