{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "\n",
    "from ese.experiment.analysis.analyze_inf import load_cal_inference_stats\n",
    "# Results loader object does everything\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from pathlib import Path\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "log:\n",
    "    root: /storage/vbutoi/scratch/ESE/inference\n",
    "    inference_groups: \n",
    "        - '07_15_24_ACDC_Experiment_1'\n",
    "\n",
    "options:\n",
    "    add_dice_loss_rows: True\n",
    "    drop_nan_metric_rows: True \n",
    "    remove_shared_columns: False\n",
    "    equal_rows_per_cfg_assert: False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping (datapoint, metric) pairs with NaN metric score. Dropped from 1080 -> 1080 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/vbutoi/projects/ESE/ese/experiment/analysis/analyze_inf.py:201: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  inference_df = pd.concat(inference_pd_collection, axis=0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Grouper for 'log_set' not 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inference_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_cal_inference_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_cached\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/pydantic/deprecated/decorator.py:55\u001b[0m, in \u001b[0;36mvalidate_arguments.<locals>.validate.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(_func)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/pydantic/deprecated/decorator.py:150\u001b[0m, in \u001b[0;36mValidatedFunction.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    149\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_model_instance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/pydantic/deprecated/decorator.py:222\u001b[0m, in \u001b[0;36mValidatedFunction.execute\u001b[0;34m(self, m)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_function(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvar_kwargs)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvar_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/vbutoi/projects/ESE/ese/experiment/analysis/analyze_inf.py:225\u001b[0m, in \u001b[0;36mload_cal_inference_stats\u001b[0;34m(results_cfg, load_cached)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropping (datapoint, metric) pairs with NaN metric score. Dropped from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_row_amount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inference_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Get the number of rows in image_info_df for each log set.\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m num_rows_per_log_set \u001b[38;5;241m=\u001b[39m \u001b[43minference_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog.root\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_set\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal_rows_per_cfg_assert\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Make sure there is only one unique value in the above.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(num_rows_per_log_set\u001b[38;5;241m.\u001b[39munique()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \\\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of rows in the image_info_df is not the same for all log sets. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows_per_log_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/pandas/core/frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   8399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8400\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8405\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8408\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8410\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8413\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[0;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/envs/UniverSegTF/lib/python3.9/site-packages/pandas/core/groupby/grouper.py:883\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    879\u001b[0m     in_axis, name, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, gpr, obj[gpr]\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# non-unique columns; raise here to get the name in the\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# exception message\u001b[39;00m\n\u001b[0;32m--> 883\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrouper for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    884\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(name)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_is_level_reference(gpr, axis\u001b[38;5;241m=\u001b[39maxis):\n",
      "\u001b[0;31mValueError\u001b[0m: Grouper for 'log_set' not 1-dimensional"
     ]
    }
   ],
   "source": [
    "inference_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg,\n",
    "    load_cached=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ikey in inference_df.keys():\n",
    "    print(ikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df['model_pretrained_exp_root'].unique()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df['loss_func_class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df['model_class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_loss_func(model_class, loss_func_class):\n",
    "    if model_class.split('.')[-1] == \"UNet\":\n",
    "        return loss_func_class\n",
    "    else:\n",
    "        #TODO: FILL ME!\n",
    "        return None\n",
    "\n",
    "def finetune_loss_func(model_class, loss_func_class):\n",
    "    if model_class.split('.')[-1] == \"UNet\":\n",
    "        return \"None\"\n",
    "    else:\n",
    "        return loss_func_class.split('.')[-1]\n",
    "\n",
    "def posthoc_method(model_class):\n",
    "    model_class_name = model_class.split('.')[-1]\n",
    "    if model_class_name == \"None\":\n",
    "        return \"Finetune\"\n",
    "    else:\n",
    "        return model_class_name\n",
    "\n",
    "inference_df.augment(pretrain_loss_func)\n",
    "inference_df.augment(finetune_loss_func)\n",
    "inference_df.augment(posthoc_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of this experiment, we only care about a few columns in particular:\n",
    "exp_columns = [\n",
    "    \"data_id\",\n",
    "    \"loss_func\",\n",
    "    \"new_gt_proportion\", # This is after our resizing and blurring\n",
    "    \"gt_proportion\",\n",
    "    \"soft_proportion\",\n",
    "    \"hard_proportion\",\n",
    "    \"experiment_pretrained_seed\", \n",
    "    \"model_pretrained_exp_root\",\n",
    "    \"split\",\n",
    "]\n",
    "# Take these columns of the inference_df, drop other columns.\n",
    "exp_df = inference_df[exp_columns].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of examples we are evaluating on.\n",
    "len(exp_df['data_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to rename 'proportion' to 'proportion' for the sake of the experiment.\n",
    "proportion_df = exp_df.rename(columns={\n",
    "    \"gt_proportion\": \"gt proportion\",\n",
    "    \"new_gt_proportion\": \"new gt proportion\",\n",
    "    \"hard_proportion\": \"hard proportion\",\n",
    "    \"soft_proportion\": \"soft proportion\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two new columns, one for the soft proportion error and one for the hard proportion error.\n",
    "proportion_df['new gt error'] = (proportion_df['new gt proportion'] - proportion_df['gt proportion'])\n",
    "proportion_df['soft error'] = (proportion_df['soft proportion'] - proportion_df['gt proportion'])\n",
    "proportion_df['hard error'] = (proportion_df['hard proportion'] - proportion_df['gt proportion'])\n",
    "# Make the normalized metric that divides the error by the ground truth proportion.\n",
    "proportion_df['new gt relative error'] = proportion_df['new gt error'] / proportion_df['gt proportion']\n",
    "proportion_df['soft relative error'] = proportion_df['soft error'] / proportion_df['gt proportion']\n",
    "proportion_df['hard relative error'] = proportion_df['hard error'] / proportion_df['gt proportion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important for this experiment that we only consider the 'val' and 'cal' splits because we trained on the 'train' split.\n",
    "proportion_df['split'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_error_df(raw_df, groupby_keys, value_vars, var_name, value_name):\n",
    "    # Make a clone of the proportion df.\n",
    "    input_df = raw_df.copy()\n",
    "    # Melt the dataframe to have a single column for the error.\n",
    "    error_df = pd.melt(\n",
    "        input_df,\n",
    "        id_vars=groupby_keys,\n",
    "        value_vars=value_vars,\n",
    "        var_name=var_name,\n",
    "        value_name=value_name,\n",
    "    )\n",
    "    # Make some columns that are useful for plotting.\n",
    "    error_df[f'absolute {value_name}'] = error_df[value_name].abs()\n",
    "    # Return the melted dataframe.\n",
    "    return error_df\n",
    "\n",
    "\n",
    "def calibrator(model_pretrained_exp_root):\n",
    "    if \"SVLS\" in model_pretrained_exp_root:\n",
    "        return \"SVLS\"\n",
    "    else:\n",
    "        return \"Uncalibrated\"\n",
    "\n",
    "\n",
    "def process_method_names(input_df, value_name):\n",
    "    # Make a clone of the input_df\n",
    "    df = input_df.copy()\n",
    "    # Drop all the rows where calibrator != Uncalibrated AND the proportion_type is hard_proportion_error.\n",
    "    df = df[~((df['calibrator'] != 'Uncalibrated') & (df['proportion_type'] == f'hard {value_name}'))]\n",
    "    # Then we augment the proportion_type with the calibrator name.\n",
    "    def proportion_type(calibrator, loss_func, proportion_type):\n",
    "        if proportion_type in [\"new gt error\", \"new gt relative error\"]:\n",
    "            return \"New GT\"\n",
    "        elif calibrator == \"Uncalibrated\":\n",
    "            return \"Uncalibrated \" + proportion_type.split(\" \")[0] + f\" ({loss_func})\"\n",
    "        else:\n",
    "            return calibrator + \" soft\" + f\" ({loss_func})\"\n",
    "    # Finally, sort by data_id\n",
    "    df['proportion type'] = df.apply(lambda x: proportion_type(x['calibrator'], x['loss_func'], x['proportion_type']), axis=1)\n",
    "    df = df.sort_values(by=\"data_id\")\n",
    "    # Drop the duplicate rows and reset the index.\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    # Return the augmented dataframe.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, let's look at absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_df.augment(calibrator)\n",
    "# Make some columns that are useful for plotting.\n",
    "raw_melted_error_df = prepare_error_df(\n",
    "    proportion_df, \n",
    "    groupby_keys=[\n",
    "        \"calibrator\",\n",
    "        \"loss_func\",\n",
    "        \"data_id\", \n",
    "        \"experiment_pretrained_seed\", \n",
    "        \"gt proportion\", \n",
    "        \"new gt proportion\",\n",
    "        \"soft proportion\", \n",
    "        \"hard proportion\",\n",
    "        \"split\",\n",
    "    ],\n",
    "    value_vars=[\"new gt error\", \"soft error\", \"hard error\"],\n",
    "    var_name=\"proportion_type\",\n",
    "    value_name=\"error\"\n",
    ")\n",
    "# Process the proportion types.\n",
    "melted_error_df = process_method_names(raw_melted_error_df, value_name=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_error_df['proportion type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we want to change the order that the methods appear in.\n",
    "melted_error_df['proportion type'] = melted_error_df['proportion type'].astype('category')\n",
    "melted_error_df['proportion type'] = melted_error_df['proportion type'].cat.reorder_categories([\n",
    "    'New GT',\n",
    "    'Uncalibrated hard (PixelCELoss)',\n",
    "    'Uncalibrated soft (PixelCELoss)',\n",
    "    'Uncalibrated hard (SoftDiceLoss)',\n",
    "    'Uncalibrated soft (SoftDiceLoss)',\n",
    "    'Uncalibrated hard (PixelFocalLoss)',\n",
    "    'Uncalibrated soft (PixelFocalLoss)',\n",
    "    'SVLS soft (PixelCELoss)',\n",
    "    'SVLS soft (SoftDiceLoss)',\n",
    "    'SVLS soft (PixelFocalLoss)',\n",
    "])\n",
    "\n",
    "# Custom palette dictionary\n",
    "custom_palette = {\n",
    "    'New GT': 'red',\n",
    "    'Uncalibrated hard (PixelCELoss)': 'blue',\n",
    "    'Uncalibrated soft (PixelCELoss)': 'royalblue',\n",
    "    'Uncalibrated hard (SoftDiceLoss)': 'darkgreen',\n",
    "    'Uncalibrated soft (SoftDiceLoss)': 'mediumseagreen',\n",
    "    'Uncalibrated hard (PixelFocalLoss)': 'indianred',\n",
    "    'Uncalibrated soft (PixelFocalLoss)': 'lightcoral',\n",
    "    'SVLS soft (PixelCELoss)': 'goldenrod',\n",
    "    'SVLS soft (SoftDiceLoss)': 'gold',\n",
    "    'SVLS soft (PixelFocalLoss)': 'khaki',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=melted_error_df,      # Ensure you use the 'data' parameter correctly.\n",
    "    x=\"proportion type\",\n",
    "    y=\"absolute error\",\n",
    "    col=\"experiment_pretrained_seed\",\n",
    "    hue=\"proportion type\",\n",
    "    col_wrap=4,\n",
    "    sharex=False,\n",
    "    aspect=1.5,\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "# For each subplot, add a line at y = 0 to show the error.\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, ls='--', color='red')\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Soft/Hard proportion Error', fontsize=30)\n",
    "\n",
    "# Add a legend\n",
    "g.add_legend(title='proportion Type')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to calculate now the average (across seeds) of the the absolute error for each method.\n",
    "# That means first averaging within each seed, then averaging across seeds.\n",
    "# We group by the data_id and the proportion_type.\n",
    "error_per_seed = melted_error_df.groupby([\"experiment_pretrained_seed\", \"proportion type\"]).agg(\n",
    "    abs_error=(\"absolute error\", \"mean\"),\n",
    ").reset_index()\n",
    "# Rename abs_error to absolute error.\n",
    "error_per_seed = error_per_seed.rename(columns={\"abs_error\": \"absolute error\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table with each row being the proportion type, and the column being the abs_error mean with standard deviation in another column.\n",
    "method_error_table = error_per_seed.pivot_table(\n",
    "    values=\"absolute error\",\n",
    "    index=\"proportion type\",\n",
    "    columns=\"experiment_pretrained_seed\",\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "# Average across the pretrained seeds to make an average column, and then make a stdv column.\n",
    "method_error_table['mean'] = method_error_table.mean(axis=1)\n",
    "method_error_table['stdv'] = method_error_table.std(axis=1)\n",
    "# We can drop all other columns that aren't mean and stdv\n",
    "method_error_table = method_error_table[['mean', 'stdv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_error_table.sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, let's look at relative error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_df.augment(calibrator)\n",
    "# Make some columns that are useful for plotting.\n",
    "raw_melted_relative_df = prepare_error_df(\n",
    "    proportion_df, \n",
    "    groupby_keys=[\n",
    "        \"calibrator\",\n",
    "        \"loss_func\",\n",
    "        \"data_id\", \n",
    "        \"experiment_pretrained_seed\", \n",
    "        \"gt proportion\", \n",
    "        \"new gt proportion\",\n",
    "        \"soft proportion\", \n",
    "        \"hard proportion\",\n",
    "        \"split\",\n",
    "    ],\n",
    "    value_vars=[\"new gt relative error\", \"soft relative error\", \"hard relative error\"],\n",
    "    var_name=\"proportion_type\",\n",
    "    value_name=\"relative error\"\n",
    ")\n",
    "# Process the proportion types.\n",
    "melted_relative_df = process_method_names(raw_melted_relative_df, value_name=\"relative error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_relative_df['proportion type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we want to change the order that the methods appear in.\n",
    "melted_error_df['proportion type'] = melted_error_df['proportion type'].astype('category')\n",
    "melted_error_df['proportion type'] = melted_error_df['proportion type'].cat.reorder_categories([\n",
    "    'New GT',\n",
    "    'Uncalibrated hard (PixelCELoss)',\n",
    "    'Uncalibrated soft (PixelCELoss)',\n",
    "    'Uncalibrated hard (SoftDiceLoss)',\n",
    "    'Uncalibrated soft (SoftDiceLoss)',\n",
    "    'Uncalibrated hard (PixelFocalLoss)',\n",
    "    'Uncalibrated soft (PixelFocalLoss)',\n",
    "    'SVLS soft (PixelCELoss)',\n",
    "    'SVLS soft (SoftDiceLoss)',\n",
    "    'SVLS soft (PixelFocalLoss)',\n",
    "])\n",
    "\n",
    "# Custom palette dictionary\n",
    "custom_palette = {\n",
    "    'New GT': 'red',\n",
    "    'Uncalibrated hard (PixelCELoss)': 'blue',\n",
    "    'Uncalibrated soft (PixelCELoss)': 'royalblue',\n",
    "    'Uncalibrated hard (SoftDiceLoss)': 'darkgreen',\n",
    "    'Uncalibrated soft (SoftDiceLoss)': 'mediumseagreen',\n",
    "    'Uncalibrated hard (PixelFocalLoss)': 'indianred',\n",
    "    'Uncalibrated soft (PixelFocalLoss)': 'lightcoral',\n",
    "    'SVLS soft (PixelCELoss)': 'goldenrod',\n",
    "    'SVLS soft (SoftDiceLoss)': 'gold',\n",
    "    'SVLS soft (PixelFocalLoss)': 'khaki',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=melted_relative_df,      # Ensure you use the 'data' parameter correctly.\n",
    "    x=\"proportion type\",\n",
    "    y=\"relative error\",\n",
    "    col=\"experiment_pretrained_seed\",\n",
    "    hue=\"proportion type\",\n",
    "    col_wrap=4,\n",
    "    sharex=False,\n",
    "    aspect=1.5,\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "# Calculate the means\n",
    "means = melted_relative_df.groupby(['experiment_pretrained_seed', 'proportion type'])['relative error'].mean().reset_index()\n",
    "\n",
    "# For each subplot make a line at y = 0 to show the error and add the means\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0.0, ls='--', color='red')\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    # Get the corresponding seed for the current subplot\n",
    "    seed = ax.get_title().split(' = ')[-1]\n",
    "    \n",
    "    # Filter means for the current subplot\n",
    "    seed_means = means[means['experiment_pretrained_seed'] == int(seed)]\n",
    "    \n",
    "    # Plot a purple circle at the mean position\n",
    "    for _, row in seed_means.iterrows():\n",
    "        ax.plot(row['proportion type'], row['relative error'], 'o', color='purple', markersize=8, zorder=10)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Relative Proportion Error (Blobs)', fontsize=30)\n",
    "\n",
    "# Add a legend\n",
    "g.add_legend(title='proportion Type')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=melted_relative_df,      # Ensure you use the 'data' parameter correctly.\n",
    "    x=\"proportion type\",\n",
    "    y=\"absolute relative error\",\n",
    "    col=\"experiment_pretrained_seed\",\n",
    "    hue=\"proportion type\",\n",
    "    col_wrap=4,\n",
    "    sharex=False,\n",
    "    aspect=1.5,\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "# Calculate the means\n",
    "means = melted_relative_df.groupby(['experiment_pretrained_seed', 'proportion type'])['absolute relative error'].mean().reset_index()\n",
    "\n",
    "# For each subplot make a line at y = 0 to show the error and add the means\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0.0, ls='--', color='red')\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    # Get the corresponding seed for the current subplot\n",
    "    seed = ax.get_title().split(' = ')[-1]\n",
    "    \n",
    "    # Filter means for the current subplot\n",
    "    seed_means = means[means['experiment_pretrained_seed'] == int(seed)]\n",
    "    \n",
    "    # Plot a purple circle at the mean position\n",
    "    for _, row in seed_means.iterrows():\n",
    "        ax.plot(row['proportion type'], row['absolute relative error'], 'o', color='purple', markersize=8, zorder=10)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Absolute Relative Proportion Error (Heart Wall)', fontsize=30)\n",
    "\n",
    "# Add a legend\n",
    "g.add_legend(title='proportion Type')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at this averaged over seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to calculate now the average (across seeds) of the the absolute error for each method.\n",
    "# That means first averaging within each seed, then averaging across seeds.\n",
    "# We group by the data_id and the proportion_type.\n",
    "relative_per_seed = melted_relative_df.groupby([\"experiment_pretrained_seed\", \"proportion type\"]).agg(\n",
    "    abs_relative_error=(\"absolute relative error\", \"mean\"),\n",
    ").reset_index()\n",
    "# Rename abs_error to absolute error.\n",
    "relative_per_seed = relative_per_seed.rename(columns={\"abs_relative_error\": \"absolute relative error\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table with each row being the proportion type, and the column being the abs_error mean with standard deviation in another column.\n",
    "method_relative_table = relative_per_seed.pivot_table(\n",
    "    values=\"absolute relative error\",\n",
    "    index=\"proportion type\",\n",
    "    columns=\"experiment_pretrained_seed\",\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "# Average across the pretrained seeds to make an average column, and then make a stdv column.\n",
    "method_relative_table['mean'] = method_relative_table.mean(axis=1)\n",
    "method_relative_table['stdv'] = method_relative_table.std(axis=1)\n",
    "# We can drop all other columns that aren't mean and stdv\n",
    "method_relative_table = method_relative_table[['mean', 'stdv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_relative_table.sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And lets look at this over Data_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to calculate now the average (across seeds) of the the absolute error for each method.\n",
    "# That means first averaging within each seed, then averaging across seeds.\n",
    "# We group by the data_id and the proportion_type.\n",
    "relative_per_dataid = melted_relative_df.groupby([\"data_id\", \"proportion type\"]).agg(\n",
    "    abs_relative_error=(\"absolute relative error\", \"mean\"),\n",
    ").reset_index()\n",
    "# Rename abs_error to absolute error.\n",
    "relative_per_dataid = relative_per_dataid.rename(columns={\"abs_relative_error\": \"absolute relative error\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table with each row being the proportion type, and the column being the abs_error mean with standard deviation in another column.\n",
    "method_relative_table_dataid = relative_per_dataid.pivot_table(\n",
    "    values=\"absolute relative error\",\n",
    "    index=\"proportion type\",\n",
    "    columns=\"data_id\",\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "# Average across the pretrained seeds to make an average column, and then make a stdv column.\n",
    "method_relative_table_dataid['mean'] = method_relative_table_dataid.mean(axis=1)\n",
    "method_relative_table_dataid['stdv'] = method_relative_table_dataid.std(axis=1)\n",
    "# We can drop all other columns that aren't mean and stdv\n",
    "method_relative_table_dataid = method_relative_table_dataid[['mean', 'stdv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_relative_table_dataid.sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, we want to look at the Dice score and Image ECE of all three methods as we are arguing that Image ECE is what we should care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relevant columns for looking at the Dice score and Image ECE\n",
    "metric_cols = [\n",
    "    \"data_id\",\n",
    "    \"loss_func\",\n",
    "    \"experiment_pretrained_seed\",\n",
    "    \"model_pretrained_exp_root\",\n",
    "    \"split\",\n",
    "    \"image_metric\",\n",
    "    \"metric_score\"\n",
    "]\n",
    "# Take these columns of the inference_df, drop other columns and delete duplicate rows.\n",
    "metric_df = inference_df[metric_cols].drop_duplicates().reset_index(drop=True)\n",
    "metric_df.augment(calibrator)\n",
    "# Drop the 'pretrained_exp_root column\n",
    "metric_df = metric_df.drop(columns=[\"model_pretrained_exp_root\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_method(calibrator, loss_func):\n",
    "    return calibrator + f\" ({loss_func})\"\n",
    "\n",
    "metric_df.augment(train_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby the pretrained_seed and image_metrics, and calibrator, and take the mean of the metric_score.\n",
    "metric_per_seed = metric_df.groupby([\"experiment_pretrained_seed\", \"loss_func\", \"image_metric\", \"train_method\"]).agg(\n",
    "    metric_score=(\"metric_score\", \"mean\"),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we want to change the order that the methods appear in.\n",
    "metric_per_seed['train_method'] = metric_per_seed['train_method'].astype('category')\n",
    "metric_per_seed['train_method'] = metric_per_seed['train_method'].cat.reorder_categories([\n",
    "    'Uncalibrated (PixelCELoss)',\n",
    "    'Uncalibrated (SoftDiceLoss)',\n",
    "    'Uncalibrated (PixelFocalLoss)',\n",
    "    'SVLS (PixelCELoss)',\n",
    "    'SVLS (SoftDiceLoss)',\n",
    "    'SVLS (PixelFocalLoss)',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = metric_per_seed.groupby(['train_method', 'image_metric']).agg(\n",
    "    mean_metric_score=('metric_score', 'mean'),\n",
    "    std_metric_score=('metric_score', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Pivot the table to have one row for each train_method\n",
    "result = result.pivot(\n",
    "    index='train_method', \n",
    "    columns='image_metric', \n",
    "    values=[\n",
    "        'mean_metric_score', \n",
    "        'std_metric_score'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Flatten the column multi-index\n",
    "result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "result = result.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the of the columns that are 'calibrator' and have 'Dice_Loss' in them\n",
    "dice_cols = [col for col in result.columns if 'Dice Loss' in col or 'train_method' in col]\n",
    "dice_result = result[dice_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_result.sort_values(by='mean_metric_score_Dice Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the of the columns that are 'calibrator' and have 'Image_ECE' in them\n",
    "image_ece_cols = [col for col in result.columns if 'Image_ECE' in col or 'train_method' in col]\n",
    "ece_result = result[image_ece_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_result.sort_values(by='mean_metric_score_Image_ECE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniverSegTF",
   "language": "python",
   "name": "universegtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
