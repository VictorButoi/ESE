{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "\n",
    "from ese.experiment.analysis.analyze_inf import load_cal_inference_stats\n",
    "# Results loader object does everything\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from pathlib import Path\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "log:\n",
    "    root: /storage/vbutoi/scratch/ESE/inference\n",
    "    inference_groups: \n",
    "        - '06_12_24_WMH_CorrectedMultiAnno'\n",
    "\n",
    "options:\n",
    "    add_dice_loss_rows: True\n",
    "    drop_nan_metric_rows: True \n",
    "    remove_shared_columns: False\n",
    "    equal_rows_per_cfg_assert: False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg,\n",
    "    load_cached=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of this experiment, we only care about a few columns in particular:\n",
    "exp_columns = [\n",
    "    \"annotator\",\n",
    "    \"data_id\",\n",
    "    \"gt_volume\",\n",
    "    \"hard_volume\",\n",
    "    \"soft_volume\",\n",
    "    \"pretrained_seed\", \n",
    "    \"slice_idx\",\n",
    "    \"split\",\n",
    "    \"task\"\n",
    "]\n",
    "# Take these columns of the inference_df, drop other columns.\n",
    "experiment_df = inference_df[exp_columns]\n",
    "# Remove the duplicate rows.\n",
    "experiment_df = experiment_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the volume df by summing over the volumes.\n",
    "vol_id_keys = [\"data_id\", \"annotator\", \"task\", \"pretrained_seed\", \"split\"]\n",
    "# Accumulate the volumes.\n",
    "volume_df = experiment_df.groupby(vol_id_keys).agg(\n",
    "    gt_volume=(\"gt_volume\", \"sum\"),\n",
    "    hard_volume=(\"hard_volume\", \"sum\"),\n",
    "    soft_volume=(\"soft_volume\", \"sum\"),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two new columns, one for the soft volume error and one for the hard volume error.\n",
    "volume_df['soft_volume_error'] = volume_df['soft_volume'] - volume_df['gt_volume']\n",
    "volume_df['hard_volume_error'] = volume_df['hard_volume'] - volume_df['gt_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_error_df = pd.melt(\n",
    "    volume_df,\n",
    "    id_vars=[\"annotator\", \"data_id\", \"pretrained_seed\", \"task\", \"gt_volume\", \"soft_volume\", \"hard_volume\"],\n",
    "    value_vars=[\"soft_volume_error\", \"hard_volume_error\"],\n",
    "    var_name=\"volume_type\",\n",
    "    value_name=\"error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make some columns that are useful for plotting.\n",
    "melted_error_df['abs_error'] = melted_error_df['error'].abs()\n",
    "melted_error_df['log_abs_error'] = melted_error_df['error'].abs().apply(lambda x: np.log(x + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Looking at one annotator on Amsterdam, let's look at how the volumetric comparison looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_1_df = melted_error_df.select(annotator='observer_o12', task='Amsterdam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    exp_1_df,\n",
    "    x=\"data_id\",\n",
    "    y=\"error\",\n",
    "    hue=\"volume_type\",\n",
    "    aspect=3,\n",
    "    height=8,\n",
    "    sharey=False,\n",
    ")\n",
    "# For each subplot make a line at y = 0 to show the error.\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, ls='--', color='red')\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Soft/Hard Volumetric Error', fontsize=30)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    exp_1_df,\n",
    "    x=\"data_id\",\n",
    "    y=\"log_abs_error\",\n",
    "    hue=\"volume_type\",\n",
    "    aspect=3,\n",
    "    height=8,\n",
    "    sharey=False,\n",
    ")\n",
    "# For each subplot make a line at y = 0 to show the error.\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, ls='--', color='red')\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Absolute Soft/Hard Volumetric Log Error', fontsize=30)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Let's look at the same thing but this time also for Singapore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_2_df = melted_error_df.select(annotator='observer_o12', task='Singapore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    exp_2_df,\n",
    "    x=\"data_id\",\n",
    "    y=\"error\",\n",
    "    hue=\"volume_type\",\n",
    "    aspect=3,\n",
    "    height=8,\n",
    "    sharey=False,\n",
    ")\n",
    "# For each subplot make a line at y = 0 to show the error.\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, ls='--', color='red')\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Singapore Soft/Hard Volumetric Error', fontsize=30)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    exp_2_df,\n",
    "    x=\"data_id\",\n",
    "    y=\"log_abs_error\",\n",
    "    hue=\"volume_type\",\n",
    "    aspect=3,\n",
    "    height=8,\n",
    "    sharey=False,\n",
    ")\n",
    "# For each subplot make a line at y = 0 to show the error.\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, ls='--', color='red')\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Singapore Absolute Soft/Hard Volumetric Log Error', fontsize=30)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: The interesting thing about WMH is that we have multiple annotations per data_id (for some of the data_ids). Let's gather all of the data_ids that have all three annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the subset of the dataframe for which the number of unique annotators for each data_id is 3\n",
    "\n",
    "# Step 1: Group by 'data_id' and count unique 'annotator' values\n",
    "unique_counts = melted_error_df.groupby('data_id')['annotator'].nunique()\n",
    "# Step 2: Filter 'data_id's that have exactly three unique 'annotator' values\n",
    "filtered_data_ids = unique_counts[unique_counts == 3].index\n",
    "# Step 3: Get the subset of rows with the filtered 'data_id's\n",
    "multianno_melted_error_df= melted_error_df[melted_error_df['data_id'].isin(filtered_data_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multianno_melted_error_df['data_id'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multianno_melted_error_df.select(data_id='101', pretrained_seed=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
