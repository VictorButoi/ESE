{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "pd.set_option('display.max_rows', 200)  # or any other number you prefer\n",
    "\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "\n",
    "from ese.experiment.analysis.analyze_inf import load_cal_inference_stats\n",
    "# Results loader object does everything\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from pathlib import Path\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "log:\n",
    "    root: \"/storage/vbutoi/scratch/ESE/inference\"\n",
    "    inference_groups: \n",
    "        - '07_16_24_ACDC_Experiment_1_ALLSPLITS'\n",
    "\n",
    "options:\n",
    "    add_dice_loss_rows: True\n",
    "    drop_nan_metric_rows: True \n",
    "    remove_shared_columns: False\n",
    "    equal_rows_per_cfg_assert: False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg,\n",
    "    load_cached=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for what the pretraining loss function was called.\n",
    "def pretrain_loss_func(model_class, pretraining_loss_func_class, loss_func_class):\n",
    "    if model_class.split('.')[-1] == \"UNet\":\n",
    "        return loss_func_class.split('.')[-1]\n",
    "    else:\n",
    "        return pretraining_loss_func_class.split('.')[-1]\n",
    "\n",
    "# Add columns for what the posthoc loss function was.\n",
    "def finetune_loss_func(model_class, loss_func_class):\n",
    "    if model_class.split('.')[-1] == \"UNet\":\n",
    "        return \"None\"\n",
    "    else:\n",
    "        return loss_func_class.split('.')[-1]\n",
    "\n",
    "# Add column for what the posthoc method was.\n",
    "def calibration_method(model_class, experiment_model_dir):\n",
    "    model_class_name = model_class.split('.')[-1]\n",
    "    if model_class_name == \"Identity\":\n",
    "        return \"Finetune\"\n",
    "    elif model_class_name == \"Temperature_Scaling\":\n",
    "        return \"TempScaling\" \n",
    "    elif model_class_name == \"ImageBasedTS\":\n",
    "        return \"IBTS\" \n",
    "    elif model_class_name == \"LocalTS\":\n",
    "        return \"LTS\" \n",
    "    else:\n",
    "        if \"SVLS\" in experiment_model_dir:\n",
    "            return \"SVLS\"\n",
    "        else:\n",
    "            return \"None\" \n",
    "\n",
    "def total_method(pretrain_loss_func, finetune_loss_func, calibration_method):\n",
    "    return f\"{calibration_method},PT:{pretrain_loss_func},FT:{finetune_loss_func}\"\n",
    "\n",
    "def loss_funcs(pretrain_loss_func, finetune_loss_func):\n",
    "    return f\"{pretrain_loss_func} -> {finetune_loss_func}\"\n",
    "\n",
    "\n",
    "inference_df.augment(pretrain_loss_func)\n",
    "inference_df.augment(finetune_loss_func)\n",
    "inference_df.augment(calibration_method)\n",
    "inference_df.augment(total_method)\n",
    "inference_df.augment(loss_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of this experiment, we only care about a few columns in particular:\n",
    "exp_columns = [\n",
    "    \"data_id\",\n",
    "    \"pretrain_loss_func\",\n",
    "    \"finetune_loss_func\",\n",
    "    \"calibration_method\",\n",
    "    \"total_method\",\n",
    "    \"loss_funcs\",\n",
    "    \"gt_proportion\",\n",
    "    \"soft_proportion\",\n",
    "    \"hard_proportion\",\n",
    "    \"experiment_pretrained_seed\", # For this experiment we only use one seed but good anyways.\n",
    "    \"split\"\n",
    "]\n",
    "# Take these columns of the inference_df, drop other columns.\n",
    "exp_df = inference_df[exp_columns].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of examples we are evaluating on.\n",
    "len(exp_df['data_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two new columns, one for the soft proportion error and one for the hard proportion error.\n",
    "for pred_type in ['soft', 'hard']:\n",
    "    exp_df[f'{pred_type}_error'] = (exp_df[f'{pred_type}_proportion'] - exp_df['gt_proportion'])\n",
    "    exp_df[f'{pred_type}_relative_error'] = exp_df[f'{pred_type}_error'] / exp_df['gt_proportion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_error_df(raw_df, groupby_keys, value_vars, var_name=\"proportion_type\", value_name=\"error\"):\n",
    "    # Make a clone of the proportion df.\n",
    "    input_df = raw_df.copy()\n",
    "    # Melt the dataframe to have a single column for the error.\n",
    "    error_df = pd.melt(\n",
    "        input_df,\n",
    "        id_vars=groupby_keys,\n",
    "        value_vars=value_vars,\n",
    "        var_name=var_name,\n",
    "        value_name=value_name,\n",
    "    )\n",
    "    # Make some columns that are useful for plotting.\n",
    "    error_df[f'absolute_{value_name}'] = error_df[value_name].abs()\n",
    "    # Return the melted dataframe.\n",
    "    return error_df\n",
    "\n",
    "def process_method_names(input_df):\n",
    "    # Make a clone of the input_df\n",
    "    df = input_df.copy()\n",
    "    # Then we augment the proportion_type with the calibrator name.\n",
    "    df = df.sort_values(by=\"data_id\")\n",
    "    # Drop the duplicate rows and reset the index.\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    # Return the augmented dataframe.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABSOLUTE ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some columns that are useful for plotting.\n",
    "raw_melted_error_df = prepare_error_df(\n",
    "    exp_df, \n",
    "    groupby_keys=exp_columns,\n",
    "    value_vars=[\"soft_error\", \"hard_error\"],\n",
    "    var_name=\"proportion_type\",\n",
    "    value_name=\"error\"\n",
    ")\n",
    "# Process the proportion types.\n",
    "melted_error_df = process_method_names(raw_melted_error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(melted_error_df['calibration_method'].unique())\n",
    "print(melted_error_df['loss_funcs'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we want to change the order that the methods appear in.\n",
    "melted_error_df['calibration_method'] = melted_error_df['calibration_method'].astype('category')\n",
    "melted_error_df['calibration_method'] = melted_error_df['calibration_method'].cat.reorder_categories([\n",
    "    'None',\n",
    "    \"SVLS\",\n",
    "    'TempScaling',\n",
    "    'IBTS',\n",
    "    'LTS'\n",
    "])\n",
    "# Finally, we want to change the order that the methods appear in.\n",
    "melted_error_df['loss_funcs'] = melted_error_df['loss_funcs'].astype('category')\n",
    "melted_error_df['loss_funcs'] = melted_error_df['loss_funcs'].cat.reorder_categories([\n",
    "    'PixelCELoss -> None',\n",
    "    'SoftDiceLoss -> None',\n",
    "    'PixelCELoss -> PixelCELoss',\n",
    "    'PixelCELoss -> SoftDiceLoss',\n",
    "    'SoftDiceLoss -> PixelCELoss',\n",
    "    'SoftDiceLoss -> SoftDiceLoss'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always should have an assert that the number of examples is the same.\n",
    "melted_error_df.groupby(['calibration_method', 'loss_funcs', 'proportion_type', 'split']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=melted_error_df,      # Ensure you use the 'data' parameter correctly.\n",
    "    x=\"calibration_method\",\n",
    "    y=\"absolute_error\",\n",
    "    hue=\"loss_funcs\",\n",
    "    col=\"proportion_type\",\n",
    "    row=\"split\",\n",
    "    kind='box',\n",
    "    height=10,  # Adjust the height as needed\n",
    ")\n",
    "\n",
    "# For each subplot, add a line at y = 0 to show the error.\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, ls='--', color='red')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to get the actual numbers now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to calculate now the average (across seeds) of the the absolute error for each method.\n",
    "# That means first averaging within each seed, then averaging across seeds.\n",
    "# We group by the data_id and the proportion_type.\n",
    "error_per_method = melted_error_df.groupby([\"experiment_pretrained_seed\", \"proportion_type\"]).agg(\n",
    "    abs_error=(\"absolute_error\", \"mean\"),\n",
    ").reset_index()\n",
    "# Rename abs_error to absolute error.\n",
    "error_per_seed = error_per_method.rename(columns={\"abs_error\": \"absolute_error\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table with each row being the proportion type, and the column being the abs_error mean with standard deviation in another column.\n",
    "method_error_table = error_per_seed.pivot_table(\n",
    "    values=\"absolute_error\",\n",
    "    index=\"proportion_type\",\n",
    "    columns=\"experiment_pretrained_seed\",\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "# Average across the pretrained seeds to make an average column, and then make a stdv column.\n",
    "method_error_table['mean'] = method_error_table.mean(axis=1)\n",
    "method_error_table['stdv'] = method_error_table.std(axis=1)\n",
    "# We can drop all other columns that aren't mean and stdv\n",
    "method_error_table = method_error_table[['mean', 'stdv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_error_table.sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELATIVE ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some columns that are useful for plotting.\n",
    "raw_melted_relative_df = prepare_error_df(\n",
    "    exp_df, \n",
    "    groupby_keys=exp_columns,\n",
    "    value_vars=[\"soft_relative_error\", \"hard_relative_error\"],\n",
    "    var_name=\"proportion_type\",\n",
    "    value_name=\"relative_error\"\n",
    ")\n",
    "# Process the proportion types.\n",
    "melted_relative_df = process_method_names(raw_melted_relative_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we want to change the order that the methods appear in.\n",
    "melted_error_df['calibration_method'] = melted_error_df['calibration_method'].astype('category')\n",
    "melted_error_df['calibration_method'] = melted_error_df['calibration_method'].cat.reorder_categories([\n",
    "    'None',\n",
    "    \"SVLS\",\n",
    "    'TempScaling',\n",
    "    'IBTS',\n",
    "    'LTS'\n",
    "])\n",
    "# Finally, we want to change the order that the methods appear in.\n",
    "melted_error_df['loss_funcs'] = melted_error_df['loss_funcs'].astype('category')\n",
    "melted_error_df['loss_funcs'] = melted_error_df['loss_funcs'].cat.reorder_categories([\n",
    "    'PixelCELoss -> None',\n",
    "    'SoftDiceLoss -> None',\n",
    "    'PixelCELoss -> PixelCELoss',\n",
    "    'PixelCELoss -> SoftDiceLoss',\n",
    "    'SoftDiceLoss -> PixelCELoss',\n",
    "    'SoftDiceLoss -> SoftDiceLoss'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_relative_df['proportion_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=melted_relative_df,      # Ensure you use the 'data' parameter correctly.\n",
    "    x=\"calibration_method\",\n",
    "    y=\"relative_error\",\n",
    "    hue=\"loss_funcs\",\n",
    "    col=\"proportion_type\",\n",
    "    row=\"split\",\n",
    "    kind='box',\n",
    "    height=10,  # Adjust the height as needed\n",
    ")\n",
    "\n",
    "# For each subplot, add a line at y = 0 to show the error.\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, ls='--', color='red')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=melted_relative_df,      # Ensure you use the 'data' parameter correctly.\n",
    "    x=\"calibration_method\",\n",
    "    y=\"absolute_relative_error\",\n",
    "    hue=\"loss_funcs\",\n",
    "    col=\"proportion_type\",\n",
    "    row=\"split\",\n",
    "    kind='box',\n",
    "    height=10,  # Adjust the height as needed\n",
    ")\n",
    "\n",
    "# For each subplot, add a line at y = 0 to show the error.\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, ls='--', color='red')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And lets look at this over Data_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to calculate now the average (across seeds) of the the absolute error for each method.\n",
    "# That means first averaging within each seed, then averaging across seeds.\n",
    "# We group by the data_id and the proportion_type.\n",
    "relative_per_dataid = melted_relative_df.groupby([\"data_id\", \"proportion type\"]).agg(\n",
    "    abs_relative_error=(\"absolute relative error\", \"mean\"),\n",
    ").reset_index()\n",
    "# Rename abs_error to absolute error.\n",
    "relative_per_dataid = relative_per_dataid.rename(columns={\"abs_relative_error\": \"absolute relative error\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table with each row being the proportion type, and the column being the abs_error mean with standard deviation in another column.\n",
    "method_relative_table_dataid = relative_per_dataid.pivot_table(\n",
    "    values=\"absolute relative error\",\n",
    "    index=\"proportion type\",\n",
    "    columns=\"data_id\",\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "# Average across the pretrained seeds to make an average column, and then make a stdv column.\n",
    "method_relative_table_dataid['mean'] = method_relative_table_dataid.mean(axis=1)\n",
    "method_relative_table_dataid['stdv'] = method_relative_table_dataid.std(axis=1)\n",
    "# We can drop all other columns that aren't mean and stdv\n",
    "method_relative_table_dataid = method_relative_table_dataid[['mean', 'stdv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_relative_table_dataid.sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, we want to look at the Dice score and Image ECE of all three methods as we are arguing that Image ECE is what we should care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relevant columns for looking at the Dice score and Image ECE\n",
    "metric_cols = [\n",
    "    \"data_id\",\n",
    "    \"loss_func\",\n",
    "    \"experiment_pretrained_seed\",\n",
    "    \"model_pretrained_exp_root\",\n",
    "    \"split\",\n",
    "    \"image_metric\",\n",
    "    \"metric_score\"\n",
    "]\n",
    "# Take these columns of the inference_df, drop other columns and delete duplicate rows.\n",
    "metric_df = inference_df[metric_cols].drop_duplicates().reset_index(drop=True)\n",
    "metric_df.augment(calibrator)\n",
    "# Drop the 'pretrained_exp_root column\n",
    "metric_df = metric_df.drop(columns=[\"model_pretrained_exp_root\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_method(calibrator, loss_func):\n",
    "    return calibrator + f\" ({loss_func})\"\n",
    "\n",
    "metric_df.augment(train_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby the pretrained_seed and image_metrics, and calibrator, and take the mean of the metric_score.\n",
    "metric_per_seed = metric_df.groupby([\"experiment_pretrained_seed\", \"loss_func\", \"image_metric\", \"train_method\"]).agg(\n",
    "    metric_score=(\"metric_score\", \"mean\"),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we want to change the order that the methods appear in.\n",
    "metric_per_seed['train_method'] = metric_per_seed['train_method'].astype('category')\n",
    "metric_per_seed['train_method'] = metric_per_seed['train_method'].cat.reorder_categories([\n",
    "    'Uncalibrated (PixelCELoss)',\n",
    "    'Uncalibrated (SoftDiceLoss)',\n",
    "    'Uncalibrated (PixelFocalLoss)',\n",
    "    'SVLS (PixelCELoss)',\n",
    "    'SVLS (SoftDiceLoss)',\n",
    "    'SVLS (PixelFocalLoss)',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = metric_per_seed.groupby(['train_method', 'image_metric']).agg(\n",
    "    mean_metric_score=('metric_score', 'mean'),\n",
    "    std_metric_score=('metric_score', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Pivot the table to have one row for each train_method\n",
    "result = result.pivot(\n",
    "    index='train_method', \n",
    "    columns='image_metric', \n",
    "    values=[\n",
    "        'mean_metric_score', \n",
    "        'std_metric_score'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Flatten the column multi-index\n",
    "result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "result = result.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the of the columns that are 'calibrator' and have 'Dice_Loss' in them\n",
    "dice_cols = [col for col in result.columns if 'Dice Loss' in col or 'train_method' in col]\n",
    "dice_result = result[dice_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_result.sort_values(by='mean_metric_score_Dice Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the of the columns that are 'calibrator' and have 'Image_ECE' in them\n",
    "image_ece_cols = [col for col in result.columns if 'Image_ECE' in col or 'train_method' in col]\n",
    "ece_result = result[image_ece_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_result.sort_values(by='mean_metric_score_Image_ECE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniverSegTF",
   "language": "python",
   "name": "universegtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
