{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is all about really crunnching when we have an image what we would 'like' to be\n",
    "# called calibrated and not. Basically, what averages will we be happy with, and which will we not.\n",
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "from ese.experiment.metrics import TL_ECE\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Bad half-boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the ground truth, which is a 10x10 tensor with a white square in the middle\n",
    "# we will slice off the right side of the square to make our contradiction.\n",
    "ground_truth1 = torch.zeros(10, 10)\n",
    "ground_truth1[3:7, 3:7] = 1\n",
    "ground_truth1[3:7, 6] = 0\n",
    "ground_truth1[3, 4:7] = 0\n",
    "\n",
    "plt.imshow(ground_truth1, cmap='gray')  \n",
    "plt.colorbar()\n",
    "plt.title('Ground Truth 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.losses.weights import dice_weights, hausdorff_weights\n",
    "\n",
    "test_x = torch.zeros(2, 10, 10)\n",
    "\n",
    "test_x[0, 3:7, 3:7] = 1\n",
    "test_x[0, 3:7, 6] = 0\n",
    "test_x[0, 3, 4:7] = 0\n",
    "\n",
    "test_x[1, 2:9, 2:9] = 1\n",
    "\n",
    "# weighted_square = dice_weights(test_x)\n",
    "weighted_square = hausdorff_weights(test_x)\n",
    "\n",
    "for b_idx in range(test_x.shape[0]):\n",
    "    plt.imshow(weighted_square[b_idx], cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now make some predictions that we want to break the ECE metric.\n",
    "# The way we do this is by making predictions in the same bin have wildly different accuracies.\n",
    "# First we make a confidence map where the boundary of the square is 0.6 and the middle is 0.1\n",
    "# and the rest if 0\n",
    "fore_conf_map1= torch.zeros(10, 10)\n",
    "fore_conf_map1[3:7, 3:7] = 0.5\n",
    "fore_conf_map1[4:6, 4:6] = 1.0\n",
    "back_conf_map1 = 1 - fore_conf_map1\n",
    "chann_conf_map1 = torch.stack([back_conf_map1, fore_conf_map1], dim=0)\n",
    "# Get the argmax\n",
    "conf_map1 = torch.max(chann_conf_map1, dim=0).values\n",
    "\n",
    "plt.imshow(fore_conf_map1, vmin=0, vmax=1, cmap='gray')\n",
    "plt.colorbar(label=\"Confidence\")\n",
    "plt.title(\"Confidence Map 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we adversarily make our pred predict exactly half of the 0.5 area correctly (the top left) and the other half incorrectly (the bottom right).\n",
    "pred_map1 = torch.argmax(chann_conf_map1, dim=0)\n",
    "\n",
    "plt.imshow(pred_map1, cmap='gray')\n",
    "plt.title(\"Prediction Map 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_map2 = torch.zeros(10, 10)\n",
    "pred_map2[6:8, 4:6] = 1.0\n",
    "\n",
    "plt.imshow(pred_map2, vmin=0, vmax=1, cmap='gray')\n",
    "plt.colorbar(label=\"Confidence\")\n",
    "plt.title(\"Confidence Map 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_map1= pred_map1[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_map1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.metrics.utils import agg_neighbors_preds\n",
    "\n",
    "num_neighbors_constant_boundary = agg_neighbors_preds(\n",
    "    pred_map1, \n",
    "    neighborhood_width=3,\n",
    "    class_wise=False,\n",
    "    discrete=True,\n",
    "    binary=False\n",
    ")\n",
    "\n",
    "plt.imshow(num_neighbors_constant_boundary.squeeze())\n",
    "plt.title(\"Num Matching Neighbors Map 1 (0 pad)\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.metrics.utils import agg_neighbors_preds\n",
    "\n",
    "num_neighbors_constant_boundary = agg_neighbors_preds(\n",
    "    pred_map1, \n",
    "    neighborhood_width=3,\n",
    "    class_wise=True,\n",
    "    num_classes=2,\n",
    "    discrete=True,\n",
    "    binary=False\n",
    ")\n",
    "\n",
    "plt.imshow(num_neighbors_constant_boundary.squeeze()[0])\n",
    "plt.title(\"Num Matching Neighbors Map 0 (0 pad)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(num_neighbors_constant_boundary.squeeze()[1])\n",
    "plt.title(\"Num Matching Neighbors Map 1 (0 pad)\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.metrics.utils import agg_neighbors_preds\n",
    "\n",
    "num_neighbors_constant_boundary = agg_neighbors_preds(\n",
    "    pred_map1, \n",
    "    neighborhood_width=3,\n",
    "    num_classes=2,\n",
    "    class_wise=True,\n",
    "    discrete=True,\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "plt.imshow(num_neighbors_constant_boundary.squeeze()[0])\n",
    "plt.title(\"Num Matching Neighbors Map 0 (0 pad)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(num_neighbors_constant_boundary.squeeze()[1])\n",
    "plt.title(\"Num Matching Neighbors Map 1 (0 pad)\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.metrics.utils import agg_neighbors_preds\n",
    "\n",
    "num_neighbors_constant_boundary = agg_neighbors_preds(\n",
    "    pred_map1.float(), \n",
    "    neighborhood_width=3,\n",
    "    class_wise=False,\n",
    "    discrete=False,\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "plt.imshow(num_neighbors_constant_boundary.squeeze())\n",
    "plt.title(\"Num Matching Neighbors Map 1 (0 pad)\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.metrics.utils import count_matching_neighbors\n",
    "\n",
    "num_neighbors_constant_boundary = count_matching_neighbors(pred_map1[None], neighborhood_width=3, binary=True)\n",
    "\n",
    "plt.imshow(num_neighbors_constant_boundary.squeeze())\n",
    "plt.title(\"Num Matching Neighbors Map 1 (0 pad)\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = torch.stack([pred_map1, pred_map2], dim=0)\n",
    "combined = count_matching_neighbors(combined)\n",
    "\n",
    "plt.imshow(combined[0])\n",
    "plt.title(\"Num Matching Neighbors Map 1 (0 pad)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(combined[1])\n",
    "plt.title(\"Num Matching Neighbors Map 2 (0 pad)\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.metrics.utils import count_matching_neighbors\n",
    "\n",
    "num_neighbors = count_matching_neighbors(pred_map1, reflect_boundaries=True)\n",
    "\n",
    "plt.imshow(num_neighbors)\n",
    "plt.title(\"Num Matching Neighbors Map 1 (reflect pad)\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.metrics.utils import get_nw_pix_props\n",
    "\n",
    "pixel_props = get_nw_pix_props(\n",
    "    pred_map1, \n",
    "    num_neighbors_map=num_neighbors\n",
    "    )\n",
    "\n",
    "plt.imshow(pixel_props)\n",
    "plt.title(\"Pixel Proportions\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_props.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert abs(pixel_props.sum() - 100) < 0.0003, \"Weights should sum to 100, got sum = {}\".format(pixel_props.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paradoxically, this will have an ECE of 0!\n",
    "cal_err = TL_ECE(\n",
    "    num_bins=10,\n",
    "    pred_map=pred_map1,\n",
    "    conf_map=conf_map1,\n",
    "    label_map=ground_truth1\n",
    ")[\"cal_score\"]\n",
    "print(\"ECE: \", cal_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I would actually argue this is a bad adversary because this is what calibration is meant to do, half of the pixels at the border are correct and half are wrong... thus 0.5. But it IS illustrative of a different problem, that is that different parts of the image and somehow compensate for each other. For example, working still with one object, consider the following example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: A Boundary Case ... Literally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the ground truth, which is a 10x10 tensor with a white square in the middle\n",
    "# we will slice off the right side of the square to make our contradiction.\n",
    "ground_truth2 = torch.zeros(10, 10)\n",
    "ground_truth2[2:8, 2:8] = 1\n",
    "\n",
    "plt.imshow(ground_truth2, cmap='gray')  \n",
    "plt.title('Ground Truth 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now make some predictions that we want to break the ECE metric.\n",
    "# The way we do this is by making predictions in the same bin have wildly different accuracies.\n",
    "# First we make a confidence map where the boundary of the square is 0.6 and the middle is 0.1\n",
    "# and the rest if 0\n",
    "fore_conf_map2 = torch.zeros(10, 10)\n",
    "fore_conf_map2[1:9, 1:9] = 0.5\n",
    "fore_conf_map2[3:7, 3:7] = 1 \n",
    "fore_conf_map2[3, 3:7] = 0.5\n",
    "fore_conf_map2[6, 3:7] = 0.5\n",
    "\n",
    "back_conf_map2 = 1 - fore_conf_map2\n",
    "chann_conf_map2 = torch.stack([back_conf_map2, fore_conf_map2], dim=0)\n",
    "# Get the argmax\n",
    "conf_map2 = torch.max(chann_conf_map2, dim=0).values\n",
    "\n",
    "plt.imshow(fore_conf_map2, vmin=0, vmax=1, cmap='gray')\n",
    "plt.colorbar(label=\"Confidence\")\n",
    "plt.title(\"Confidence Map 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we adversarily make our pred predict exactly half of the 0.5 area correctly (the top left) and the other half incorrectly (the bottom right).\n",
    "pred_map2 = torch.argmax(chann_conf_map2, dim=0)\n",
    "plt.imshow(pred_map2, cmap='gray')\n",
    "plt.title(\"Prediction Map 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paradoxically, this will have an ECE of 0!\n",
    "cal_err = TL_ECE(\n",
    "    num_bins=10,\n",
    "    pred_map=pred_map2,\n",
    "    conf_map=conf_map2,\n",
    "    label_map=ground_truth2\n",
    ")[\"cal_score\"]\n",
    "\n",
    "print(\"ECE: \", cal_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we observe a different kind of failure case. Regardless of how we choose to decide the coinflip at the label, the border of pixels can be COMPLETELY incorrect and ECE report the model is perfectly calibrated. This would be ok, except that pixels at the boundary with low confidence are significantly more likely to be wrong in comparison to pixels which are more inwards, thus this failure case means that perfectly calibrated models CAN just ignore the boundary as long as the distribution of confidence goes slightly inwards.\n",
    "\n",
    "## Maybe a way to show how much a problem this can be is we look at how the confidence changes as we go further from the boundary and the accuracy going from the boundary, if the accuracy line goes up BEFORE the confidence line then this problem can be present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: The complications of multiple structures.\n",
    "\n",
    "#### In this example, we will study what happens when we introduce additional structures into the mix, and that they in fact can ALSO hide errors with the boundaries (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the ground truth, which is a 10x10 tensor with a white square in the middle\n",
    "# we will slice off the right side of the square to make our contradiction.\n",
    "ground_truth3 = torch.zeros(10, 10)\n",
    "ground_truth3[6, 6] = 1\n",
    "ground_truth3[1:5, 1:3] = 1\n",
    "\n",
    "plt.imshow(ground_truth3, cmap='gray')  \n",
    "plt.title('Ground Truth 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fore_conf_map3 = torch.zeros(10, 10)\n",
    "fore_conf_map3[5:8, 5:8] = 0.5 \n",
    "fore_conf_map3[6, 6] = 1\n",
    "fore_conf_map3[1:5, 1:3] = 0.5 \n",
    "\n",
    "back_conf_map3 = 1 - fore_conf_map3\n",
    "chann_conf_map3 = torch.stack([back_conf_map3, fore_conf_map3], dim=0)\n",
    "# Get the argmax\n",
    "conf_map3 = torch.max(chann_conf_map3, dim=0).values\n",
    "\n",
    "plt.imshow(fore_conf_map3, vmin=0, vmax=1, cmap='gray')\n",
    "plt.colorbar(label=\"Confidence\")\n",
    "plt.title(\"Confidence Map 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we adversarily make our pred predict exactly half of the 0.5 area correctly (the top left) and the other half incorrectly (the bottom right).\n",
    "pred_map3 = torch.argmax(chann_conf_map3, dim=0)\n",
    "plt.imshow(pred_map3, cmap='gray')\n",
    "plt.title(\"Possible Pred Map 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paradoxically, this will have an ECE of 0!\n",
    "cal_err_3 = TL_ECE(\n",
    "    num_bins=10,\n",
    "    pred_map=pred_map3,\n",
    "    conf_map=conf_map3,\n",
    "    label_map=ground_truth3\n",
    ")[\"cal_score\"]\n",
    "\n",
    "print(\"ECE: \", cal_err_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is illustrative here is that the boundary of some objects can hide the miscalibration of others. I think the simplest possible conditioning is really the pixel distance to the nearest class-boundary. But this unfortunately doesn't catch this 3rd case... what would we like to say?\n",
    "\n",
    "### Namely, do we care if different blobs have entirely different accuracies? Or, better yet, is there some difference between the size of the prediction region and the rate at which the confidence/accuracy converge, going inwards from the boundary?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
