{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "\n",
    "# Results loader object does everything\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from pathlib import Path\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "log:\n",
    "    root: /storage/vbutoi/scratch/ESE/inference\n",
    "    load_pixel_meters: False \n",
    "    remove_shared_columns: False\n",
    "    add_dice_loss_rows: True\n",
    "    drop_nan_metric_rows: True \n",
    "    add_baseline_rows: True \n",
    "    equal_rows_per_cfg_assert: True \n",
    "    inference_group: \"02_07_24_WMH_NewMetrics\"\n",
    "    min_fg_pixels: 100\n",
    "    \n",
    "calibration:\n",
    "    num_bins: 15\n",
    "    square_diff: False \n",
    "    neighborhood_width: 3\n",
    "    metric_cfg_file: \"/storage/vbutoi/projects/ESE/ese/experiment/configs/inference/Calibration_Metrics.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.analysis.analyze_inf import load_cal_inference_stats\n",
    "\n",
    "image_info_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg,\n",
    "    load_cached=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on the cal split\n",
    "image_info_df = image_info_df[image_info_df['split'] == 'cal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to see if there is any hope with having better ECE/ELM makes better ensembles. Note that this isn't a conclusive result just because the number of samples per images that are used to calculate ECE/ELM are not sufficient to get actual statistical quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to add to each row a column that is the difference betweeen the row's metric_score and the metric_score corresponding to the same image metric as mean uncalibrated UNet performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the rows corresponding to a unet aveages across multiple seeds with no calibration.\n",
    "average_unet_row = image_info_df[(image_info_df['pretrained_seed'] == 'Average') & (image_info_df['calibrator'] == 'Uncalibrated')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_datapoint_cols = ['data_id', 'slice_idx', 'image_metric', 'groupavg_image_metric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that for each datapoint we only have one average unet row.\n",
    "num_avg_unets_per_datapoint = average_unet_row.groupby(unique_datapoint_cols).size()\n",
    "assert num_avg_unets_per_datapoint.max() == 1,\\\n",
    "    f\"There should be only one row for each data_id, slice_idx, image_metric, and groupavg image metric combination, got {num_avg_unets_per_datapoint}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are no NaNs in the average UNet rows.\n",
    "assert average_unet_row['metric_score'].isna().sum() == 0, \"There should be no NaNs in metric_score of UNet rows.\"\n",
    "assert average_unet_row['groupavg_metric_score'].isna().sum() == 0, \"There should be no NaNs in groupavg_metric_score of UNet rows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Merge based on 'image_metric', 'subject_id', and 'slice_idx'\n",
    "info_df_w_avg_unet_cols = pd.merge(\n",
    "    image_info_df, \n",
    "    average_unet_row[unique_datapoint_cols + ['metric_score', 'groupavg_metric_score']], \n",
    "    on=unique_datapoint_cols, \n",
    "    how='left', \n",
    "    suffixes=('', '_average_unet')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate the difference\n",
    "info_df_w_avg_unet_cols['metric_delta'] = info_df_w_avg_unet_cols['metric_score'] - info_df_w_avg_unet_cols['metric_score_average_unet'] # Current - Baseline\n",
    "info_df_w_avg_unet_cols['groupavg_metric_delta'] = info_df_w_avg_unet_cols['groupavg_metric_score'] - info_df_w_avg_unet_cols['groupavg_metric_score_average_unet'] # Current - Baseline\n",
    "# Drop those columns\n",
    "info_df_w_delta = info_df_w_avg_unet_cols.drop(columns=['metric_score_average_unet', 'groupavg_metric_score_average_unet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can look at trends! We want to make some scatterplots to look at relationships between calibration scores and their relative improvement over the baseline.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we want only the rows corresponding to group metrics, no longer looking at seeds.\n",
    "grouped_models_df = info_df_w_delta[info_df_w_delta['model_type'] == 'group'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECKS, MAKE SURE THAT FOR CALIBRATORS UNCALIBRATED, TEMPERATURE_SCALING, LTS\n",
    "for calibrator in [\"Uncalibrated\", \"Temperature_Scaling\", \"LTS\"]:\n",
    "    unique_qual_metrics = grouped_models_df[grouped_models_df['metric_type'] == 'quality']['image_metric'].unique()\n",
    "    for quality_metric in unique_qual_metrics:\n",
    "        # Checkign that the delta is 0 for the calibrator and the quality_metric\n",
    "        rows = grouped_models_df[\n",
    "            (grouped_models_df['calibrator'] == calibrator) & \n",
    "            (grouped_models_df['image_metric'] == quality_metric) &\n",
    "            (grouped_models_df['method_name'] == 'UNet (seed=Average)')\n",
    "        ]\n",
    "        assert (rows['metric_delta'] == 0).all(),\\\n",
    "            f\"Delta from base should be 0 for the calibrator {calibrator} and the quality metric {quality_metric}, got {rows['metric_delta']}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a bunch of new rows where the image_metric is the groupavg_image_metric and the metric_score is the groupavg_metric_score and the metric_delta is the groupavg_metric_delta\n",
    "groupavg_rows = grouped_models_df.copy()\n",
    "groupavg_rows['image_metric'] = groupavg_rows['groupavg_image_metric']\n",
    "groupavg_rows['metric_score'] = groupavg_rows['groupavg_metric_score']\n",
    "groupavg_rows['metric_delta'] = groupavg_rows['groupavg_metric_delta']\n",
    "# Drop the groupavg columns\n",
    "standard_image_rows = grouped_models_df.drop(columns=['groupavg_image_metric', 'groupavg_metric_score', 'groupavg_metric_delta']) \n",
    "groupavg_rows = groupavg_rows.drop(columns=['groupavg_image_metric', 'groupavg_metric_score', 'groupavg_metric_delta'])\n",
    "# Concatenate the two\n",
    "grouped_models_df = pd.concat([standard_image_rows, groupavg_rows], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table with 'metric_type' as columns\n",
    "pivot_grouped_models_df = grouped_models_df.pivot_table(\n",
    "    index=['configuration', 'method_name', 'calibrator', 'data_id', 'slice_idx'],\n",
    "    values=['metric_score', 'metric_delta'], \n",
    "    columns=['metric_type', 'image_metric'], \n",
    "    aggfunc='mean'\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy so that we can modify the column names\n",
    "pivot_perf_per_datpoint = pivot_grouped_models_df.copy()\n",
    "# Make new column names.\n",
    "new_cols = []\n",
    "for col in pivot_grouped_models_df.columns.values:\n",
    "    if col[0] == 'metric_delta':\n",
    "        new_cols.append(f'delta_{col[-1]}')\n",
    "    elif col[-1] == '':\n",
    "        new_cols.append(col[0])\n",
    "    else:\n",
    "        new_cols.append(col[-1])\n",
    "# Set the column names to be the lowest non empty level per column in the multi-index\n",
    "pivot_perf_per_datpoint.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_perf_per_datpoint['method_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_perf_per_datpoint['calibrator'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_perf_per_datpoint['method_name'] = pivot_perf_per_datpoint['method_name'].astype('category')\n",
    "pivot_perf_per_datpoint['method_name'] = pivot_perf_per_datpoint['method_name'].cat.reorder_categories([\n",
    "    'Average UNet',\n",
    "    'Ensemble (mean, logits)', \n",
    "    'Ensemble (mean, probs)', \n",
    "    'Ensemble (product, probs)', \n",
    "    ])\n",
    "\n",
    "\n",
    "pivot_perf_per_datpoint['calibrator'] = pivot_perf_per_datpoint['calibrator'].astype('category')\n",
    "pivot_perf_per_datpoint['calibrator'] = pivot_perf_per_datpoint['calibrator'].cat.reorder_categories([\n",
    "    'Uncalibrated',\n",
    "    'Vanilla',\n",
    "    'Temperature_Scaling', \n",
    "    'Vector_Scaling', \n",
    "    'Dirichlet_Scaling',\n",
    "    'LTS', \n",
    "    'NECTAR_Scaling'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at change in *predicted ensemble* calibration vs change in Dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that each subplot will have the same number of datapoints on it.\n",
    "num_per_config = pivot_perf_per_datpoint.groupby(['method_name', 'calibrator']).size()\n",
    "assert len(num_per_config.unique()) == 1,\\\n",
    "    f\"Each subplot should have the same number of datapoints on it, got {num_per_config}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_per_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.experiment.analysis.analysis_utils.plot_utils import plot_method_vs_calibrator_scatterplots, clump_df_datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_perf_per_datpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clump_points = False\n",
    "\n",
    "for calibration_metric in ['ECE', 'CW-ECE', 'Edge-ECE', 'ECW-ECE', 'Edge-ELM', 'ELM']:\n",
    "    for quality_metric in ['delta_Dice', 'delta_HD95', 'delta_BoundaryIOU']:\n",
    "        # for foreground_option in ['', 'Foreground-']:\n",
    "        for foreground_option in ['', 'Foreground-']:\n",
    "            # for group_option in ['', 'GroupAvg_']:\n",
    "            x_metric_name = f'delta_GroupAvg_Image_{foreground_option}{calibration_metric}'\n",
    "            y_metric_name = quality_metric \n",
    "            if clump_points:\n",
    "                # Bin the predictions\n",
    "                num_bins = 90\n",
    "                # Gather points into super-points.\n",
    "                clumped_df = clump_df_datapoints(\n",
    "                    pivot_perf_per_datpoint, \n",
    "                    num_bins=50, \n",
    "                    x='method_name', \n",
    "                    y='calibrator',\n",
    "                    x_metric=x_metric_name,\n",
    "                    y_metric=y_metric_name\n",
    "                )\n",
    "                # Assert that the number of points in the plot is the same\n",
    "                num_per_config = clumped_df.groupby(['method_name', 'calibrator']).size()\n",
    "                assert num_per_config.max() == num_per_config.min(),\\\n",
    "                    f\"Each subplot should have the same number of datapoints on it, got {num_per_config}.\"\n",
    "                # Verify that there are no NaNs in the binned_pivot_per_datapoint for rows of the chosen y metric\n",
    "                assert clumped_df[y_metric_name].isna().sum() == 0,\\\n",
    "                    f\"There should be no NaNs in {y_metric_name} in binned_pivot_per_datapoint, got {clumped_df[y_metric_name]}.\"\n",
    "                plot_df = clumped_df\n",
    "            else:\n",
    "                plot_df = pivot_perf_per_datpoint \n",
    "\n",
    "            # Plot the relationship between the two metrics\n",
    "            plot_method_vs_calibrator_scatterplots(\n",
    "                df=plot_df, \n",
    "                x=x_metric_name, \n",
    "                y=y_metric_name,\n",
    "                sharex=False,\n",
    "                sharey=False,\n",
    "                height=5\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
