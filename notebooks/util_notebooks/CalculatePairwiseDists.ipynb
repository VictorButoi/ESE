{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Define some useful paths.\n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "       '/storage'\n",
    "))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load our dataset and look at examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml train_dataset_cfg\n",
    "\n",
    "task: \"WBC/CV/EM/0\"\n",
    "# task: \"ACDC/Challenge2017/MRI/2\"\n",
    "split: \"train\"\n",
    "return_data_id: True\n",
    "label_threshold: 0.5\n",
    "slicing: \"midslice\"\n",
    "resolution: 128 \n",
    "label: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml val_dataset_cfg\n",
    "\n",
    "task: \"WBC/CV/EM/0\"\n",
    "# task: \"ACDC/Challenge2017/MRI/2\"\n",
    "split: \"val\"\n",
    "return_data_id: True\n",
    "label_threshold: 0.5\n",
    "slicing: \"midslice\"\n",
    "resolution: 128 \n",
    "label: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese.datasets import Segment2D\n",
    "\n",
    "# Load the training and validation datasets\n",
    "train_dataset = Segment2D(**train_dataset_cfg)\n",
    "val_dataset = Segment2D(**val_dataset_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num train examples:\", len(train_dataset))\n",
    "print(\"Num val examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_im = train_dataset[0]['img'][None]\n",
    "val_im = val_dataset[0]['img'][None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the images\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(train_im.squeeze(), cmap='gray')\n",
    "plt.title(\"Training Image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(val_im.squeeze(), cmap='gray')\n",
    "plt.title(\"Validation Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "def preprocess_image(image_tensors, target_size, apply_clip_norm, num_channels=3):\n",
    "    assert len(image_tensors.shape) == 4, \"Input must be a 4D tensor\"\n",
    "    \"\"\"\n",
    "    Preprocess image tensors for CLIP.\n",
    "\n",
    "    Args:\n",
    "        image_tensors (torch.Tensor): Tensor of shape (B, C, H, W).\n",
    "        target_size (int): Desired image size for CLIP.\n",
    "        num_channels (int): Number of channels expected by CLIP (usually 3).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed image tensors.\n",
    "    \"\"\"\n",
    "    # If images are grayscale, repeat the channel to make it 3-channel\n",
    "    if image_tensors.shape[1] == 1 and num_channels == 3:\n",
    "        image_tensors = image_tensors.repeat(1, 3, 1, 1)\n",
    "    \n",
    "    if apply_clip_norm:\n",
    "        # Define the preprocessing pipeline\n",
    "        preprocess_transform = T.Compose([\n",
    "            T.Resize(target_size, interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.CenterCrop(target_size),\n",
    "            T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                        std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "    else:\n",
    "        # Define the preprocessing pipeline\n",
    "        preprocess_transform = T.Compose([\n",
    "            T.Resize(target_size, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        ])\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    return preprocess_transform(image_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocess the images before we pass into our embedding module.\n",
    "proc_train_im = preprocess_image(train_im, apply_clip_norm=True, target_size=224)\n",
    "proc_val_im = preprocess_image(val_im, apply_clip_norm=True, target_size=224)\n",
    "\n",
    "# move the channel dim to the end\n",
    "vis_train_im = proc_train_im.squeeze().permute(1, 2, 0)\n",
    "vis_val_im = proc_val_im.squeeze().permute(1, 2, 0)\n",
    "# Print the range of pixel values\n",
    "print(\"Training Image Pixel Range:\", vis_train_im.min(), vis_train_im.max())\n",
    "print(\"Validation Image Pixel Range:\", vis_val_im.min(), vis_val_im.max())\n",
    "# visualize the processed images\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(vis_train_im)\n",
    "plt.title(\"Training Image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(vis_val_im)\n",
    "plt.title(\"Validation Image\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Load and prepare the functions for the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip \n",
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "def load_model(model_type: str, device: Optional[Any] = None) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained DINOv2 model from the timm library.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'dino' in model_type:\n",
    "            print(\"Loading DINOv2 model...\")\n",
    "            # List available DINOv2 models\n",
    "            available_models = timm.list_models('*dinov2*', pretrained=True)\n",
    "            if not available_models:\n",
    "                raise ValueError(\"No DINOv2 models found in timm. Please ensure timm is updated and supports DINOv2.\")\n",
    "            \n",
    "            # Select a specific DINOv2 model, e.g., 'dinov2_vitl14'\n",
    "            model = timm.create_model('vit_large_patch14_dinov2.lvd142m', pretrained=True)\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            if device is not None:\n",
    "                model.to(device)\n",
    "            return model\n",
    "        elif 'clip' in model_type:\n",
    "            print(\"Loading CLIP model...\")\n",
    "            model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "            return model\n",
    "        else:\n",
    "            raise ValueError(f\"Model type {model_type} not supported.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_type}: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_dino_embedding(model, image_tensor, device):\n",
    "    \"\"\"\n",
    "    Passes the image tensor through the model to obtain the embedding.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        # Forward pass\n",
    "        # Depending on the model architecture, you might need to access a specific layer.\n",
    "        # For simplicity, we'll assume the model's forward method returns the desired embedding.\n",
    "        embedding = model.forward_features(image_tensor)\n",
    "        # Flatten the embedding if necessary\n",
    "        embedding = embedding.view(embedding.size(0), -1)\n",
    "        # Normalize the embedding\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding.cpu()\n",
    "\n",
    "def get_clip_embedding(model, image_tensor, device):\n",
    "    \"\"\"\n",
    "    Passes the image tensor through the model to obtain the embedding.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        # Forward pass\n",
    "        # Depending on the model architecture, you might need to access a specific layer.\n",
    "        # For simplicity, we'll assume the model's forward method returns the desired embedding.\n",
    "        embedding = model.encode_image(image_tensor)\n",
    "        # Normalize the embedding\n",
    "        embedding /= embedding.norm(dim=-1, keepdim=True)\n",
    "        return embedding.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "# model_type = \"dino\"\n",
    "model_type = \"clip\"\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_type, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to build dictionaries that map from the image ID to the image\n",
    "train_dataid_to_im = {train_dataset[i]['data_id']: train_dataset[i]['img'][None] for i in range(len(train_dataset))}\n",
    "val_dataid_to_im = {val_dataset[i]['data_id']: val_dataset[i]['img'][None] for i in range(len(val_dataset))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "embedding_fn = get_dino_embedding if model_type == \"dino\" else get_clip_embedding\n",
    "\n",
    "records = []\n",
    "for val_data_id, val_image in tqdm(val_dataid_to_im.items(), desc='Computing similiarity over validation images'):\n",
    "    # Compute the embedding\n",
    "    if model_type == \"dino\":\n",
    "        proc_val_im = preprocess_image(val_image, apply_clip_norm=False, target_size=518)\n",
    "    elif model_type == \"clip\":\n",
    "        proc_val_im = preprocess_image(val_image, apply_clip_norm=True, target_size=224)\n",
    "    # Compute the similarity between the validation image and each training image\n",
    "    embedding_val = embedding_fn(model, proc_val_im, device)\n",
    "    # Compute the similarity between the validation image and each training image\n",
    "    for train_data_id, train_image in tqdm(train_dataid_to_im.items(), desc='Computing similarity over training images', leave=False):\n",
    "        # Compute the embedding\n",
    "        if model_type == \"dino\":\n",
    "            proc_train_im = preprocess_image(train_image, apply_clip_norm=False, target_size=518)\n",
    "        elif model_type == \"clip\":\n",
    "            proc_train_im = preprocess_image(train_image, apply_clip_norm=True, target_size=224)\n",
    "        # Compute the embedding on training\n",
    "        embedding_train = embedding_fn(model, proc_train_im, device)\n",
    "        # Compute the cosine similarity between the embeddings\n",
    "        similarity = torch.nn.functional.cosine_similarity(embedding_val, embedding_train).item()\n",
    "        # Store the similarity in a dictionary\n",
    "        records.append({\n",
    "            'val_data_id': val_data_id,\n",
    "            'train_data_id': train_data_id,\n",
    "            'similarity': similarity\n",
    "        })\n",
    "\n",
    "# Convert the records to a DataFrame\n",
    "similarity_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_images = 8\n",
    "# For each validation image, find the 5 most similar training images and plot them side by side\n",
    "for val_id in similarity_df['val_data_id'].unique():\n",
    "    val_id_df = similarity_df[similarity_df['val_data_id'] == val_id]\n",
    "    # Get the 5 most similar training images by the similarity score\n",
    "    most_similar_train_ids = val_id_df.sort_values(by='similarity', ascending=False)['train_data_id'].values[:num_train_images]\n",
    "    # Get the validation image\n",
    "    val_image = val_dataid_to_im[val_id].squeeze()\n",
    "    # Get the most similar training images\n",
    "    train_images = [train_dataid_to_im[train_id].squeeze() for train_id in most_similar_train_ids]\n",
    "    # Plot the images\n",
    "    plt.figure(figsize=(3*(num_train_images + 1), 3))\n",
    "    plt.subplot(1, num_train_images + 1, 1)\n",
    "    plt.imshow(val_image, cmap='gray')\n",
    "    plt.title(\"Validation Image\")\n",
    "    for i, train_image in enumerate(train_images):\n",
    "        plt.subplot(1, num_train_images + 1, i+2)\n",
    "        plt.imshow(train_image, cmap='gray')\n",
    "        plt.title(f\"Train Image {i+1}: Sim {val_id_df[val_id_df['train_data_id'] == most_similar_train_ids[i]]['similarity'].values[0]:.2f}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
