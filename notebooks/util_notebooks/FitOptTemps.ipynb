{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "# Ionpy imports\n",
    "from ionpy.analysis import ResultsLoader\n",
    "# Local imports\n",
    "from ese.analysis.baselines import fit_posthoc_calibrators, viz_posthoc_calibrators\n",
    "from ese.analysis.analyze_inf import load_cal_inference_stats\n",
    "from ese.analysis.analysis_utils.parse_sweep import get_global_optimal_parameter, get_per_subject_optimal_values\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "log:\n",
    "    # root: '/storage/vbutoi/scratch/ESE/inference/10_26_24_OCTA_Benchmark'\n",
    "    # root: '/storage/vbutoi/scratch/ESE/inference/10_26_24_ISLES_Benchmark'\n",
    "    # root: '/storage/vbutoi/scratch/ESE/inference/10_26_24_Roads_FULLRES_Benchmark'\n",
    "    root: '/storage/vbutoi/scratch/ESE/inference/10_26_24_WMH_Benchmark'\n",
    "    inference_group: 'Sweep_Temperature'\n",
    "\n",
    "options:\n",
    "    verify_graceful_exit: True\n",
    "    equal_rows_per_cfg_assert: False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg,\n",
    "    load_cached=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    'soft_RAVE',\n",
    "    'experiment_model_dir',\n",
    "    'temperature',\n",
    "    'hard_volume',\n",
    "    'soft_volume',\n",
    "    'gt_volume',\n",
    "    'data_id',\n",
    "    'split'\n",
    "]\n",
    "# Filter out the columns we want to keep\n",
    "exp_df = inference_df[cols_to_keep].drop_duplicates().reset_index(drop=True)\n",
    "# We need to make sure that the cal split goes first.\n",
    "exp_df = exp_df.sort_values('split', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# This cell is quite important, it allows us to see the base soft volume for each data_id and loss_func_class\n",
    "##############################################################################################################\n",
    "# We want to add a base soft volume column to let us so what the uncalibrated volume is.\n",
    "bsv = exp_df[exp_df['temperature'] == 1.01][['data_id', 'experiment_model_dir', 'soft_volume']].drop_duplicates().reset_index(drop=True)\n",
    "# Make a new column that is the combination of data_id and loss_func_class\n",
    "bsv['data_id_experiment_model_dir'] = bsv['data_id'] + '_' + bsv['experiment_model_dir']\n",
    "exp_df['data_id_experiment_model_dir'] = exp_df['data_id'] + '_' + exp_df['experiment_model_dir']\n",
    "# Drop the columns data_id and experiment_model_dir\n",
    "bsv = bsv.drop(columns=['data_id', 'experiment_model_dir'])\n",
    "# Convert this to a dictionary mapping from data_id to base soft volume\n",
    "bsv_dict = dict(zip(bsv['data_id_experiment_model_dir'], bsv['soft_volume']))\n",
    "# Make a new column of exp_df, called base_soft_volume, where the value is the corresponding value for the data_id of that row in the bsv_dict\n",
    "exp_df['base_soft_volume'] = exp_df['data_id_experiment_model_dir'].map(bsv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_opt_temp_df = get_global_optimal_parameter(\n",
    "    exp_df, \n",
    "    sweep_key='temperature', \n",
    "    y_key='soft_RAVE',\n",
    "    group_keys=['split', 'experiment_model_dir']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_scores, opt_temps_df = get_per_subject_optimal_values(\n",
    "    exp_df, \n",
    "    sweep_key='temperature', \n",
    "    y_key='soft_RAVE',\n",
    "    group_keys=['split', 'experiment_model_dir'],\n",
    "    keep_keys=['hard_volume', 'base_soft_volume'],\n",
    "    return_optimal_values=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_dir in opt_temps_df['experiment_model_dir'].unique():\n",
    "    # We also want to add our baseline of predicting a global optimal temperature\n",
    "    loss_fn_global_temp = global_opt_temp_df[global_opt_temp_df['experiment_model_dir'] == model_dir]\n",
    "    cal_global_opt_temp = float(loss_fn_global_temp[loss_fn_global_temp['split'] == 'cal']['temperature'])\n",
    "\n",
    "    # Get the subset of the temps corresponding to one model.\n",
    "    model_temps_df = opt_temps_df[opt_temps_df['experiment_model_dir'] == model_dir]\n",
    "\n",
    "    # Split the data into training and validation sets based on 'split' column\n",
    "    model_df_train = model_temps_df[model_temps_df['split'] == 'cal']\n",
    "    model_df_val = model_temps_df[model_temps_df['split'] == 'val']\n",
    "\n",
    "    # Prepare the features (X) and target (y)\n",
    "    X_train = model_df_train['hard_volume'].to_numpy().reshape(-1, 1)\n",
    "    y_train = model_df_train['temperature']\n",
    "    # \n",
    "    X_val = model_df_val['hard_volume'].to_numpy().reshape(-1, 1)\n",
    "    y_val = model_df_val['temperature']\n",
    "\n",
    "    # Fit the calibrators and get the results\n",
    "    results, fitted_models = fit_posthoc_calibrators(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        global_opt_temp=cal_global_opt_temp\n",
    "    )\n",
    "\n",
    "    # Convert results to a DataFrame for better visualization\n",
    "    results_df = pd.DataFrame(results).T  # Transpose for better format\n",
    "    # We want to srt by the best MSE\n",
    "    results_df = results_df.sort_values('MSE')\n",
    "    print(\"Model Performance for model_dir:\", model_dir)\n",
    "    display(results_df)\n",
    "\n",
    "    # Now we want to see the actual fits\n",
    "    viz_posthoc_calibrators(\n",
    "        fitted_models, \n",
    "        X_data=X_val, \n",
    "        y_data=y_val, \n",
    "        global_temp=cal_global_opt_temp\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
