{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "# Ionpy imports\n",
    "from ionpy.analysis import ResultsLoader\n",
    "# Local imports\n",
    "from ese.analysis.analyze_inf import load_cal_inference_stats\n",
    "from ese.analysis.analysis_utils.plot_utils import get_prop_color_palette\n",
    "from ese.analysis.analysis_utils.parse_sweep import get_global_optimal_parameter, get_per_subject_optimal_values\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "pd.set_option('display.max_rows', 50)\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "log:\n",
    "    root: '/storage/vbutoi/scratch/ESE/inference/11_05_24_UVS_InContext_CrossEval'\n",
    "    inference_group: \n",
    "        - 'Sweep_Threshold'\n",
    "        # - 'Sweep_Temperature'\n",
    "\n",
    "options:\n",
    "    verify_graceful_exit: True\n",
    "    equal_rows_per_cfg_assert: False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful cell for controlling the plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######This cells controls what gets plotted in the following cells so we don't have to change each one\n",
    "x_key = 'threshold'\n",
    "y_key = 'hard_RAVE'\n",
    "xtick_range = np.arange(0, 1.1, 0.1)\n",
    "cmap = 'viridis_r'\n",
    "aspect = 1\n",
    "x_lims = (0, 1)\n",
    "y_lims = (0.0, 2)\n",
    "\n",
    "# x_key = 'temperature'\n",
    "# y_key = 'soft_RAVE'\n",
    "# xtick_range = np.arange(0, 3.1, 0.1)\n",
    "# cmap = 'magma_r'\n",
    "# aspect = 2\n",
    "# x_lims = (0, 3.0)\n",
    "# y_lims = (-0.5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg,\n",
    "    load_cached=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the rows where the image_metric is 'Dice'\n",
    "inference_df = inference_df[inference_df['image_metric'] == 'Dice']\n",
    "# Rename the column metric score for this new df to Dice\n",
    "inference_df = inference_df.rename(columns={'metric_score': 'Dice'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df['split'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ikey in inference_df.keys():\n",
    "#     print(ikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in inference_df['inference_data_task'].unique():\n",
    "    print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    'soft_abs_area_estimation_error',\n",
    "    'hard_abs_area_estimation_error',\n",
    "    'soft_RAVE',\n",
    "    'hard_RAVE',\n",
    "    'log_soft_RAVE',\n",
    "    'log_hard_RAVE',\n",
    "    'Dice',\n",
    "    'inference_data_task',\n",
    "    'loss_func_class',\n",
    "    'threshold',\n",
    "    'temperature',\n",
    "    'hard_volume',\n",
    "    'soft_volume',\n",
    "    'gt_volume',\n",
    "    'data_id',\n",
    "    'split'\n",
    "]\n",
    "# Filter out the columns we want to keep\n",
    "exp_df = inference_df[cols_to_keep].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to make sure that the cal split goes first.\n",
    "exp_df = exp_df.sort_values('split', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20))\n",
    "# We want to plot the mean error vs temperature\n",
    "g = sns.relplot(\n",
    "    data=exp_df,\n",
    "    x=x_key,\n",
    "    y=y_key,\n",
    "    col='inference_data_task',\n",
    "    col_wrap=5,\n",
    "    kind='line',\n",
    "    height=10,\n",
    "    aspect=aspect,\n",
    ")\n",
    "# If the x_key is temperature, place a dashed red vertical line at 1.01\n",
    "if x_key == 'temperature':\n",
    "    for ax in g.axes.flat:\n",
    "        ax.axvline(x=1.01, color='r', linestyle='--')\n",
    "else:\n",
    "    for ax in g.axes.flat:\n",
    "        ax.axvline(x=0.5, color='r', linestyle='--')\n",
    "\n",
    "g.set(xticks=xtick_range, xlim=x_lims, ylim=y_lims)\n",
    "# Make a global title using suptitle with some spacing\n",
    "plt.suptitle(f'{y_key} vs {x_key}', fontsize=30)\n",
    "# Add spacing between the title and the plot\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20))\n",
    "# We want to plot the mean error vs temperature\n",
    "g = sns.relplot(\n",
    "    data=exp_df,\n",
    "    x=x_key,\n",
    "    y='Dice',\n",
    "    col='inference_data_task',\n",
    "    col_wrap=5,\n",
    "    kind='line',\n",
    "    height=10,\n",
    "    aspect=aspect,\n",
    "    # legend=(x_key == 'temperature')\n",
    ")\n",
    "# If the x_key is temperature, place a dashed red vertical line at 1.01\n",
    "if x_key == 'temperature':\n",
    "    for ax in g.axes.flat:\n",
    "        ax.axvline(x=1.01, color='r', linestyle='--')\n",
    "else:\n",
    "    for ax in g.axes.flat:\n",
    "        ax.axvline(x=0.5, color='r', linestyle='--')\n",
    "\n",
    "g.set(xticks=xtick_range, xlim=x_lims, ylim=[0, 1])\n",
    "# Make a global title using suptitle with some spacing\n",
    "plt.suptitle(f'Dice vs {x_key}', fontsize=30)\n",
    "# Add spacing between the title and the plot\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_name in exp_df['inference_data_task'].unique():\n",
    "    task_df = exp_df[exp_df['inference_data_task'] == task_name]\n",
    "    # We want to plot the mean error vs temperature\n",
    "    g = sns.relplot(\n",
    "        data=task_df,\n",
    "        x=x_key,\n",
    "        y=y_key,\n",
    "        hue='data_id',\n",
    "        kind='line',\n",
    "        height=8,\n",
    "        aspect=aspect,\n",
    "        legend=False,\n",
    "        palette=get_prop_color_palette(\n",
    "                    task_df, \n",
    "                    hue_key='data_id', \n",
    "                    magnitude_key='gt_volume',\n",
    "                    cmap=cmap\n",
    "                )\n",
    "    )\n",
    "    g.set(xticks=xtick_range, ylim=y_lims)\n",
    "    # Set the title as the task name\n",
    "    plt.title(task_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_name in exp_df['inference_data_task'].unique():\n",
    "    task_df = exp_df[exp_df['inference_data_task'] == task_name]\n",
    "    # We want to plot the mean error vs temperature\n",
    "    g = sns.relplot(\n",
    "        data=task_df,\n",
    "        x=x_key,\n",
    "        y='Dice',\n",
    "        hue='data_id',\n",
    "        kind='line',\n",
    "        height=8,\n",
    "        aspect=aspect,\n",
    "        legend=False,\n",
    "        palette=get_prop_color_palette(\n",
    "                    task_df, \n",
    "                    hue_key='data_id', \n",
    "                    magnitude_key='gt_volume',i\n",
    "                    # cmap=cmapiii\n",
    "                )\n",
    "    )\n",
    "\n",
    "    g.set(xticks=xtick_range, ylim=y_lims)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
