{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/storage/vbutoi/projects')\n",
    "sys.path.append('/storage/vbutoi/libraries')\n",
    "sys.path.append('/storage/vbutoi/projects/ESE')\n",
    "sys.path.append('/storage/vbutoi/projects/UniverSeg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "import os \n",
    "os.environ['DATAPATH'] = ':'.join((\n",
    "       '/storage/vbutoi/datasets',\n",
    "))\n",
    "\n",
    "from ese.experiment.analysis.analyze_inf import load_cal_inference_stats\n",
    "# Results loader object does everything\n",
    "from ionpy.analysis import ResultsLoader\n",
    "from pathlib import Path\n",
    "root = Path(\"/storage/vbutoi/scratch/ESE\")\n",
    "rs = ResultsLoader()\n",
    "\n",
    "# For using code without restarting.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For using yaml configs.\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml results_cfg \n",
    "\n",
    "log:\n",
    "    root: /storage/vbutoi/scratch/ESE/inference\n",
    "    inference_groups: \n",
    "        - '05_27_24_SW_Pairwise'\n",
    "\n",
    "options:\n",
    "    add_dice_loss_rows: True\n",
    "    drop_nan_metric_rows: True \n",
    "    remove_shared_columns: False\n",
    "    equal_rows_per_cfg_assert: False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = load_cal_inference_stats(\n",
    "    results_cfg=results_cfg,\n",
    "    load_cached=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Load all of the preds so we can compare them\n",
    "# loaded_pred_dict = {}\n",
    "# for pred_hash_id in inference_df['pred_hash'].unique():\n",
    "#     # Get the rows corresponding to the hash\n",
    "#     pred_rows = inference_df[inference_df['pred_hash'] == pred_hash_id]\n",
    "#     # Get the log set corresponding to the hash\n",
    "#     log_root = pred_rows['root'].unique()[0]\n",
    "#     log_set = pred_rows['log_set'].unique()[0]\n",
    "#     # Load the prediction pickle\n",
    "#     with open(f'{log_root}/{log_set}/preds/{pred_hash_id}.pkl', 'rb') as f:\n",
    "#         loaded_pred_dict[pred_hash_id] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "unique_subj_ids = inference_df['data_id'].unique()\n",
    "\n",
    "pw_error_list = []\n",
    "# Iterate through all the pair-wise comparisons.\n",
    "for (subj_id_1, subj_id_2) in list(itertools.combinations(unique_subj_ids, 2)):\n",
    "    if subj_id_1 != subj_id_2:\n",
    "        # Get the dfs corresponding to these two ids:\n",
    "        data_id_1_df = inference_df[inference_df['data_id'] == subj_id_1]\n",
    "        data_id_2_df = inference_df[inference_df['data_id'] == subj_id_2]\n",
    "        # Get the unique support indices.\n",
    "        unique_sup_ids_1 = data_id_1_df['sup_idx'].unique()\n",
    "        unique_sup_ids_2 = data_id_2_df['sup_idx'].unique()\n",
    "        for (sup_id_1, sup_id_2) in list(itertools.product(unique_sup_ids_1, unique_sup_ids_2)):\n",
    "            # Get the dfs corresponding to these sup ids\n",
    "            sup_id_1_df = data_id_1_df[data_id_1_df['sup_idx'] == sup_id_1].reset_index(drop=True)\n",
    "            sup_id_2_df = data_id_2_df[data_id_2_df['sup_idx'] == sup_id_2].reset_index(drop=True)\n",
    "\n",
    "            # Get our desired quantities.\n",
    "            # GT\n",
    "            gt_volume_1 = sup_id_1_df['gt_volume'].values[0]\n",
    "            gt_volume_2 = sup_id_2_df['gt_volume'].values[0]\n",
    "            # Soft\n",
    "            soft_volume_1 = sup_id_1_df['soft_volume'].values[0]\n",
    "            soft_volume_2 = sup_id_2_df['soft_volume'].values[0]\n",
    "            # Hard\n",
    "            hard_volume_1 = sup_id_1_df['hard_volume'].values[0]\n",
    "            hard_volume_2 = sup_id_2_df['hard_volume'].values[0]\n",
    "\n",
    "            # Now we can get differences in volume by looking at the differences.\n",
    "            # GT\n",
    "            gt_volume_diff = gt_volume_2 - gt_volume_1\n",
    "            gt_volume_quot = gt_volume_2 / gt_volume_1\n",
    "            # Soft\n",
    "            soft_volume_diff = soft_volume_2 - soft_volume_1\n",
    "            soft_volume_quot = soft_volume_2 / soft_volume_1\n",
    "            # Hard\n",
    "            hard_volume_diff = hard_volume_2 - hard_volume_1\n",
    "            hard_volume_quot = hard_volume_2 / hard_volume_1\n",
    "\n",
    "            # Error between predicted difference and actual difference.\n",
    "            # Soft\n",
    "            soft_diff_error = soft_volume_diff - gt_volume_diff\n",
    "            hard_diff_error = hard_volume_diff - gt_volume_diff\n",
    "            # Hard\n",
    "            soft_quot_error = soft_volume_quot - gt_volume_quot\n",
    "            hard_quot_error = hard_volume_quot - gt_volume_quot\n",
    "\n",
    "            # Get the metric information from each of the dataframes\n",
    "            # Dice\n",
    "            dice_1 = sup_id_1_df[sup_id_1_df['image_metric'] == 'Dice']['metric_score'].iloc[0]\n",
    "            dice_2 = sup_id_2_df[sup_id_2_df['image_metric'] == 'Dice']['metric_score'].iloc[0]\n",
    "            # ECE\n",
    "            ece_1 = sup_id_1_df[sup_id_1_df['image_metric'] == 'Image_ECE']['metric_score'].iloc[0]\n",
    "            ece_2 = sup_id_2_df[sup_id_2_df['image_metric'] == 'Image_ECE']['metric_score'].iloc[0]\n",
    "\n",
    "            # Place all of these into a record\n",
    "            pair_record = {\n",
    "                'subj_id_1': subj_id_1,\n",
    "                'subj_id_2': subj_id_2,\n",
    "                'subj_combo': f'{subj_id_1},{subj_id_2}',\n",
    "                'sup_id_1': sup_id_1,\n",
    "                'sup_id_2': sup_id_2,\n",
    "                'dice_1': dice_1,\n",
    "                'dice_2': dice_2,\n",
    "                'ece_1': ece_1,\n",
    "                'ece_2': ece_2,\n",
    "                'mean_dice': np.mean([dice_1, dice_2]),\n",
    "                'mean_ece': np.mean([ece_1, ece_2]),\n",
    "                'gt_volume_diff': gt_volume_diff,\n",
    "                'gt_volume_quot': gt_volume_quot,\n",
    "                'soft_volume_diff': soft_volume_diff,\n",
    "                'soft_volume_quot': soft_volume_quot,\n",
    "                'hard_volume_diff': hard_volume_diff,\n",
    "                'hard_volume_quot': hard_volume_quot,\n",
    "            }\n",
    "            # Add the record to the list\n",
    "            pw_error_list.append(pair_record)\n",
    "# Convert the final dataframe into a pandas dataframe\n",
    "pairwise_df = pd.DataFrame(pw_error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: On average, how do the real compare with respect to difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe\n",
    "pw_diff_df = pd.melt(\n",
    "    pairwise_df, \n",
    "    id_vars=[\n",
    "        'subj_id_1', \n",
    "        'subj_id_2', \n",
    "        'sup_id_1', \n",
    "        'sup_id_2', \n",
    "        'subj_combo'\n",
    "    ], \n",
    "    value_vars=[\n",
    "        'gt_volume_diff', \n",
    "        'soft_volume_diff', \n",
    "        'hard_volume_diff'\n",
    "    ], \n",
    "    var_name='Pred_Type', \n",
    "    value_name='Volume Difference'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create the boxplot with modified whiskers and without showing outliers\n",
    "ax = sns.boxplot(\n",
    "    x='Pred_Type',\n",
    "    y='Volume Difference',\n",
    "    data=pw_diff_df,\n",
    "    palette=\"Set2\",       # Use a color palette\n",
    "    linewidth=2,          # Set the linewidth of the edge\n",
    "    showfliers=False,     # Do not show outliers\n",
    "    whis=0.5              # Shorten the whiskers to half the IQR\n",
    ")\n",
    "\n",
    "# Enhance the plot\n",
    "ax.set_title('Volume Difference by Prediction Type', fontsize=20)\n",
    "ax.set_xlabel('Prediction Type', fontsize=15)\n",
    "ax.set_ylabel('Volume Difference', fontsize=15)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "# Remove the top and right spines for a cleaner look\n",
    "sns.despine()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1.5: Per-subject, how do the real compare with respect to difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot\n",
    "g = sns.catplot(\n",
    "    x='subj_combo',\n",
    "    y='Volume Difference',\n",
    "    kind='box',\n",
    "    data=pw_diff_df,\n",
    "    hue='Pred_Type',\n",
    "    palette=\"Set2\",       # Use a color palette\n",
    "    linewidth=2,          # Set the linewidth of the edge\n",
    "    aspect=1.5,            # Adjust the aspect ratio\n",
    "    height=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: On average, how do the real compare with respect to quotient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe\n",
    "pw_quot_df = pd.melt(\n",
    "    pairwise_df, \n",
    "    id_vars=[\n",
    "        'subj_id_1', \n",
    "        'subj_id_2', \n",
    "        'sup_id_1', \n",
    "        'sup_id_2', \n",
    "        'subj_combo'\n",
    "    ], \n",
    "    value_vars=[\n",
    "        'gt_volume_quot', \n",
    "        'soft_volume_quot', \n",
    "        'hard_volume_quot'\n",
    "    ], \n",
    "    var_name='Pred_Type', \n",
    "    value_name='Volume Quotient'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create the boxplot\n",
    "ax = sns.boxplot(\n",
    "    x='Pred_Type',\n",
    "    y='Volume Quotient',\n",
    "    data=pw_quot_df,\n",
    "    palette=\"Set2\",       # Use a color palette\n",
    "    linewidth=2           # Set the linewidth of the edge\n",
    ")\n",
    "\n",
    "# Enhance the plot\n",
    "ax.set_title('Volume Quotient by Prediction Type', fontsize=20)\n",
    "ax.set_xlabel('Prediction Type', fontsize=15)\n",
    "ax.set_ylabel('Volume Quotient', fontsize=15)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "# Remove the top and right spines for a cleaner look\n",
    "sns.despine()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2.5: Per-subject, how do the real compare with respect to quotient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot\n",
    "g = sns.catplot(\n",
    "    x='subj_combo',\n",
    "    y='Volume Quotient',\n",
    "    kind='box',\n",
    "    data=pw_quot_df,\n",
    "    hue='Pred_Type',\n",
    "    palette=\"Set2\",       # Use a color palette\n",
    "    linewidth=2,          # Set the linewidth of the edge\n",
    "    aspect=1.5,            # Adjust the aspect ratio\n",
    "    height=8\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
